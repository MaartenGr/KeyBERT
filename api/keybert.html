
<!doctype html>
<html lang="en" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
        <meta name="description" content="Leveraging BERT to extract important keywords">
      
      
        <meta name="author" content="Maarten P. Grootendorst">
      
      
        <link rel="canonical" href="https://maartengr.github.io/keyBERT/api/keybert.html">
      
      
        <link rel="prev" href="../guides/llms.html">
      
      
        <link rel="next" href="mmr.html">
      
      
      <link rel="icon" href="../icon.png">
      <meta name="generator" content="mkdocs-1.6.1, mkdocs-material-9.5.44">
    
    
      
        <title>KeyBERT - KeyBERT</title>
      
    
    
      <link rel="stylesheet" href="../assets/stylesheets/main.0253249f.min.css">
      
        
        <link rel="stylesheet" href="../assets/stylesheets/palette.06af60db.min.css">
      
      


    
    
      
    
    
      
        
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Ubuntu:300,300i,400,400i,700,700i%7CUbuntu+Mono:400,400i,700,700i&display=fallback">
        <style>:root{--md-text-font:"Ubuntu";--md-code-font:"Ubuntu Mono"}</style>
      
    
    
      <link rel="stylesheet" href="../assets/_mkdocstrings.css">
    
      <link rel="stylesheet" href="../stylesheets/extra.css">
    
    <script>__md_scope=new URL("..",location),__md_hash=e=>[...e].reduce(((e,_)=>(e<<5)-e+_.charCodeAt(0)),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script>
    
      

    
    
    
  </head>
  
  
    
    
      
    
    
    
    
    <body dir="ltr" data-md-color-scheme="black" data-md-color-primary="custom" data-md-color-accent="indigo">
  
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#keybert" class="md-skip">
          Skip to content
        </a>
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
    
      

<header class="md-header" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="Header">
    <a href="../index.html" title="KeyBERT" class="md-header__button md-logo" aria-label="KeyBERT" data-md-component="logo">
      
  <img src="../icon.png" alt="logo">

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3zm0 5h18v2H3zm0 5h18v2H3z"/></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            KeyBERT
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              KeyBERT
            
          </span>
        </div>
      </div>
    </div>
    
      
        <form class="md-header__option" data-md-component="palette">
  
    
    
    
    <input class="md-option" data-md-color-media="(prefers-color-scheme: light)" data-md-color-scheme="black" data-md-color-primary="custom" data-md-color-accent="indigo"  aria-label="Switch to dark mode"  type="radio" name="__palette" id="__palette_0">
    
      <label class="md-header__button md-icon" title="Switch to dark mode" for="__palette_1" hidden>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 7a5 5 0 0 1 5 5 5 5 0 0 1-5 5 5 5 0 0 1-5-5 5 5 0 0 1 5-5m0 2a3 3 0 0 0-3 3 3 3 0 0 0 3 3 3 3 0 0 0 3-3 3 3 0 0 0-3-3m0-7 2.39 3.42C13.65 5.15 12.84 5 12 5s-1.65.15-2.39.42zM3.34 7l4.16-.35A7.2 7.2 0 0 0 5.94 8.5c-.44.74-.69 1.5-.83 2.29zm.02 10 1.76-3.77a7.131 7.131 0 0 0 2.38 4.14zM20.65 7l-1.77 3.79a7.02 7.02 0 0 0-2.38-4.15zm-.01 10-4.14.36c.59-.51 1.12-1.14 1.54-1.86.42-.73.69-1.5.83-2.29zM12 22l-2.41-3.44c.74.27 1.55.44 2.41.44.82 0 1.63-.17 2.37-.44z"/></svg>
      </label>
    
  
    
    
    
    <input class="md-option" data-md-color-media="(prefers-color-scheme: dark)" data-md-color-scheme="slate" data-md-color-primary="black" data-md-color-accent="indigo"  aria-label="Switch to light mode"  type="radio" name="__palette" id="__palette_1">
    
      <label class="md-header__button md-icon" title="Switch to light mode" for="__palette_0" hidden>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="m17.75 4.09-2.53 1.94.91 3.06-2.63-1.81-2.63 1.81.91-3.06-2.53-1.94L12.44 4l1.06-3 1.06 3zm3.5 6.91-1.64 1.25.59 1.98-1.7-1.17-1.7 1.17.59-1.98L15.75 11l2.06-.05L18.5 9l.69 1.95zm-2.28 4.95c.83-.08 1.72 1.1 1.19 1.85-.32.45-.66.87-1.08 1.27C15.17 23 8.84 23 4.94 19.07c-3.91-3.9-3.91-10.24 0-14.14.4-.4.82-.76 1.27-1.08.75-.53 1.93.36 1.85 1.19-.27 2.86.69 5.83 2.89 8.02a9.96 9.96 0 0 0 8.02 2.89m-1.64 2.02a12.08 12.08 0 0 1-7.8-3.47c-2.17-2.19-3.33-5-3.49-7.82-2.81 3.14-2.7 7.96.31 10.98 3.02 3.01 7.84 3.12 10.98.31"/></svg>
      </label>
    
  
</form>
      
    
    
      <script>var palette=__md_get("__palette");if(palette&&palette.color){if("(prefers-color-scheme)"===palette.color.media){var media=matchMedia("(prefers-color-scheme: light)"),input=document.querySelector(media.matches?"[data-md-color-media='(prefers-color-scheme: light)']":"[data-md-color-media='(prefers-color-scheme: dark)']");palette.color.media=input.getAttribute("data-md-color-media"),palette.color.scheme=input.getAttribute("data-md-color-scheme"),palette.color.primary=input.getAttribute("data-md-color-primary"),palette.color.accent=input.getAttribute("data-md-color-accent")}for(var[key,value]of Object.entries(palette.color))document.body.setAttribute("data-md-color-"+key,value)}</script>
    
    
    
      <label class="md-header__button md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
      </label>
      <div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" aria-label="Search" placeholder="Search" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="search-query" required>
      <label class="md-search__icon md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11z"/></svg>
      </label>
      <nav class="md-search__options" aria-label="Search">
        
        <button type="reset" class="md-search__icon md-icon" title="Clear" aria-label="Clear" tabindex="-1">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12z"/></svg>
        </button>
      </nav>
      
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" tabindex="0" data-md-scrollfix>
        <div class="md-search-result" data-md-component="search-result">
          <div class="md-search-result__meta">
            Initializing search
          </div>
          <ol class="md-search-result__list" role="presentation"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
    
    
      <div class="md-header__source">
        <a href="https://github.com/MaartenGr/keyBERT" title="Go to repository" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><!--! Font Awesome Free 6.6.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M439.55 236.05 244 40.45a28.87 28.87 0 0 0-40.81 0l-40.66 40.63 51.52 51.52c27.06-9.14 52.68 16.77 43.39 43.68l49.66 49.66c34.23-11.8 61.18 31 35.47 56.69-26.49 26.49-70.21-2.87-56-37.34L240.22 199v121.85c25.3 12.54 22.26 41.85 9.08 55a34.34 34.34 0 0 1-48.55 0c-17.57-17.6-11.07-46.91 11.25-56v-123c-20.8-8.51-24.6-30.74-18.64-45L142.57 101 8.45 235.14a28.86 28.86 0 0 0 0 40.81l195.61 195.6a28.86 28.86 0 0 0 40.8 0l194.69-194.69a28.86 28.86 0 0 0 0-40.81"/></svg>
  </div>
  <div class="md-source__repository">
    GitHub
  </div>
</a>
      </div>
    
  </nav>
  
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
          
            
<nav class="md-tabs" aria-label="Tabs" data-md-component="tabs">
  <div class="md-grid">
    <ul class="md-tabs__list">
      
        
  
  
  
    <li class="md-tabs__item">
      <a href="../index.html" class="md-tabs__link">
        
  
    
  
  Home

      </a>
    </li>
  

      
        
  
  
  
    
    
      <li class="md-tabs__item">
        <a href="../guides/quickstart.html" class="md-tabs__link">
          
  
  Guides

        </a>
      </li>
    
  

      
        
  
  
    
  
  
    
    
      <li class="md-tabs__item md-tabs__item--active">
        <a href="keybert.html" class="md-tabs__link">
          
  
  API

        </a>
      </li>
    
  

      
        
  
  
  
    <li class="md-tabs__item">
      <a href="../faq.html" class="md-tabs__link">
        
  
    
  
  FAQ

      </a>
    </li>
  

      
        
  
  
  
    <li class="md-tabs__item">
      <a href="../changelog.html" class="md-tabs__link">
        
  
    
  
  Changelog

      </a>
    </li>
  

      
    </ul>
  </div>
</nav>
          
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    


  


<nav class="md-nav md-nav--primary md-nav--lifted" aria-label="Navigation" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href="../index.html" title="KeyBERT" class="md-nav__button md-logo" aria-label="KeyBERT" data-md-component="logo">
      
  <img src="../icon.png" alt="logo">

    </a>
    KeyBERT
  </label>
  
    <div class="md-nav__source">
      <a href="https://github.com/MaartenGr/keyBERT" title="Go to repository" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><!--! Font Awesome Free 6.6.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M439.55 236.05 244 40.45a28.87 28.87 0 0 0-40.81 0l-40.66 40.63 51.52 51.52c27.06-9.14 52.68 16.77 43.39 43.68l49.66 49.66c34.23-11.8 61.18 31 35.47 56.69-26.49 26.49-70.21-2.87-56-37.34L240.22 199v121.85c25.3 12.54 22.26 41.85 9.08 55a34.34 34.34 0 0 1-48.55 0c-17.57-17.6-11.07-46.91 11.25-56v-123c-20.8-8.51-24.6-30.74-18.64-45L142.57 101 8.45 235.14a28.86 28.86 0 0 0 0 40.81l195.61 195.6a28.86 28.86 0 0 0 40.8 0l194.69-194.69a28.86 28.86 0 0 0 0-40.81"/></svg>
  </div>
  <div class="md-source__repository">
    GitHub
  </div>
</a>
    </div>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../index.html" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Home
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    
    
    
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2" >
        
          
          <label class="md-nav__link" for="__nav_2" id="__nav_2_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    Guides
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_2_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_2">
            <span class="md-nav__icon md-icon"></span>
            Guides
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../guides/quickstart.html" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Quickstart
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../guides/embeddings.html" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Embedding Models
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../guides/countvectorizer.html" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    CountVectorizer
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../guides/keyllm.html" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    KeyLLM
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../guides/llms.html" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    LLMs
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
    
  
  
  
    
    
    
      
        
        
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--active md-nav__item--section md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3" checked>
        
          
          <label class="md-nav__link" for="__nav_3" id="__nav_3_label" tabindex="">
            
  
  <span class="md-ellipsis">
    API
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_3_label" aria-expanded="true">
          <label class="md-nav__title" for="__nav_3">
            <span class="md-nav__icon md-icon"></span>
            API
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
    
  
  
  
    <li class="md-nav__item md-nav__item--active">
      
      <input class="md-nav__toggle md-toggle" type="checkbox" id="__toc">
      
      
        
      
      
        <label class="md-nav__link md-nav__link--active" for="__toc">
          
  
  <span class="md-ellipsis">
    KeyBERT
  </span>
  

          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <a href="keybert.html" class="md-nav__link md-nav__link--active">
        
  
  <span class="md-ellipsis">
    KeyBERT
  </span>
  

      </a>
      
        

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#keybert._model.KeyBERT" class="md-nav__link">
    <span class="md-ellipsis">
      KeyBERT
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#keybert._model.KeyBERT.__init__" class="md-nav__link">
    <span class="md-ellipsis">
      __init__
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#keybert._model.KeyBERT.extract_embeddings" class="md-nav__link">
    <span class="md-ellipsis">
      extract_embeddings
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#keybert._model.KeyBERT.extract_keywords" class="md-nav__link">
    <span class="md-ellipsis">
      extract_keywords
    </span>
  </a>
  
</li>
      
    </ul>
  
</nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="mmr.html" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    MMR
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="maxsum.html" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    MaxSum
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="keyllm.html" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    KeyLLM
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    
    
    
      
      
        
          
          
        
      
    
    
    <li class="md-nav__item md-nav__item--section md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3_5" >
        
          
          <label class="md-nav__link" for="__nav_3_5" id="__nav_3_5_label" tabindex="">
            
  
  <span class="md-ellipsis">
    LLM
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_3_5_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_3_5">
            <span class="md-nav__icon md-icon"></span>
            LLM
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="openai.html" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    OpenAI
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="cohere.html" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Cohere
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="langchain.html" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    LangChain
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="textgeneration.html" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    TextGeneration
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="litellm.html" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    LiteLLM
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../faq.html" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    FAQ
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../changelog.html" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Changelog
  </span>
  

      </a>
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              
              <div class="md-sidebar md-sidebar--secondary" data-md-component="sidebar" data-md-type="toc" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#keybert._model.KeyBERT" class="md-nav__link">
    <span class="md-ellipsis">
      KeyBERT
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#keybert._model.KeyBERT.__init__" class="md-nav__link">
    <span class="md-ellipsis">
      __init__
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#keybert._model.KeyBERT.extract_embeddings" class="md-nav__link">
    <span class="md-ellipsis">
      extract_embeddings
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#keybert._model.KeyBERT.extract_keywords" class="md-nav__link">
    <span class="md-ellipsis">
      extract_keywords
    </span>
  </a>
  
</li>
      
    </ul>
  
</nav>
                  </div>
                </div>
              </div>
            
          
          
            <div class="md-content" data-md-component="content">
              <article class="md-content__inner md-typeset">
                
                  

  
  


<h1 id="keybert"><code>KeyBERT</code><a class="headerlink" href="#keybert" title="Permanent link">&para;</a></h1>


<div class="doc doc-object doc-class">



<a id="keybert._model.KeyBERT"></a>
    <div class="doc doc-contents first">


        <p>A minimal method for keyword extraction with BERT.</p>
<p>The keyword extraction is done by finding the sub-phrases in
a document that are the most similar to the document itself.</p>
<p>First, document embeddings are extracted with BERT to get a
document-level representation. Then, word embeddings are extracted
for N-gram words/phrases. Finally, we use cosine similarity to find the
words/phrases that are the most similar to the document.</p>
<p>The most similar words could then be identified as the words that
best describe the entire document.</p>
<div class="excalidraw">
<svg version="1.1" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 1549.2086356411555 727.0271449192701" width="1549.2086356411555" height="727.0271449192701">
  <!-- svg-source:excalidraw -->

  <defs>
    <style>
      @font-face {
        font-family: "Virgil";
        src: url("https://unpkg.com/@excalidraw/excalidraw@0.12.0/dist/excalidraw-assets/Virgil.woff2");
      }
      @font-face {
        font-family: "Cascadia";
        src: url("https://unpkg.com/@excalidraw/excalidraw@0.12.0/dist/excalidraw-assets/Cascadia.woff2");
      }
    </style>
  </defs>
  <rect x="0" y="0" width="1549.2086356411555" height="727.0271449192701" fill="#ffffff"></rect><g stroke-linecap="round" transform="translate(10 301.34604882399924) rotate(0 152.22219848632812 56.66667175292969)"><path d="M-1.35 -0.89 C121.92 1.95, 242.59 1.89, 305.01 1.15 M-0.36 -0.37 C65.29 -0.81, 128.5 -0.25, 304.03 -0.38 M305.5 1.52 C304.58 39.35, 303.72 77.14, 305.74 112.8 M304.89 -0.44 C304.69 36.82, 304.18 72.79, 304.08 114.29 M304.89 113.45 C211.94 111.05, 117.88 111.36, 1.22 113.89 M304.61 112.96 C204.89 113.07, 105.86 114.09, 0.6 113.05 M-1 112.68 C-0.97 71.47, 0.97 25.9, -1.34 1.2 M0.79 114.32 C0.19 68.23, 0.18 24.49, -0.39 0.72" stroke="#000000" stroke-width="1" fill="none"></path></g><g stroke-linecap="round" transform="translate(401.3110530110598 181.25266287882926) rotate(0 86.33320109049475 160.6667022705078)"><path d="M-0.57 1.52 C59.48 -1, 122.76 -0.08, 174.21 1.72 M-0.29 0.65 C65.1 1.41, 129.74 -0.17, 172.44 -0.77 M172.64 -0.03 C175.87 83.29, 175.55 168.04, 172.59 321.2 M172.28 0.15 C173.06 117.21, 173.79 233.9, 172.01 321.52 M172.93 320.25 C117.47 320.34, 64.17 322.1, -1.99 322.74 M173.01 322.33 C120.69 321.05, 68.8 320.48, 0.58 322.1 M0.86 321.16 C-0.39 236.72, -1.54 154.31, 0.55 -0.46 M-0.69 320.81 C1.54 241.71, 0.94 163.4, -0.15 -0.23" stroke="#000000" stroke-width="1" fill="none"></path></g><g stroke-linecap="round" transform="translate(1109.1849667944355 297.89678242924856) rotate(0 181.28019748391853 44.86960156463721)"><path d="M-0.53 0.71 C114.43 1.9, 228.62 1.35, 361.38 0.31 M-0.16 0 C140.79 1.29, 281.51 1.6, 362.51 -0.17 M361.5 1.94 C361.48 26.73, 363.68 51.06, 364.53 91.09 M362.71 -0.82 C361.55 33.65, 362.31 66.01, 362.47 89.75 M361.9 90.92 C251.15 89.01, 139.72 88.77, 0.09 89.21 M362.92 89.81 C288.78 91.87, 214.36 91.34, -0.49 89.78 M-0.73 90.4 C0.89 65.51, 1.7 44.63, 1.73 0.7 M-0.45 89.2 C1.04 61.44, 1.06 32.36, 0.34 -0.63" stroke="#000000" stroke-width="1" fill="none"></path></g><g transform="translate(41.98626952178029 256.99810770039915) rotate(0 112.08337437343164 18.021248428669423)"><text x="0" y="28.042496857338797" font-family="Helvetica, Segoe UI Emoji" font-size="31.647070411321874px" fill="#364fc7" text-anchor="start" style="white-space: pre;" direction="ltr">Input Document</text></g><g transform="translate(381.8388081952603 141.70318677845512) rotate(0 108.72932382403383 17.413680768692927)"><text x="0" y="27.827361537385833" font-family="Helvetica, Segoe UI Emoji" font-size="30.580122325509503px" fill="#364fc7" text-anchor="start" style="white-space: pre;" direction="ltr">Tokenize Words</text></g><g transform="translate(732.9213147580451 81.94276557539672) rotate(0 99 17)"><text x="0" y="27" font-family="Cascadia, Segoe UI Emoji" font-size="28px" fill="#0b7285" text-anchor="start" style="white-space: pre;" direction="ltr">Embed Tokens</text></g><g transform="translate(692.8916050136372 10) rotate(0 146.85429739770518 18.64094796689136)"><text x="0" y="30.28189593378275" font-family="Helvetica, Segoe UI Emoji" font-size="32.7353232589312px" fill="#364fc7" text-anchor="start" style="white-space: pre;" direction="ltr">Extract Embeddings</text></g><g transform="translate(721.3558272927344 330.72314563375755) rotate(0 115.5 17)"><text x="0" y="27" font-family="Cascadia, Segoe UI Emoji" font-size="28px" fill="#0b7285" text-anchor="start" style="white-space: pre;" direction="ltr">Embed Document</text></g><g transform="translate(1167.544612692157 219.70398294626193) rotate(0 123.70834794710288 38.170233753195305)"><text x="123.7083479471029" y="30.170233753195312" font-family="Helvetica, Segoe UI Emoji" font-size="33.11152807506101px" fill="#364fc7" text-anchor="middle" style="white-space: pre;" direction="ltr">Calculate</text><text x="123.7083479471029" y="68.34046750639062" font-family="Helvetica, Segoe UI Emoji" font-size="33.11152807506101px" fill="#364fc7" text-anchor="middle" style="white-space: pre;" direction="ltr">Cosine Similarity</text></g><g transform="translate(30.000015258789062 312.95716332595237) rotate(0 135 41.5)"><text x="0" y="20.666666666666668" font-family="Virgil, Segoe UI Emoji" font-size="22.17865588618258px" fill="#000000" text-anchor="start" style="white-space: pre;" direction="ltr">Most microbats use </text><text x="0" y="48.333333333333336" font-family="Virgil, Segoe UI Emoji" font-size="22.17865588618258px" fill="#000000" text-anchor="start" style="white-space: pre;" direction="ltr">echolocationto navigate </text><text x="0" y="76" font-family="Virgil, Segoe UI Emoji" font-size="22.17865588618258px" fill="#000000" text-anchor="start" style="white-space: pre;" direction="ltr">and find food.</text></g><g transform="translate(699.7201346545717 507.82171006231204) rotate(0 135 41)"><text x="135" y="19.333333333333332" font-family="Virgil, Segoe UI Emoji" font-size="22.17865588618258px" fill="#000000" text-anchor="middle" style="white-space: pre;" direction="ltr">Most microbats use </text><text x="135" y="46.666666666666664" font-family="Virgil, Segoe UI Emoji" font-size="22.17865588618258px" fill="#000000" text-anchor="middle" style="white-space: pre;" direction="ltr">echolocationto navigate </text><text x="135" y="74" font-family="Virgil, Segoe UI Emoji" font-size="22.17865588618258px" fill="#000000" text-anchor="middle" style="white-space: pre;" direction="ltr">and find food.</text></g><g transform="translate(415.9776332112551 195.53046439250113) rotate(0 75 144)"><text x="0" y="23" font-family="Virgil, Segoe UI Emoji" font-size="25.424312845136125px" fill="#000000" text-anchor="start" style="white-space: pre;" direction="ltr">most</text><text x="0" y="55" font-family="Virgil, Segoe UI Emoji" font-size="25.424312845136125px" fill="#000000" text-anchor="start" style="white-space: pre;" direction="ltr">microbats</text><text x="0" y="87" font-family="Virgil, Segoe UI Emoji" font-size="25.424312845136125px" fill="#000000" text-anchor="start" style="white-space: pre;" direction="ltr">use </text><text x="0" y="119" font-family="Virgil, Segoe UI Emoji" font-size="25.424312845136125px" fill="#000000" text-anchor="start" style="white-space: pre;" direction="ltr">echolocation</text><text x="0" y="151" font-family="Virgil, Segoe UI Emoji" font-size="25.424312845136125px" fill="#000000" text-anchor="start" style="white-space: pre;" direction="ltr">to</text><text x="0" y="183" font-family="Virgil, Segoe UI Emoji" font-size="25.424312845136125px" fill="#000000" text-anchor="start" style="white-space: pre;" direction="ltr">navigate </text><text x="0" y="215" font-family="Virgil, Segoe UI Emoji" font-size="25.424312845136125px" fill="#000000" text-anchor="start" style="white-space: pre;" direction="ltr">and</text><text x="0" y="247" font-family="Virgil, Segoe UI Emoji" font-size="25.424312845136125px" fill="#000000" text-anchor="start" style="white-space: pre;" direction="ltr">find</text><text x="0" y="279" font-family="Virgil, Segoe UI Emoji" font-size="25.424312845136125px" fill="#000000" text-anchor="start" style="white-space: pre;" direction="ltr">food</text></g><g stroke-linecap="round"><g transform="translate(338.8332773844397 352.71555512181226) rotate(0 24.267613196075956 0.35124721677271964)"><path d="M0.11 0.83 C8.45 1.18, 41.3 1.28, 49.83 1.3 M-1.29 0.23 C6.87 0.28, 39.93 -0.9, 48.65 -0.53" stroke="#000000" stroke-width="2" fill="none"></path></g><g transform="translate(338.8332773844397 352.71555512181226) rotate(0 24.267613196075956 0.35124721677271964)"><path d="M26.43 7.35 C32.67 7.29, 38.83 1.75, 50.5 0.99 M24.78 8.44 C31.65 6.66, 36.89 3.86, 48.17 -0.27" stroke="#000000" stroke-width="2" fill="none"></path></g><g transform="translate(338.8332773844397 352.71555512181226) rotate(0 24.267613196075956 0.35124721677271964)"><path d="M26.41 -9.84 C32.55 -5.05, 38.71 -5.73, 50.5 0.99 M24.77 -8.75 C31.5 -6.12, 36.75 -4.5, 48.17 -0.27" stroke="#000000" stroke-width="2" fill="none"></path></g></g><g stroke-linecap="round"><g transform="translate(585.9578047818059 344.8390443250703) rotate(0 19.34384170457588 0.09624185837600407)"><path d="M0.47 1.13 C7.21 1.33, 33.04 0.99, 39.43 0.74 M-0.74 0.68 C5.93 0.58, 32.05 -1.01, 38.46 -1" stroke="#000000" stroke-width="2" fill="none"></path></g><g transform="translate(585.9578047818059 344.8390443250703) rotate(0 19.34384170457588 0.09624185837600407)"><path d="M22.02 6.31 C25.36 3.62, 27.51 2.14, 39.83 -0.51 M19.31 5.61 C25.75 4.77, 31.74 0.83, 39.22 -0.47" stroke="#000000" stroke-width="2" fill="none"></path></g><g transform="translate(585.9578047818059 344.8390443250703) rotate(0 19.34384170457588 0.09624185837600407)"><path d="M21.56 -7.18 C24.96 -6.94, 27.2 -5.5, 39.83 -0.51 M18.85 -7.88 C25.37 -4.23, 31.52 -3.68, 39.22 -0.47" stroke="#000000" stroke-width="2" fill="none"></path></g></g><g stroke-linecap="round"><g transform="translate(728.7365014891425 131.50148222967937) rotate(0 -0.3603741368511919 58.395572234154656)"><path d="M0.07 -1.02 C0.13 18.38, 0.68 97.04, 0.64 116.64 M-1.36 1.05 C-1.43 20.59, -0.03 98.41, 0.02 117.82" stroke="#000000" stroke-width="2" fill="none"></path></g></g><g stroke-linecap="round"><g transform="translate(792.7143485491285 378.1687696314818) rotate(0 0.7835678047081274 58.78827979585617)"><path d="M-0.25 0.12 C-0.2 20.06, 0.54 98.8, 0.49 118.44 M1.82 -0.86 C1.74 18.87, 0.37 97.43, -0.2 116.89" stroke="#000000" stroke-width="2" fill="none"></path></g></g><g stroke-linecap="round"><g transform="translate(869.7487782111721 130.93597058185026) rotate(0 -0.051383505497483384 58.66422028616063)"><path d="M0.57 -0.82 C0.45 18.85, 0.58 98.35, 0.42 118.15 M-0.59 1.37 C-0.93 20.69, -0.17 97.36, -0.31 116.45" stroke="#000000" stroke-width="2" fill="none"></path></g></g><g stroke-linecap="round"><g transform="translate(793.7465830430244 130.49946591890307) rotate(0 -0.5423950767237784 58.523907961072325)"><path d="M-1.05 0.14 C-1.33 19.43, -0.78 97.08, -0.48 116.68 M0.6 -0.83 C0.01 18.53, -1.64 98.47, -1.68 117.87" stroke="#000000" stroke-width="2" fill="none"></path></g></g><g stroke-linecap="round"><g transform="translate(857.7244301030105 377.1667533207055) rotate(0 -0.6291444302955824 59.29219547918524)"><path d="M0.49 1.1 C0.45 20.74, -0.43 98.69, -0.53 117.95 M-0.71 0.64 C-0.94 19.91, -1.71 96.66, -1.75 116.15" stroke="#000000" stroke-width="2" fill="none"></path></g></g><g stroke-linecap="round"><g transform="translate(934.758859765054 129.93395427107396) rotate(0 0.06617031456437417 58.91531916421832)"><path d="M-0.82 0.42 C-0.71 19.81, 0.58 97.41, 0.81 116.92 M0.95 -0.41 C0.96 19.1, 0.43 98.35, 0.29 118.24" stroke="#000000" stroke-width="2" fill="none"></path></g></g><g stroke-linecap="round"><g transform="translate(727.794235248127 130.63372401633433) rotate(0 9.878745196024738 -0.060496431823651164)"><path d="M0.87 -1.09 C4 -1.12, 17.02 -1.01, 19.89 -1.01 M-0.13 0.96 C2.81 1.1, 15.9 0.22, 19.34 0" stroke="#000000" stroke-width="2" fill="none"></path></g></g><g stroke-linecap="round"><g transform="translate(791.7720823081131 377.30101141813674) rotate(0 9.252171697016593 -0.04375947788355461)"><path d="M-0.53 0.62 C2.56 0.31, 15.84 -0.66, 19.03 -0.7 M1.4 -0.11 C4.28 0.25, 15.33 0.74, 18.03 0.47" stroke="#000000" stroke-width="2" fill="none"></path></g></g><g stroke-linecap="round"><g transform="translate(868.8065119701566 130.06821236850521) rotate(0 8.95003676376382 -0.15441920883188232)"><path d="M0.42 0.81 C3.44 1.05, 15.57 0.67, 18.72 0.61 M-0.82 0.2 C1.96 0.15, 14.31 -0.88, 17.55 -1.2" stroke="#000000" stroke-width="2" fill="none"></path></g></g><g stroke-linecap="round"><g transform="translate(772.804316802009 129.63170770555803) rotate(0 9.80534051685197 -0.5227484212769582)"><path d="M0.14 -0.48 C3.07 -0.18, 15.25 0.81, 18.48 0.73 M-1.24 -1.78 C1.99 -1.73, 17.48 -1.28, 20.85 -1" stroke="#000000" stroke-width="2" fill="none"></path></g></g><g stroke-linecap="round"><g transform="translate(836.7821638619951 376.29899510736044) rotate(0 8.496880036155403 0.0016108332294777483)"><path d="M-0.1 -0.7 C3.09 -0.74, 15.33 0.11, 18.61 0.38 M-1.62 1.54 C1.41 1.08, 13.95 -1.13, 17.39 -1.54" stroke="#000000" stroke-width="2" fill="none"></path></g></g><g stroke-linecap="round"><g transform="translate(913.8165935240386 129.06619605772892) rotate(0 9.761701882640637 0.04750062567183022)"><path d="M0.81 -0.42 C4.24 -0.46, 16.62 -0.09, 19.74 0.16 M-0.22 -1.68 C3.18 -1.54, 15.91 1.62, 19.11 1.78" stroke="#000000" stroke-width="2" fill="none"></path></g></g><g stroke-linecap="round"><g transform="translate(730.1036362520331 249.7338663687981) rotate(0 8.594152575335443 -0.8818261392342492)"><path d="M-0.62 -0.39 C2.17 -0.63, 14.87 -1.13, 17.81 -1.08 M1.25 -1.64 C3.88 -1.8, 14.86 -0.58, 17.38 -0.11" stroke="#000000" stroke-width="2" fill="none"></path></g></g><g stroke-linecap="round"><g transform="translate(794.0814833120191 496.4011537706005) rotate(0 8.362663560709962 0.0844441132686029)"><path d="M-0.52 0.38 C2.37 0.65, 14.32 1.02, 17.25 0.93 M1.4 -0.46 C4.18 -0.45, 14.03 -0.96, 16.52 -0.7" stroke="#000000" stroke-width="2" fill="none"></path></g></g><g stroke-linecap="round"><g transform="translate(871.1159129740627 249.16835472096898) rotate(0 8.282786361477406 -0.45065344541797003)"><path d="M-0.42 0.61 C2.34 0.54, 13.93 0.65, 16.98 0.4 M1.57 -0.12 C4.15 -0.63, 13.43 -1.52, 16.12 -1.51" stroke="#000000" stroke-width="2" fill="none"></path></g></g><g stroke-linecap="round"><g transform="translate(775.113717805915 248.7318500580218) rotate(0 8.539076310000496 0.057944700540986105)"><path d="M-0.48 -0.65 C2.62 -0.61, 14.83 0.29, 17.56 0.41 M1.47 1.62 C4.57 1.28, 14.19 -1.47, 17 -1.5" stroke="#000000" stroke-width="2" fill="none"></path></g></g><g stroke-linecap="round"><g transform="translate(839.0915648659011 495.3991374598242) rotate(0 8.106932300692506 0.5897904961863674)"><path d="M0.42 0.93 C3.35 0.84, 14.44 -0.18, 17.03 -0.15 M-0.81 0.37 C2.02 0.44, 12.99 1.48, 16.19 1.31" stroke="#000000" stroke-width="2" fill="none"></path></g></g><g stroke-linecap="round"><g transform="translate(916.1259945279446 248.16633841019268) rotate(0 8.346900170510594 0.1391034362808341)"><path d="M0.61 0.16 C3.34 0.4, 14.67 1.27, 17.23 1.05 M-0.54 -0.81 C2.01 -0.84, 13.34 -0.58, 16.49 -0.52" stroke="#000000" stroke-width="2" fill="none"></path></g></g><g stroke-linecap="round" transform="translate(807.2065019732206 177.16901764129022) rotate(0 3.8095310756137906 3.8095310756138474)"><path d="M2.18 -0.66 C2.97 -1.06, 4.43 -0.74, 5.44 -0.13 C6.45 0.47, 8.03 2.03, 8.25 2.99 C8.47 3.94, 7.1 4.96, 6.77 5.61 C6.44 6.26, 6.75 6.65, 6.26 6.9 C5.77 7.15, 4.75 7.2, 3.83 7.13 C2.91 7.06, 1.3 7.1, 0.76 6.48 C0.23 5.86, 0.51 4.36, 0.63 3.41 C0.76 2.46, 1.05 1.44, 1.5 0.78 C1.95 0.13, 2.69 -0.45, 3.35 -0.5 C4 -0.55, 5.47 0.24, 5.44 0.48 M3.51 1.06 C4.3 1.13, 6.55 -0.33, 7.43 0.12 C8.32 0.57, 9.01 2.86, 8.83 3.77 C8.64 4.69, 7.24 4.88, 6.32 5.61 C5.39 6.34, 3.97 8.02, 3.27 8.16 C2.58 8.29, 2.45 7.07, 2.13 6.43 C1.8 5.79, 1.47 4.81, 1.32 4.32 C1.18 3.83, 1.26 4.04, 1.27 3.49 C1.27 2.94, 0.99 1.8, 1.37 1.03 C1.75 0.26, 2.99 -0.99, 3.53 -1.12 C4.06 -1.24, 4.24 -0.1, 4.61 0.29" stroke="none" stroke-width="0" fill="000000"></path><path d="M6.17 0.66 C6.91 1.06, 6.73 2.03, 7.04 2.82 C7.35 3.6, 8.34 4.79, 8.04 5.38 C7.74 5.97, 6.14 5.96, 5.24 6.36 C4.35 6.76, 3.44 7.76, 2.65 7.78 C1.86 7.79, 1.03 7, 0.5 6.45 C-0.03 5.89, -0.57 5.32, -0.52 4.45 C-0.47 3.57, 0.27 2.06, 0.83 1.19 C1.39 0.31, 1.94 -0.64, 2.82 -0.8 C3.7 -0.96, 5.36 0, 6.12 0.23 C6.87 0.46, 7.3 0.59, 7.36 0.57 M3.18 -1.13 C3.95 -1.1, 4.56 0.11, 5.49 0.68 C6.42 1.26, 8.34 1.43, 8.77 2.3 C9.2 3.17, 8.69 4.83, 8.08 5.89 C7.47 6.96, 6.04 8.42, 5.11 8.69 C4.18 8.97, 3.21 7.95, 2.49 7.54 C1.77 7.13, 0.97 7.25, 0.78 6.25 C0.6 5.25, 1.41 2.31, 1.37 1.56 C1.34 0.82, -0.04 2, 0.56 1.78 C1.17 1.55, 4.57 0.72, 4.99 0.19 C5.41 -0.34, 3.19 -1.61, 3.1 -1.4" stroke="#000000" stroke-width="2" fill="none"></path></g><g stroke-linecap="round" transform="translate(825.7245897379687 177.00226935153069) rotate(0 3.8095310756137906 3.8095310756138474)"><path d="M4.54 0.51 C5.32 0.37, 5.19 0.05, 5.81 0.65 C6.44 1.25, 8.25 3.08, 8.3 4.12 C8.34 5.17, 6.69 6.39, 6.1 6.9 C5.5 7.41, 5.47 7.29, 4.74 7.19 C4.01 7.1, 2.4 6.63, 1.73 6.33 C1.06 6.03, 1.06 5.87, 0.72 5.38 C0.38 4.89, -0.4 4.08, -0.33 3.41 C-0.27 2.73, 0.16 1.93, 1.11 1.34 C2.07 0.75, 4.65 0.1, 5.41 -0.14 C6.16 -0.39, 5.89 -0.28, 5.65 -0.14 M3.96 -0.81 C4.68 -0.91, 6.78 0.4, 7.56 1.24 C8.35 2.09, 8.96 3.38, 8.67 4.27 C8.38 5.16, 6.44 6.17, 5.82 6.57 C5.2 6.97, 5.63 6.45, 4.94 6.69 C4.24 6.92, 2.58 8.26, 1.64 7.99 C0.7 7.72, -0.41 5.95, -0.71 5.06 C-1.01 4.18, -0.43 3.28, -0.14 2.69 C0.14 2.09, 0.07 1.7, 1 1.49 C1.94 1.27, 4.89 1.43, 5.46 1.39 C6.03 1.34, 4.5 1.22, 4.42 1.22" stroke="none" stroke-width="0" fill="000000"></path><path d="M3.06 -0.54 C3.63 -0.76, 4.54 -0.68, 5.12 -0.35 C5.69 -0.01, 6.16 0.5, 6.5 1.49 C6.83 2.47, 7.38 4.54, 7.15 5.58 C6.92 6.62, 5.76 7.36, 5.13 7.74 C4.5 8.13, 3.88 8.04, 3.34 7.88 C2.8 7.71, 2.41 7.32, 1.89 6.75 C1.37 6.18, 0.33 5.16, 0.23 4.44 C0.13 3.72, 0.88 3.29, 1.31 2.43 C1.74 1.56, 2.18 -0.45, 2.81 -0.76 C3.44 -1.06, 4.94 0.54, 5.09 0.61 M2.56 1.62 C3.07 0.92, 3.1 -0.32, 3.94 -0.56 C4.78 -0.81, 7.15 -0.53, 7.58 0.17 C8.01 0.87, 6.8 2.41, 6.5 3.66 C6.2 4.91, 6 7.21, 5.79 7.69 C5.59 8.18, 5.78 6.58, 5.27 6.58 C4.77 6.58, 3.63 7.83, 2.75 7.71 C1.87 7.59, 0.51 6.79, -0.02 5.86 C-0.54 4.92, -0.77 2.84, -0.41 2.1 C-0.04 1.36, 1.9 1.5, 2.17 1.41 C2.44 1.32, 1.41 1.83, 1.24 1.57" stroke="#000000" stroke-width="2" fill="none"></path></g><g stroke-linecap="round" transform="translate(843.3036096771264 175.90450918590386) rotate(0 3.8095310756137906 3.8095310756138474)"><path d="M2.79 -0.46 C3.3 -0.75, 3.56 0.61, 4.35 0.85 C5.14 1.08, 6.85 0.38, 7.52 0.96 C8.2 1.54, 8.64 3.44, 8.4 4.32 C8.16 5.2, 6.97 5.6, 6.09 6.22 C5.22 6.84, 3.89 8, 3.14 8.03 C2.4 8.05, 1.99 7.03, 1.6 6.37 C1.21 5.71, 0.88 4.69, 0.8 4.07 C0.72 3.46, 0.8 3.28, 1.13 2.67 C1.45 2.06, 2.39 0.95, 2.74 0.38 C3.09 -0.18, 3.13 -0.73, 3.22 -0.69 M4.75 1.56 C5.37 2.07, 6.93 3.05, 7.53 3.42 C8.13 3.79, 8.51 3.18, 8.35 3.78 C8.18 4.39, 7.49 6.53, 6.53 7.06 C5.56 7.58, 3.35 7.23, 2.54 6.93 C1.74 6.63, 2.19 5.8, 1.69 5.27 C1.19 4.74, -0.26 4.45, -0.45 3.73 C-0.64 3.02, -0.15 1.33, 0.54 0.96 C1.24 0.59, 2.65 1.54, 3.72 1.51 C4.78 1.49, 6.45 0.83, 6.95 0.82 C7.44 0.81, 6.76 1.46, 6.67 1.46" stroke="none" stroke-width="0" fill="000000"></path><path d="M4.48 -0.07 C5.25 -0.01, 6.03 0.46, 6.42 1.17 C6.81 1.87, 6.74 3.38, 6.83 4.17 C6.93 4.96, 7.38 5.34, 7 5.9 C6.63 6.45, 5.48 7.23, 4.58 7.5 C3.68 7.76, 2.32 7.96, 1.61 7.49 C0.9 7.02, 0.44 5.48, 0.31 4.65 C0.18 3.82, 0.33 3.36, 0.84 2.49 C1.35 1.62, 2.7 -0.34, 3.35 -0.56 C4.01 -0.77, 4.27 0.95, 4.77 1.18 C5.27 1.4, 6.38 1.03, 6.35 0.8 M5.31 -0.61 C5.8 -0.55, 5.7 1.42, 5.88 1.92 C6.05 2.42, 6.21 1.84, 6.34 2.38 C6.47 2.91, 6.71 4.13, 6.68 5.13 C6.64 6.14, 7.06 7.88, 6.14 8.39 C5.23 8.9, 2.05 8.72, 1.19 8.18 C0.33 7.65, 1.04 6.2, 1 5.19 C0.96 4.18, 0.91 2.86, 0.97 2.14 C1.02 1.41, 0.63 1.35, 1.32 0.86 C2.01 0.38, 4.46 -0.47, 5.11 -0.77 C5.75 -1.07, 5.51 -0.99, 5.19 -0.92" stroke="#000000" stroke-width="2" fill="none"></path></g><g transform="translate(743.361052458237 140.69363343412977) rotate(0 19 11.5)"><text x="0" y="18" font-family="Helvetica, Segoe UI Emoji" font-size="20px" fill="#000000" text-anchor="start" style="white-space: pre;" direction="ltr">0.11</text></g><g transform="translate(807.3388995182231 387.3609208359322) rotate(0 20 11.5)"><text x="0" y="18" font-family="Helvetica, Segoe UI Emoji" font-size="20px" fill="#000000" text-anchor="start" style="white-space: pre;" direction="ltr">0.55</text></g><g transform="translate(884.3733291802666 140.12812178630065) rotate(0 20 11.5)"><text x="0" y="18" font-family="Helvetica, Segoe UI Emoji" font-size="20px" fill="#000000" text-anchor="start" style="white-space: pre;" direction="ltr">0.28</text></g><g transform="translate(748.1155290723825 170.8618770765887) rotate(0.2716758202664933 11.5 11.5)"><text x="0" y="18" font-family="Helvetica, Segoe UI Emoji" font-size="20px" fill="#000000" text-anchor="start" style="white-space: pre;" direction="ltr">....</text></g><g transform="translate(812.0933761323686 417.5291644783911) rotate(0.2716758202664933 11.5 11.5)"><text x="0" y="18" font-family="Helvetica, Segoe UI Emoji" font-size="20px" fill="#000000" text-anchor="start" style="white-space: pre;" direction="ltr">....</text></g><g transform="translate(889.1278057944121 170.2963654287596) rotate(0.2716758202664933 11.5 11.5)"><text x="0" y="18" font-family="Helvetica, Segoe UI Emoji" font-size="20px" fill="#000000" text-anchor="start" style="white-space: pre;" direction="ltr">....</text></g><g transform="translate(740.1914013821402 206.78459733319312) rotate(0 20 11.5)"><text x="0" y="18" font-family="Helvetica, Segoe UI Emoji" font-size="20px" fill="#000000" text-anchor="start" style="white-space: pre;" direction="ltr">0.72</text></g><g transform="translate(804.1692484421262 453.45188473499553) rotate(0 20 11.5)"><text x="0" y="18" font-family="Helvetica, Segoe UI Emoji" font-size="20px" fill="#000000" text-anchor="start" style="white-space: pre;" direction="ltr">0.96</text></g><g transform="translate(881.2036781041697 206.219085685364) rotate(0 20 11.5)"><text x="0" y="18" font-family="Helvetica, Segoe UI Emoji" font-size="20px" fill="#000000" text-anchor="start" style="white-space: pre;" direction="ltr">0.34</text></g><g transform="translate(727.5127970777526 251.1597698320312) rotate(0 34.921294223157474 18.575156501679487)"><text x="0" y="26.150313003358974" font-family="Virgil, Segoe UI Emoji" font-size="29.720250402687178px" fill="#000000" text-anchor="start" style="white-space: pre;" direction="ltr">most</text></g><g transform="translate(874.3360362859103 259.0467013579914) rotate(0 32.5 18.5)"><text x="0" y="26" font-family="Virgil, Segoe UI Emoji" font-size="29.720250402687178px" fill="#000000" text-anchor="start" style="white-space: pre;" direction="ltr">food</text></g><g stroke-linecap="round" transform="translate(652.0482750290653 54.027898043181835) rotate(0 188.07852502466915 288.96834003430234)"><path d="M0.68 0.93 C146.17 0.64, 292.5 0.05, 377.34 -1.13 M0.27 0.43 C130.25 -1.75, 260.57 -1.19, 376.09 -0.06 M375.82 0.8 C376.53 147.6, 376.32 294.74, 375.38 578.04 M375.77 0.02 C374.89 209.79, 374.17 420.59, 376.46 577.57 M376.63 577.84 C257.38 576.32, 136.53 576.18, -0.89 578.94 M376.75 578.25 C292.07 577.55, 208.82 577.51, -0.38 578.5 M0.3 577.91 C2.12 438.65, 2.66 299.74, 0.38 -0.11 M0.06 577.87 C0.69 427.85, 0.19 278.03, 0.39 -0.12" stroke="#000000" stroke-width="2" fill="none"></path></g><g transform="translate(1119.568222557745 337.895485013133) rotate(0 84 12.5)"><text x="84" y="18" font-family="Virgil, Segoe UI Emoji" font-size="20px" fill="#000000" text-anchor="middle" style="white-space: pre;" direction="ltr">Most microbats...</text></g><g stroke-linecap="round"><g transform="translate(1294.9020475463583 315.47006916072587) rotate(0 0.016902400086109992 26.120304160057856)"><path d="M0.98 -0.39 C0.93 8.4, -0.82 44.84, -1.08 53.88 M0.04 -1.64 C0.4 7.36, 1.34 43.4, 1.06 52.35" stroke="#000000" stroke-width="1" fill="none"></path></g></g><g stroke-linecap="round"><g transform="translate(1118.832644229994 334.0036991495107) rotate(0 166.92543376833146 0.4804521431425428)"><path d="M0.94 -0.79 C56.41 -0.92, 278.27 -0.18, 333.88 0.02 M-0.03 1.41 C55.14 1.42, 276.97 2.09, 332.63 1.56" stroke="#000000" stroke-width="1" fill="none"></path></g></g><g stroke-linecap="round"><g transform="translate(1352.6413993947808 309.76740624936826) rotate(0 0.03314729335716038 29.19689163102248)"><path d="M-0.84 0.06 C-0.9 9.76, -0.24 49.49, 0.06 59.35 M0.92 -0.96 C0.67 8.89, -0.82 47.19, -0.86 57.46" stroke="#000000" stroke-width="1" fill="none"></path></g></g><g stroke-linecap="round"><g transform="translate(1386.857376862926 306.9160911091076) rotate(0 0.3004985167783616 27.155084824428258)"><path d="M1.13 -0.88 C1.1 7.93, 0.36 44.57, 0.28 53.58 M0.26 1.28 C0.06 10.24, -0.51 46.34, -0.53 55.19" stroke="#000000" stroke-width="1" fill="none"></path></g></g><g stroke-linecap="round"><g transform="translate(1450.2993589918701 314.7572485333699) rotate(0 0.3542860669036827 24.85532761878281)"><path d="M-0.59 0.37 C-0.46 8.62, 0.92 41.83, 1.17 50.19 M1.3 -0.48 C1.33 7.35, 0.95 40.14, 0.84 48.33" stroke="#000000" stroke-width="1" fill="none"></path></g></g><g transform="translate(1298.4896882175979 310.7470581865349) rotate(0 23.5 12.5)"><text x="23.5" y="18" font-family="Virgil, Segoe UI Emoji" font-size="20px" fill="#000000" text-anchor="middle" style="white-space: pre;" direction="ltr">most</text></g><g transform="translate(1396.2218760366288 310.7470581865348) rotate(0 22 12.5)"><text x="22" y="18" font-family="Virgil, Segoe UI Emoji" font-size="20px" fill="#000000" text-anchor="middle" style="white-space: pre;" direction="ltr">food</text></g><g transform="translate(1357.6851544687268 308.60856367363044) rotate(0 8.5 12.5)"><text x="8.5" y="18" font-family="Virgil, Segoe UI Emoji" font-size="20px" fill="#000000" text-anchor="middle" style="white-space: pre;" direction="ltr">...</text></g><g transform="translate(1356.2595784756884 337.121845599582) rotate(0 8.5 12.5)"><text x="8.5" y="18" font-family="Virgil, Segoe UI Emoji" font-size="20px" fill="#000000" text-anchor="middle" style="white-space: pre;" direction="ltr">...</text></g><g transform="translate(1302.8019467915462 343.44453458507235) rotate(0 14.5 11.5)"><text x="14.5" y="18" font-family="Helvetica, Segoe UI Emoji" font-size="20px" fill="#000000" text-anchor="middle" style="white-space: pre;" direction="ltr">.08</text></g><g transform="translate(1399.6784755689764 342.01460850273463) rotate(0 14.5 11.5)"><text x="14.5" y="18" font-family="Helvetica, Segoe UI Emoji" font-size="20px" fill="#000000" text-anchor="middle" style="white-space: pre;" direction="ltr">.73</text></g><g stroke-linecap="round"><g transform="translate(1053.0175881990187 333.651945706191) rotate(0 23.999761654509143 -0.434224009080026)"><path d="M1.15 0.85 C8.88 0.97, 37.77 0.46, 45.43 0.25 M0.29 0.25 C8.49 0.03, 39.96 -1.81, 47.71 -1.74" stroke="#000000" stroke-width="1" fill="none"></path></g><g transform="translate(1053.0175881990187 333.651945706191) rotate(0 23.999761654509143 -0.434224009080026)"><path d="M27.64 5.44 C31.37 6.14, 34.95 3.96, 48.68 -1.89 M26.32 6.24 C33.12 3.88, 40.25 1.98, 46.87 -1.63" stroke="#000000" stroke-width="1" fill="none"></path></g><g transform="translate(1053.0175881990187 333.651945706191) rotate(0 23.999761654509143 -0.434224009080026)"><path d="M27.18 -10.37 C30.95 -6.22, 34.63 -4.95, 48.68 -1.89 M25.85 -9.57 C32.86 -6.53, 40.14 -3.02, 46.87 -1.63" stroke="#000000" stroke-width="1" fill="none"></path></g></g><g transform="translate(313.9861396328182 515.6677637262858) rotate(0 162.87565097827456 51.04476143099038)"><text x="162.87565097827462" y="20.522380715495185" font-family="Helvetica, Segoe UI Emoji" font-size="22.186543984401645px" fill="#000000" text-anchor="middle" style="white-space: pre;" direction="ltr">We use the CountVectorizer </text><text x="162.87565097827462" y="46.04476143099037" font-family="Helvetica, Segoe UI Emoji" font-size="22.186543984401645px" fill="#000000" text-anchor="middle" style="white-space: pre;" direction="ltr">from Scikit-Learn to tokenize our </text><text x="162.87565097827462" y="71.56714214648555" font-family="Helvetica, Segoe UI Emoji" font-size="22.186543984401645px" fill="#000000" text-anchor="middle" style="white-space: pre;" direction="ltr">document into candidate </text><text x="162.87565097827462" y="97.08952286198074" font-family="Helvetica, Segoe UI Emoji" font-size="22.186543984401645px" fill="#000000" text-anchor="middle" style="white-space: pre;" direction="ltr">kewords/keyphrases.</text></g><g transform="translate(620.6050791973807 641.0054631240885) rotate(0 216.3264033436417 38.01084089759081)"><text x="216.32640334364172" y="20.340560598393875" font-family="Helvetica, Segoe UI Emoji" font-size="22.359318175053414px" fill="#000000" text-anchor="middle" style="white-space: pre;" direction="ltr">We can use any language model that </text><text x="216.32640334364172" y="45.68112119678775" font-family="Helvetica, Segoe UI Emoji" font-size="22.359318175053414px" fill="#000000" text-anchor="middle" style="white-space: pre;" direction="ltr">can embed both documents and keywords, </text><text x="216.32640334364172" y="71.02168179518162" font-family="Helvetica, Segoe UI Emoji" font-size="22.359318175053414px" fill="#000000" text-anchor="middle" style="white-space: pre;" direction="ltr">like sentence-transformers.</text></g><g transform="translate(1079.2086356411555 400.35702887221646) rotate(0 230 52)"><text x="230" y="21" font-family="Helvetica, Segoe UI Emoji" font-size="22.64449030606811px" fill="#000000" text-anchor="middle" style="white-space: pre;" direction="ltr">We calculate the cosine similarity between all </text><text x="230" y="47" font-family="Helvetica, Segoe UI Emoji" font-size="22.64449030606811px" fill="#000000" text-anchor="middle" style="white-space: pre;" direction="ltr">candidate keywords and the input document. </text><text x="230" y="73" font-family="Helvetica, Segoe UI Emoji" font-size="22.64449030606811px" fill="#000000" text-anchor="middle" style="white-space: pre;" direction="ltr">The keywords that have the largest similarity </text><text x="230" y="99" font-family="Helvetica, Segoe UI Emoji" font-size="22.64449030606811px" fill="#000000" text-anchor="middle" style="white-space: pre;" direction="ltr">to the document are extracted.</text></g><g stroke-linecap="round" transform="translate(676.3524188243175 364.5585451585322) rotate(0 163.3896911287635 121.2245815286364)"><path d="M0.16 1.16 C115.34 1.15, 227.33 0.68, 327.88 -0.85 M327.13 1.01 C326.07 76.25, 327.44 152.28, 326.5 241.18 M328 242.65 C196.97 244.24, 68.6 243.66, -1.19 241.59 M0.14 242.09 C1.26 185.76, 0.43 131.57, 0.61 0.29" stroke="#c92a2a" stroke-width="2.5" fill="none" stroke-dasharray="1.5 8"></path></g><g stroke-linecap="round" transform="translate(680.0276572097087 121.56808057417686) rotate(0 158.11908861932397 85.9113130957029)"><path d="M1.3 -0.92 C118.12 0.01, 236.59 0.18, 315.91 0.78 M315.44 0.71 C315.36 45.71, 313.59 94.3, 318.23 170.7 M316.7 171.87 C221.7 171.25, 128.37 173.25, 0.39 172.43 M-1.78 172.51 C1.19 119.88, 2.01 63.49, -1.15 1.63" stroke="#c92a2a" stroke-width="2.5" fill="none" stroke-dasharray="1.5 8"></path></g></svg>
</div>






              <details class="quote">
                <summary>Source code in <code>keybert\_model.py</code></summary>
                <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"> 23</span>
<span class="normal"> 24</span>
<span class="normal"> 25</span>
<span class="normal"> 26</span>
<span class="normal"> 27</span>
<span class="normal"> 28</span>
<span class="normal"> 29</span>
<span class="normal"> 30</span>
<span class="normal"> 31</span>
<span class="normal"> 32</span>
<span class="normal"> 33</span>
<span class="normal"> 34</span>
<span class="normal"> 35</span>
<span class="normal"> 36</span>
<span class="normal"> 37</span>
<span class="normal"> 38</span>
<span class="normal"> 39</span>
<span class="normal"> 40</span>
<span class="normal"> 41</span>
<span class="normal"> 42</span>
<span class="normal"> 43</span>
<span class="normal"> 44</span>
<span class="normal"> 45</span>
<span class="normal"> 46</span>
<span class="normal"> 47</span>
<span class="normal"> 48</span>
<span class="normal"> 49</span>
<span class="normal"> 50</span>
<span class="normal"> 51</span>
<span class="normal"> 52</span>
<span class="normal"> 53</span>
<span class="normal"> 54</span>
<span class="normal"> 55</span>
<span class="normal"> 56</span>
<span class="normal"> 57</span>
<span class="normal"> 58</span>
<span class="normal"> 59</span>
<span class="normal"> 60</span>
<span class="normal"> 61</span>
<span class="normal"> 62</span>
<span class="normal"> 63</span>
<span class="normal"> 64</span>
<span class="normal"> 65</span>
<span class="normal"> 66</span>
<span class="normal"> 67</span>
<span class="normal"> 68</span>
<span class="normal"> 69</span>
<span class="normal"> 70</span>
<span class="normal"> 71</span>
<span class="normal"> 72</span>
<span class="normal"> 73</span>
<span class="normal"> 74</span>
<span class="normal"> 75</span>
<span class="normal"> 76</span>
<span class="normal"> 77</span>
<span class="normal"> 78</span>
<span class="normal"> 79</span>
<span class="normal"> 80</span>
<span class="normal"> 81</span>
<span class="normal"> 82</span>
<span class="normal"> 83</span>
<span class="normal"> 84</span>
<span class="normal"> 85</span>
<span class="normal"> 86</span>
<span class="normal"> 87</span>
<span class="normal"> 88</span>
<span class="normal"> 89</span>
<span class="normal"> 90</span>
<span class="normal"> 91</span>
<span class="normal"> 92</span>
<span class="normal"> 93</span>
<span class="normal"> 94</span>
<span class="normal"> 95</span>
<span class="normal"> 96</span>
<span class="normal"> 97</span>
<span class="normal"> 98</span>
<span class="normal"> 99</span>
<span class="normal">100</span>
<span class="normal">101</span>
<span class="normal">102</span>
<span class="normal">103</span>
<span class="normal">104</span>
<span class="normal">105</span>
<span class="normal">106</span>
<span class="normal">107</span>
<span class="normal">108</span>
<span class="normal">109</span>
<span class="normal">110</span>
<span class="normal">111</span>
<span class="normal">112</span>
<span class="normal">113</span>
<span class="normal">114</span>
<span class="normal">115</span>
<span class="normal">116</span>
<span class="normal">117</span>
<span class="normal">118</span>
<span class="normal">119</span>
<span class="normal">120</span>
<span class="normal">121</span>
<span class="normal">122</span>
<span class="normal">123</span>
<span class="normal">124</span>
<span class="normal">125</span>
<span class="normal">126</span>
<span class="normal">127</span>
<span class="normal">128</span>
<span class="normal">129</span>
<span class="normal">130</span>
<span class="normal">131</span>
<span class="normal">132</span>
<span class="normal">133</span>
<span class="normal">134</span>
<span class="normal">135</span>
<span class="normal">136</span>
<span class="normal">137</span>
<span class="normal">138</span>
<span class="normal">139</span>
<span class="normal">140</span>
<span class="normal">141</span>
<span class="normal">142</span>
<span class="normal">143</span>
<span class="normal">144</span>
<span class="normal">145</span>
<span class="normal">146</span>
<span class="normal">147</span>
<span class="normal">148</span>
<span class="normal">149</span>
<span class="normal">150</span>
<span class="normal">151</span>
<span class="normal">152</span>
<span class="normal">153</span>
<span class="normal">154</span>
<span class="normal">155</span>
<span class="normal">156</span>
<span class="normal">157</span>
<span class="normal">158</span>
<span class="normal">159</span>
<span class="normal">160</span>
<span class="normal">161</span>
<span class="normal">162</span>
<span class="normal">163</span>
<span class="normal">164</span>
<span class="normal">165</span>
<span class="normal">166</span>
<span class="normal">167</span>
<span class="normal">168</span>
<span class="normal">169</span>
<span class="normal">170</span>
<span class="normal">171</span>
<span class="normal">172</span>
<span class="normal">173</span>
<span class="normal">174</span>
<span class="normal">175</span>
<span class="normal">176</span>
<span class="normal">177</span>
<span class="normal">178</span>
<span class="normal">179</span>
<span class="normal">180</span>
<span class="normal">181</span>
<span class="normal">182</span>
<span class="normal">183</span>
<span class="normal">184</span>
<span class="normal">185</span>
<span class="normal">186</span>
<span class="normal">187</span>
<span class="normal">188</span>
<span class="normal">189</span>
<span class="normal">190</span>
<span class="normal">191</span>
<span class="normal">192</span>
<span class="normal">193</span>
<span class="normal">194</span>
<span class="normal">195</span>
<span class="normal">196</span>
<span class="normal">197</span>
<span class="normal">198</span>
<span class="normal">199</span>
<span class="normal">200</span>
<span class="normal">201</span>
<span class="normal">202</span>
<span class="normal">203</span>
<span class="normal">204</span>
<span class="normal">205</span>
<span class="normal">206</span>
<span class="normal">207</span>
<span class="normal">208</span>
<span class="normal">209</span>
<span class="normal">210</span>
<span class="normal">211</span>
<span class="normal">212</span>
<span class="normal">213</span>
<span class="normal">214</span>
<span class="normal">215</span>
<span class="normal">216</span>
<span class="normal">217</span>
<span class="normal">218</span>
<span class="normal">219</span>
<span class="normal">220</span>
<span class="normal">221</span>
<span class="normal">222</span>
<span class="normal">223</span>
<span class="normal">224</span>
<span class="normal">225</span>
<span class="normal">226</span>
<span class="normal">227</span>
<span class="normal">228</span>
<span class="normal">229</span>
<span class="normal">230</span>
<span class="normal">231</span>
<span class="normal">232</span>
<span class="normal">233</span>
<span class="normal">234</span>
<span class="normal">235</span>
<span class="normal">236</span>
<span class="normal">237</span>
<span class="normal">238</span>
<span class="normal">239</span>
<span class="normal">240</span>
<span class="normal">241</span>
<span class="normal">242</span>
<span class="normal">243</span>
<span class="normal">244</span>
<span class="normal">245</span>
<span class="normal">246</span>
<span class="normal">247</span>
<span class="normal">248</span>
<span class="normal">249</span>
<span class="normal">250</span>
<span class="normal">251</span>
<span class="normal">252</span>
<span class="normal">253</span>
<span class="normal">254</span>
<span class="normal">255</span>
<span class="normal">256</span>
<span class="normal">257</span>
<span class="normal">258</span>
<span class="normal">259</span>
<span class="normal">260</span>
<span class="normal">261</span>
<span class="normal">262</span>
<span class="normal">263</span>
<span class="normal">264</span>
<span class="normal">265</span>
<span class="normal">266</span>
<span class="normal">267</span>
<span class="normal">268</span>
<span class="normal">269</span>
<span class="normal">270</span>
<span class="normal">271</span>
<span class="normal">272</span>
<span class="normal">273</span>
<span class="normal">274</span>
<span class="normal">275</span>
<span class="normal">276</span>
<span class="normal">277</span>
<span class="normal">278</span>
<span class="normal">279</span>
<span class="normal">280</span>
<span class="normal">281</span>
<span class="normal">282</span>
<span class="normal">283</span>
<span class="normal">284</span>
<span class="normal">285</span>
<span class="normal">286</span>
<span class="normal">287</span>
<span class="normal">288</span>
<span class="normal">289</span>
<span class="normal">290</span>
<span class="normal">291</span>
<span class="normal">292</span>
<span class="normal">293</span>
<span class="normal">294</span>
<span class="normal">295</span>
<span class="normal">296</span>
<span class="normal">297</span>
<span class="normal">298</span>
<span class="normal">299</span>
<span class="normal">300</span>
<span class="normal">301</span>
<span class="normal">302</span>
<span class="normal">303</span>
<span class="normal">304</span>
<span class="normal">305</span>
<span class="normal">306</span>
<span class="normal">307</span>
<span class="normal">308</span>
<span class="normal">309</span>
<span class="normal">310</span>
<span class="normal">311</span>
<span class="normal">312</span>
<span class="normal">313</span>
<span class="normal">314</span>
<span class="normal">315</span>
<span class="normal">316</span>
<span class="normal">317</span>
<span class="normal">318</span>
<span class="normal">319</span>
<span class="normal">320</span>
<span class="normal">321</span>
<span class="normal">322</span>
<span class="normal">323</span>
<span class="normal">324</span>
<span class="normal">325</span>
<span class="normal">326</span>
<span class="normal">327</span>
<span class="normal">328</span>
<span class="normal">329</span>
<span class="normal">330</span>
<span class="normal">331</span>
<span class="normal">332</span>
<span class="normal">333</span>
<span class="normal">334</span>
<span class="normal">335</span>
<span class="normal">336</span>
<span class="normal">337</span>
<span class="normal">338</span>
<span class="normal">339</span>
<span class="normal">340</span>
<span class="normal">341</span>
<span class="normal">342</span>
<span class="normal">343</span>
<span class="normal">344</span>
<span class="normal">345</span>
<span class="normal">346</span>
<span class="normal">347</span>
<span class="normal">348</span>
<span class="normal">349</span>
<span class="normal">350</span>
<span class="normal">351</span>
<span class="normal">352</span>
<span class="normal">353</span>
<span class="normal">354</span>
<span class="normal">355</span>
<span class="normal">356</span>
<span class="normal">357</span>
<span class="normal">358</span>
<span class="normal">359</span>
<span class="normal">360</span>
<span class="normal">361</span>
<span class="normal">362</span>
<span class="normal">363</span>
<span class="normal">364</span>
<span class="normal">365</span>
<span class="normal">366</span>
<span class="normal">367</span>
<span class="normal">368</span>
<span class="normal">369</span>
<span class="normal">370</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">class</span> <span class="nc">KeyBERT</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;A minimal method for keyword extraction with BERT.</span>

<span class="sd">    The keyword extraction is done by finding the sub-phrases in</span>
<span class="sd">    a document that are the most similar to the document itself.</span>

<span class="sd">    First, document embeddings are extracted with BERT to get a</span>
<span class="sd">    document-level representation. Then, word embeddings are extracted</span>
<span class="sd">    for N-gram words/phrases. Finally, we use cosine similarity to find the</span>
<span class="sd">    words/phrases that are the most similar to the document.</span>

<span class="sd">    The most similar words could then be identified as the words that</span>
<span class="sd">    best describe the entire document.</span>

<span class="sd">    &lt;div class=&quot;excalidraw&quot;&gt;</span>
<span class="sd">    --8&lt;-- &quot;docs/images/pipeline.svg&quot;</span>
<span class="sd">    &lt;/div&gt;</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">model</span><span class="o">=</span><span class="s2">&quot;all-MiniLM-L6-v2&quot;</span><span class="p">,</span>
        <span class="n">llm</span><span class="p">:</span> <span class="n">BaseLLM</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;KeyBERT initialization.</span>

<span class="sd">        Arguments:</span>
<span class="sd">            model: Use a custom embedding model or a specific KeyBERT Backend.</span>
<span class="sd">                   The following backends are currently supported:</span>
<span class="sd">                      * SentenceTransformers</span>
<span class="sd">                      * 🤗 Transformers</span>
<span class="sd">                      * Flair</span>
<span class="sd">                      * Spacy</span>
<span class="sd">                      * Gensim</span>
<span class="sd">                      * USE (TF-Hub)</span>
<span class="sd">                    You can also pass in a string that points to one of the following</span>
<span class="sd">                    sentence-transformers models:</span>
<span class="sd">                      * https://www.sbert.net/docs/pretrained_models.html</span>
<span class="sd">            llm: The Large Language Model used to extract keywords</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">model</span> <span class="o">=</span> <span class="n">select_backend</span><span class="p">(</span><span class="n">model</span><span class="p">)</span>

        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">llm</span><span class="p">,</span> <span class="n">BaseLLM</span><span class="p">):</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">llm</span> <span class="o">=</span> <span class="n">KeyLLM</span><span class="p">(</span><span class="n">llm</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">llm</span> <span class="o">=</span> <span class="n">llm</span>

    <span class="k">def</span> <span class="nf">extract_keywords</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">docs</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]],</span>
        <span class="n">candidates</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">keyphrase_ngram_range</span><span class="p">:</span> <span class="n">Tuple</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span>
        <span class="n">stop_words</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]]</span> <span class="o">=</span> <span class="s2">&quot;english&quot;</span><span class="p">,</span>
        <span class="n">top_n</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">5</span><span class="p">,</span>
        <span class="n">min_df</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span>
        <span class="n">use_maxsum</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">use_mmr</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">diversity</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.5</span><span class="p">,</span>
        <span class="n">nr_candidates</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">20</span><span class="p">,</span>
        <span class="n">vectorizer</span><span class="p">:</span> <span class="n">CountVectorizer</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">highlight</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">seed_keywords</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">],</span> <span class="n">List</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">doc_embeddings</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">word_embeddings</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">threshold</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Union</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="n">Tuple</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="nb">float</span><span class="p">]],</span> <span class="n">List</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="n">Tuple</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="nb">float</span><span class="p">]]]]:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Extract keywords and/or keyphrases.</span>

<span class="sd">        To get the biggest speed-up, make sure to pass multiple documents</span>
<span class="sd">        at once instead of iterating over a single document.</span>

<span class="sd">        Arguments:</span>
<span class="sd">            docs: The document(s) for which to extract keywords/keyphrases</span>
<span class="sd">            candidates: Candidate keywords/keyphrases to use instead of extracting them from the document(s)</span>
<span class="sd">                        NOTE: This is not used if you passed a `vectorizer`.</span>
<span class="sd">            keyphrase_ngram_range: Length, in words, of the extracted keywords/keyphrases.</span>
<span class="sd">                                   NOTE: This is not used if you passed a `vectorizer`.</span>
<span class="sd">            stop_words: Stopwords to remove from the document.</span>
<span class="sd">                        NOTE: This is not used if you passed a `vectorizer`.</span>
<span class="sd">            top_n: Return the top n keywords/keyphrases</span>
<span class="sd">            min_df: Minimum document frequency of a word across all documents</span>
<span class="sd">                    if keywords for multiple documents need to be extracted.</span>
<span class="sd">                    NOTE: This is not used if you passed a `vectorizer`.</span>
<span class="sd">            use_maxsum: Whether to use Max Sum Distance for the selection</span>
<span class="sd">                        of keywords/keyphrases.</span>
<span class="sd">            use_mmr: Whether to use Maximal Marginal Relevance (MMR) for the</span>
<span class="sd">                     selection of keywords/keyphrases.</span>
<span class="sd">            diversity: The diversity of the results between 0 and 1 if `use_mmr`</span>
<span class="sd">                       is set to True.</span>
<span class="sd">            nr_candidates: The number of candidates to consider if `use_maxsum` is</span>
<span class="sd">                           set to True.</span>
<span class="sd">            vectorizer: Pass in your own `CountVectorizer` from</span>
<span class="sd">                        `sklearn.feature_extraction.text.CountVectorizer`</span>
<span class="sd">            highlight: Whether to print the document and highlight its keywords/keyphrases.</span>
<span class="sd">                       NOTE: This does not work if multiple documents are passed.</span>
<span class="sd">            seed_keywords: Seed keywords that may guide the extraction of keywords by</span>
<span class="sd">                           steering the similarities towards the seeded keywords.</span>
<span class="sd">                           NOTE: when multiple documents are passed,</span>
<span class="sd">                           `seed_keywords`funtions in either of the two ways below:</span>
<span class="sd">                           - globally: when a flat list of str is passed, keywords are shared by all documents,</span>
<span class="sd">                           - locally: when a nested list of str is passed, keywords differs among documents.</span>
<span class="sd">            doc_embeddings: The embeddings of each document.</span>
<span class="sd">            word_embeddings: The embeddings of each potential keyword/keyphrase across</span>
<span class="sd">                             across the vocabulary of the set of input documents.</span>
<span class="sd">                             NOTE: The `word_embeddings` should be generated through</span>
<span class="sd">                             `.extract_embeddings` as the order of these embeddings depend</span>
<span class="sd">                             on the vectorizer that was used to generate its vocabulary.</span>
<span class="sd">            threshold: Minimum similarity value between 0 and 1 used to decide how similar documents need to receive the same keywords.</span>

<span class="sd">        Returns:</span>
<span class="sd">            keywords: The top n keywords for a document with their respective distances</span>
<span class="sd">                      to the input document.</span>

<span class="sd">        Usage:</span>

<span class="sd">        To extract keywords from a single document:</span>

<span class="sd">        ```python</span>
<span class="sd">        from keybert import KeyBERT</span>

<span class="sd">        kw_model = KeyBERT()</span>
<span class="sd">        keywords = kw_model.extract_keywords(doc)</span>
<span class="sd">        ```</span>

<span class="sd">        To extract keywords from multiple documents, which is typically quite a bit faster:</span>

<span class="sd">        ```python</span>
<span class="sd">        from keybert import KeyBERT</span>

<span class="sd">        kw_model = KeyBERT()</span>
<span class="sd">        keywords = kw_model.extract_keywords(docs)</span>
<span class="sd">        ```</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># Check for a single, empty document</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">docs</span><span class="p">,</span> <span class="nb">str</span><span class="p">):</span>
            <span class="k">if</span> <span class="n">docs</span><span class="p">:</span>
                <span class="n">docs</span> <span class="o">=</span> <span class="p">[</span><span class="n">docs</span><span class="p">]</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="k">return</span> <span class="p">[]</span>

        <span class="c1"># Extract potential words using a vectorizer / tokenizer</span>
        <span class="k">if</span> <span class="n">vectorizer</span><span class="p">:</span>
            <span class="n">count</span> <span class="o">=</span> <span class="n">vectorizer</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">docs</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">try</span><span class="p">:</span>
                <span class="n">count</span> <span class="o">=</span> <span class="n">CountVectorizer</span><span class="p">(</span>
                    <span class="n">ngram_range</span><span class="o">=</span><span class="n">keyphrase_ngram_range</span><span class="p">,</span>
                    <span class="n">stop_words</span><span class="o">=</span><span class="n">stop_words</span><span class="p">,</span>
                    <span class="n">min_df</span><span class="o">=</span><span class="n">min_df</span><span class="p">,</span>
                    <span class="n">vocabulary</span><span class="o">=</span><span class="n">candidates</span><span class="p">,</span>
                <span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">docs</span><span class="p">)</span>
            <span class="k">except</span> <span class="ne">ValueError</span><span class="p">:</span>
                <span class="k">return</span> <span class="p">[]</span>

        <span class="c1"># Scikit-Learn Deprecation: get_feature_names is deprecated in 1.0</span>
        <span class="c1"># and will be removed in 1.2. Please use get_feature_names_out instead.</span>
        <span class="k">if</span> <span class="n">version</span><span class="o">.</span><span class="n">parse</span><span class="p">(</span><span class="n">sklearn_version</span><span class="p">)</span> <span class="o">&gt;=</span> <span class="n">version</span><span class="o">.</span><span class="n">parse</span><span class="p">(</span><span class="s2">&quot;1.0.0&quot;</span><span class="p">):</span>
            <span class="n">words</span> <span class="o">=</span> <span class="n">count</span><span class="o">.</span><span class="n">get_feature_names_out</span><span class="p">()</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">words</span> <span class="o">=</span> <span class="n">count</span><span class="o">.</span><span class="n">get_feature_names</span><span class="p">()</span>
        <span class="n">df</span> <span class="o">=</span> <span class="n">count</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">docs</span><span class="p">)</span>

        <span class="c1"># Check if the right number of word embeddings are generated compared with the vectorizer</span>
        <span class="k">if</span> <span class="n">word_embeddings</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">word_embeddings</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">!=</span> <span class="nb">len</span><span class="p">(</span><span class="n">words</span><span class="p">):</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                    <span class="s2">&quot;Make sure that the `word_embeddings` are generated from the function &quot;</span>
                    <span class="s2">&quot;`.extract_embeddings`. </span><span class="se">\n</span><span class="s2">Moreover, the `candidates`, `keyphrase_ngram_range`,&quot;</span>
                    <span class="s2">&quot;`stop_words`, and `min_df` parameters need to have the same values in both &quot;</span>
                    <span class="s2">&quot;`.extract_embeddings` and `.extract_keywords`.&quot;</span>
                <span class="p">)</span>

        <span class="c1"># Extract embeddings</span>
        <span class="k">if</span> <span class="n">doc_embeddings</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">doc_embeddings</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">embed</span><span class="p">(</span><span class="n">docs</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">word_embeddings</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">word_embeddings</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">embed</span><span class="p">(</span><span class="n">words</span><span class="p">)</span>

        <span class="c1"># Guided KeyBERT either local (keywords shared among documents) or global (keywords per document)</span>
        <span class="k">if</span> <span class="n">seed_keywords</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">seed_keywords</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="nb">str</span><span class="p">):</span>
                <span class="n">seed_embeddings</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">embed</span><span class="p">(</span><span class="n">seed_keywords</span><span class="p">)</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">keepdims</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
            <span class="k">elif</span> <span class="nb">len</span><span class="p">(</span><span class="n">docs</span><span class="p">)</span> <span class="o">!=</span> <span class="nb">len</span><span class="p">(</span><span class="n">seed_keywords</span><span class="p">):</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;The length of docs must match the length of seed_keywords&quot;</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">seed_embeddings</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">vstack</span><span class="p">(</span>
                    <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">embed</span><span class="p">(</span><span class="n">keywords</span><span class="p">)</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">keepdims</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span> <span class="k">for</span> <span class="n">keywords</span> <span class="ow">in</span> <span class="n">seed_keywords</span><span class="p">]</span>
                <span class="p">)</span>
            <span class="n">doc_embeddings</span> <span class="o">=</span> <span class="p">(</span><span class="n">doc_embeddings</span> <span class="o">*</span> <span class="mi">3</span> <span class="o">+</span> <span class="n">seed_embeddings</span><span class="p">)</span> <span class="o">/</span> <span class="mi">4</span>

        <span class="c1"># Find keywords</span>
        <span class="n">all_keywords</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">index</span><span class="p">,</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">docs</span><span class="p">):</span>
            <span class="k">try</span><span class="p">:</span>
                <span class="c1"># Select embeddings</span>
                <span class="n">candidate_indices</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="n">index</span><span class="p">]</span><span class="o">.</span><span class="n">nonzero</span><span class="p">()[</span><span class="mi">1</span><span class="p">]</span>
                <span class="n">candidates</span> <span class="o">=</span> <span class="p">[</span><span class="n">words</span><span class="p">[</span><span class="n">index</span><span class="p">]</span> <span class="k">for</span> <span class="n">index</span> <span class="ow">in</span> <span class="n">candidate_indices</span><span class="p">]</span>
                <span class="n">candidate_embeddings</span> <span class="o">=</span> <span class="n">word_embeddings</span><span class="p">[</span><span class="n">candidate_indices</span><span class="p">]</span>
                <span class="n">doc_embedding</span> <span class="o">=</span> <span class="n">doc_embeddings</span><span class="p">[</span><span class="n">index</span><span class="p">]</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>

                <span class="c1"># Maximal Marginal Relevance (MMR)</span>
                <span class="k">if</span> <span class="n">use_mmr</span><span class="p">:</span>
                    <span class="n">keywords</span> <span class="o">=</span> <span class="n">mmr</span><span class="p">(</span>
                        <span class="n">doc_embedding</span><span class="p">,</span>
                        <span class="n">candidate_embeddings</span><span class="p">,</span>
                        <span class="n">candidates</span><span class="p">,</span>
                        <span class="n">top_n</span><span class="p">,</span>
                        <span class="n">diversity</span><span class="p">,</span>
                    <span class="p">)</span>

                <span class="c1"># Max Sum Distance</span>
                <span class="k">elif</span> <span class="n">use_maxsum</span><span class="p">:</span>
                    <span class="n">keywords</span> <span class="o">=</span> <span class="n">max_sum_distance</span><span class="p">(</span>
                        <span class="n">doc_embedding</span><span class="p">,</span>
                        <span class="n">candidate_embeddings</span><span class="p">,</span>
                        <span class="n">candidates</span><span class="p">,</span>
                        <span class="n">top_n</span><span class="p">,</span>
                        <span class="n">nr_candidates</span><span class="p">,</span>
                    <span class="p">)</span>

                <span class="c1"># Cosine-based keyword extraction</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="n">distances</span> <span class="o">=</span> <span class="n">cosine_similarity</span><span class="p">(</span><span class="n">doc_embedding</span><span class="p">,</span> <span class="n">candidate_embeddings</span><span class="p">)</span>
                    <span class="n">keywords</span> <span class="o">=</span> <span class="p">[</span>
                        <span class="p">(</span><span class="n">candidates</span><span class="p">[</span><span class="n">index</span><span class="p">],</span> <span class="nb">round</span><span class="p">(</span><span class="nb">float</span><span class="p">(</span><span class="n">distances</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="n">index</span><span class="p">]),</span> <span class="mi">4</span><span class="p">))</span>
                        <span class="k">for</span> <span class="n">index</span> <span class="ow">in</span> <span class="n">distances</span><span class="o">.</span><span class="n">argsort</span><span class="p">()[</span><span class="mi">0</span><span class="p">][</span><span class="o">-</span><span class="n">top_n</span><span class="p">:]</span>
                    <span class="p">][::</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>

                <span class="n">all_keywords</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">keywords</span><span class="p">)</span>

            <span class="c1"># Capturing empty keywords</span>
            <span class="k">except</span> <span class="ne">ValueError</span><span class="p">:</span>
                <span class="n">all_keywords</span><span class="o">.</span><span class="n">append</span><span class="p">([])</span>

        <span class="c1"># Highlight keywords in the document</span>
        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">all_keywords</span><span class="p">)</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">highlight</span><span class="p">:</span>
                <span class="n">highlight_document</span><span class="p">(</span><span class="n">docs</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">all_keywords</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">count</span><span class="p">)</span>
            <span class="n">all_keywords</span> <span class="o">=</span> <span class="n">all_keywords</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>

        <span class="c1"># Fine-tune keywords using an LLM</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">llm</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="kn">import</span> <span class="nn">torch</span>

            <span class="n">doc_embeddings</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">from_numpy</span><span class="p">(</span><span class="n">doc_embeddings</span><span class="p">)</span><span class="o">.</span><span class="n">float</span><span class="p">()</span>
            <span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">is_available</span><span class="p">():</span>
                <span class="n">doc_embeddings</span> <span class="o">=</span> <span class="n">doc_embeddings</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="s2">&quot;cuda&quot;</span><span class="p">)</span>
            <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">all_keywords</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="nb">tuple</span><span class="p">):</span>
                <span class="n">candidate_keywords</span> <span class="o">=</span> <span class="p">[[</span><span class="n">keyword</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="k">for</span> <span class="n">keyword</span> <span class="ow">in</span> <span class="n">all_keywords</span><span class="p">]]</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">candidate_keywords</span> <span class="o">=</span> <span class="p">[[</span><span class="n">keyword</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="k">for</span> <span class="n">keyword</span> <span class="ow">in</span> <span class="n">keywords</span><span class="p">]</span> <span class="k">for</span> <span class="n">keywords</span> <span class="ow">in</span> <span class="n">all_keywords</span><span class="p">]</span>
            <span class="n">keywords</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">llm</span><span class="o">.</span><span class="n">extract_keywords</span><span class="p">(</span>
                <span class="n">docs</span><span class="p">,</span>
                <span class="n">embeddings</span><span class="o">=</span><span class="n">doc_embeddings</span><span class="p">,</span>
                <span class="n">candidate_keywords</span><span class="o">=</span><span class="n">candidate_keywords</span><span class="p">,</span>
                <span class="n">threshold</span><span class="o">=</span><span class="n">threshold</span><span class="p">,</span>
            <span class="p">)</span>
            <span class="k">return</span> <span class="n">keywords</span>
        <span class="k">return</span> <span class="n">all_keywords</span>

    <span class="k">def</span> <span class="nf">extract_embeddings</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">docs</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]],</span>
        <span class="n">candidates</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">keyphrase_ngram_range</span><span class="p">:</span> <span class="n">Tuple</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span>
        <span class="n">stop_words</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]]</span> <span class="o">=</span> <span class="s2">&quot;english&quot;</span><span class="p">,</span>
        <span class="n">min_df</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span>
        <span class="n">vectorizer</span><span class="p">:</span> <span class="n">CountVectorizer</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Union</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="n">Tuple</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="nb">float</span><span class="p">]],</span> <span class="n">List</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="n">Tuple</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="nb">float</span><span class="p">]]]]:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Extract document and word embeddings for the input documents and the</span>
<span class="sd">        generated candidate keywords/keyphrases respectively.</span>

<span class="sd">        Note that all potential keywords/keyphrases are not returned but only their</span>
<span class="sd">        word embeddings. This means that the values of `candidates`, `keyphrase_ngram_range`,</span>
<span class="sd">        `stop_words`, and `min_df` need to be the same between using `.extract_embeddings` and</span>
<span class="sd">        `.extract_keywords`.</span>

<span class="sd">        Arguments:</span>
<span class="sd">            docs: The document(s) for which to extract keywords/keyphrases</span>
<span class="sd">            candidates: Candidate keywords/keyphrases to use instead of extracting them from the document(s)</span>
<span class="sd">                        NOTE: This is not used if you passed a `vectorizer`.</span>
<span class="sd">            keyphrase_ngram_range: Length, in words, of the extracted keywords/keyphrases.</span>
<span class="sd">                                   NOTE: This is not used if you passed a `vectorizer`.</span>
<span class="sd">            stop_words: Stopwords to remove from the document.</span>
<span class="sd">                        NOTE: This is not used if you passed a `vectorizer`.</span>
<span class="sd">            min_df: Minimum document frequency of a word across all documents</span>
<span class="sd">                    if keywords for multiple documents need to be extracted.</span>
<span class="sd">                    NOTE: This is not used if you passed a `vectorizer`.</span>
<span class="sd">            vectorizer: Pass in your own `CountVectorizer` from</span>
<span class="sd">                        `sklearn.feature_extraction.text.CountVectorizer`</span>

<span class="sd">        Returns:</span>
<span class="sd">            doc_embeddings: The embeddings of each document.</span>
<span class="sd">            word_embeddings: The embeddings of each potential keyword/keyphrase across</span>
<span class="sd">                             across the vocabulary of the set of input documents.</span>
<span class="sd">                             NOTE: The `word_embeddings` should be generated through</span>
<span class="sd">                             `.extract_embeddings` as the order of these embeddings depend</span>
<span class="sd">                             on the vectorizer that was used to generate its vocabulary.</span>

<span class="sd">        Usage:</span>

<span class="sd">        To generate the word and document embeddings from a set of documents:</span>

<span class="sd">        ```python</span>
<span class="sd">        from keybert import KeyBERT</span>

<span class="sd">        kw_model = KeyBERT()</span>
<span class="sd">        doc_embeddings, word_embeddings = kw_model.extract_embeddings(docs)</span>
<span class="sd">        ```</span>

<span class="sd">        You can then use these embeddings and pass them to `.extract_keywords` to speed up the tuning the model:</span>

<span class="sd">        ```python</span>
<span class="sd">        keywords = kw_model.extract_keywords(docs, doc_embeddings=doc_embeddings, word_embeddings=word_embeddings)</span>
<span class="sd">        ```</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># Check for a single, empty document</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">docs</span><span class="p">,</span> <span class="nb">str</span><span class="p">):</span>
            <span class="k">if</span> <span class="n">docs</span><span class="p">:</span>
                <span class="n">docs</span> <span class="o">=</span> <span class="p">[</span><span class="n">docs</span><span class="p">]</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="k">return</span> <span class="p">[]</span>

        <span class="c1"># Extract potential words using a vectorizer / tokenizer</span>
        <span class="k">if</span> <span class="n">vectorizer</span><span class="p">:</span>
            <span class="n">count</span> <span class="o">=</span> <span class="n">vectorizer</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">docs</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">try</span><span class="p">:</span>
                <span class="n">count</span> <span class="o">=</span> <span class="n">CountVectorizer</span><span class="p">(</span>
                    <span class="n">ngram_range</span><span class="o">=</span><span class="n">keyphrase_ngram_range</span><span class="p">,</span>
                    <span class="n">stop_words</span><span class="o">=</span><span class="n">stop_words</span><span class="p">,</span>
                    <span class="n">min_df</span><span class="o">=</span><span class="n">min_df</span><span class="p">,</span>
                    <span class="n">vocabulary</span><span class="o">=</span><span class="n">candidates</span><span class="p">,</span>
                <span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">docs</span><span class="p">)</span>
            <span class="k">except</span> <span class="ne">ValueError</span><span class="p">:</span>
                <span class="k">return</span> <span class="p">[]</span>

        <span class="c1"># Scikit-Learn Deprecation: get_feature_names is deprecated in 1.0</span>
        <span class="c1"># and will be removed in 1.2. Please use get_feature_names_out instead.</span>
        <span class="k">if</span> <span class="n">version</span><span class="o">.</span><span class="n">parse</span><span class="p">(</span><span class="n">sklearn_version</span><span class="p">)</span> <span class="o">&gt;=</span> <span class="n">version</span><span class="o">.</span><span class="n">parse</span><span class="p">(</span><span class="s2">&quot;1.0.0&quot;</span><span class="p">):</span>
            <span class="n">words</span> <span class="o">=</span> <span class="n">count</span><span class="o">.</span><span class="n">get_feature_names_out</span><span class="p">()</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">words</span> <span class="o">=</span> <span class="n">count</span><span class="o">.</span><span class="n">get_feature_names</span><span class="p">()</span>

        <span class="n">doc_embeddings</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">embed</span><span class="p">(</span><span class="n">docs</span><span class="p">)</span>
        <span class="n">word_embeddings</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">embed</span><span class="p">(</span><span class="n">words</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">doc_embeddings</span><span class="p">,</span> <span class="n">word_embeddings</span>
</code></pre></div></td></tr></table></div>
              </details>



  <div class="doc doc-children">









<div class="doc doc-object doc-function">


<h2 id="keybert._model.KeyBERT.__init__" class="doc doc-heading">
            <code class="highlight language-python"><span class="fm">__init__</span><span class="p">(</span><span class="n">model</span><span class="o">=</span><span class="s1">&#39;all-MiniLM-L6-v2&#39;</span><span class="p">,</span> <span class="n">llm</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span></code>

<a href="#keybert._model.KeyBERT.__init__" class="headerlink" title="Permanent link">&para;</a></h2>


    <div class="doc doc-contents ">

        <p>KeyBERT initialization.</p>


<p><span class="doc-section-title">Parameters:</span></p>
    <table>
      <thead>
        <tr>
          <th>Name</th>
          <th>Type</th>
          <th>Description</th>
          <th>Default</th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>model</code>
            </td>
            <td>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Use a custom embedding model or a specific KeyBERT Backend.
   The following backends are currently supported:
      * SentenceTransformers
      * 🤗 Transformers
      * Flair
      * Spacy
      * Gensim
      * USE (TF-Hub)
    You can also pass in a string that points to one of the following
    sentence-transformers models:
      * https://www.sbert.net/docs/pretrained_models.html</p>
              </div>
            </td>
            <td>
                  <code>&#39;all-MiniLM-L6-v2&#39;</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>llm</code>
            </td>
            <td>
                  <code><span title="keybert.llm._base.BaseLLM">BaseLLM</span></code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>The Large Language Model used to extract keywords</p>
              </div>
            </td>
            <td>
                  <code>None</code>
            </td>
          </tr>
      </tbody>
    </table>

            <details class="quote">
              <summary>Source code in <code>keybert\_model.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">42</span>
<span class="normal">43</span>
<span class="normal">44</span>
<span class="normal">45</span>
<span class="normal">46</span>
<span class="normal">47</span>
<span class="normal">48</span>
<span class="normal">49</span>
<span class="normal">50</span>
<span class="normal">51</span>
<span class="normal">52</span>
<span class="normal">53</span>
<span class="normal">54</span>
<span class="normal">55</span>
<span class="normal">56</span>
<span class="normal">57</span>
<span class="normal">58</span>
<span class="normal">59</span>
<span class="normal">60</span>
<span class="normal">61</span>
<span class="normal">62</span>
<span class="normal">63</span>
<span class="normal">64</span>
<span class="normal">65</span>
<span class="normal">66</span>
<span class="normal">67</span>
<span class="normal">68</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">model</span><span class="o">=</span><span class="s2">&quot;all-MiniLM-L6-v2&quot;</span><span class="p">,</span>
    <span class="n">llm</span><span class="p">:</span> <span class="n">BaseLLM</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
<span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;KeyBERT initialization.</span>

<span class="sd">    Arguments:</span>
<span class="sd">        model: Use a custom embedding model or a specific KeyBERT Backend.</span>
<span class="sd">               The following backends are currently supported:</span>
<span class="sd">                  * SentenceTransformers</span>
<span class="sd">                  * 🤗 Transformers</span>
<span class="sd">                  * Flair</span>
<span class="sd">                  * Spacy</span>
<span class="sd">                  * Gensim</span>
<span class="sd">                  * USE (TF-Hub)</span>
<span class="sd">                You can also pass in a string that points to one of the following</span>
<span class="sd">                sentence-transformers models:</span>
<span class="sd">                  * https://www.sbert.net/docs/pretrained_models.html</span>
<span class="sd">        llm: The Large Language Model used to extract keywords</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">model</span> <span class="o">=</span> <span class="n">select_backend</span><span class="p">(</span><span class="n">model</span><span class="p">)</span>

    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">llm</span><span class="p">,</span> <span class="n">BaseLLM</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">llm</span> <span class="o">=</span> <span class="n">KeyLLM</span><span class="p">(</span><span class="n">llm</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">llm</span> <span class="o">=</span> <span class="n">llm</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h2 id="keybert._model.KeyBERT.extract_embeddings" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">extract_embeddings</span><span class="p">(</span><span class="n">docs</span><span class="p">,</span> <span class="n">candidates</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">keyphrase_ngram_range</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">stop_words</span><span class="o">=</span><span class="s1">&#39;english&#39;</span><span class="p">,</span> <span class="n">min_df</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">vectorizer</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span></code>

<a href="#keybert._model.KeyBERT.extract_embeddings" class="headerlink" title="Permanent link">&para;</a></h2>


    <div class="doc doc-contents ">

        <p>Extract document and word embeddings for the input documents and the
generated candidate keywords/keyphrases respectively.</p>
<p>Note that all potential keywords/keyphrases are not returned but only their
word embeddings. This means that the values of <code>candidates</code>, <code>keyphrase_ngram_range</code>,
<code>stop_words</code>, and <code>min_df</code> need to be the same between using <code>.extract_embeddings</code> and
<code>.extract_keywords</code>.</p>


<p><span class="doc-section-title">Parameters:</span></p>
    <table>
      <thead>
        <tr>
          <th>Name</th>
          <th>Type</th>
          <th>Description</th>
          <th>Default</th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>docs</code>
            </td>
            <td>
                  <code><span title="typing.Union">Union</span>[str, <span title="typing.List">List</span>[str]]</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>The document(s) for which to extract keywords/keyphrases</p>
              </div>
            </td>
            <td>
                <em>required</em>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>candidates</code>
            </td>
            <td>
                  <code><span title="typing.List">List</span>[str]</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Candidate keywords/keyphrases to use instead of extracting them from the document(s)
        NOTE: This is not used if you passed a <code>vectorizer</code>.</p>
              </div>
            </td>
            <td>
                  <code>None</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>keyphrase_ngram_range</code>
            </td>
            <td>
                  <code><span title="typing.Tuple">Tuple</span>[int, int]</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Length, in words, of the extracted keywords/keyphrases.
                   NOTE: This is not used if you passed a <code>vectorizer</code>.</p>
              </div>
            </td>
            <td>
                  <code>(1, 1)</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>stop_words</code>
            </td>
            <td>
                  <code><span title="typing.Union">Union</span>[str, <span title="typing.List">List</span>[str]]</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Stopwords to remove from the document.
        NOTE: This is not used if you passed a <code>vectorizer</code>.</p>
              </div>
            </td>
            <td>
                  <code>&#39;english&#39;</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>min_df</code>
            </td>
            <td>
                  <code>int</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Minimum document frequency of a word across all documents
    if keywords for multiple documents need to be extracted.
    NOTE: This is not used if you passed a <code>vectorizer</code>.</p>
              </div>
            </td>
            <td>
                  <code>1</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>vectorizer</code>
            </td>
            <td>
                  <code><span title="sklearn.feature_extraction.text.CountVectorizer">CountVectorizer</span></code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Pass in your own <code>CountVectorizer</code> from
        <code>sklearn.feature_extraction.text.CountVectorizer</code></p>
              </div>
            </td>
            <td>
                  <code>None</code>
            </td>
          </tr>
      </tbody>
    </table>


    <p><span class="doc-section-title">Returns:</span></p>
    <table>
      <thead>
        <tr>
<th>Name</th>          <th>Type</th>
          <th>Description</th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
<td><code>doc_embeddings</code></td>            <td>
                  <code><span title="typing.Union">Union</span>[<span title="typing.List">List</span>[<span title="typing.Tuple">Tuple</span>[str, float]], <span title="typing.List">List</span>[<span title="typing.List">List</span>[<span title="typing.Tuple">Tuple</span>[str, float]]]]</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>The embeddings of each document.</p>
              </div>
            </td>
          </tr>
          <tr class="doc-section-item">
<td><code>word_embeddings</code></td>            <td>
                  <code><span title="typing.Union">Union</span>[<span title="typing.List">List</span>[<span title="typing.Tuple">Tuple</span>[str, float]], <span title="typing.List">List</span>[<span title="typing.List">List</span>[<span title="typing.Tuple">Tuple</span>[str, float]]]]</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>The embeddings of each potential keyword/keyphrase across
             across the vocabulary of the set of input documents.
             NOTE: The <code>word_embeddings</code> should be generated through
             <code>.extract_embeddings</code> as the order of these embeddings depend
             on the vectorizer that was used to generate its vocabulary.</p>
              </div>
            </td>
          </tr>
      </tbody>
    </table>
        <p>Usage:</p>
<p>To generate the word and document embeddings from a set of documents:</p>
<div class="highlight"><pre><span></span><code><span class="kn">from</span> <span class="nn">keybert</span> <span class="kn">import</span> <span class="n">KeyBERT</span>

<span class="n">kw_model</span> <span class="o">=</span> <span class="n">KeyBERT</span><span class="p">()</span>
<span class="n">doc_embeddings</span><span class="p">,</span> <span class="n">word_embeddings</span> <span class="o">=</span> <span class="n">kw_model</span><span class="o">.</span><span class="n">extract_embeddings</span><span class="p">(</span><span class="n">docs</span><span class="p">)</span>
</code></pre></div>
<p>You can then use these embeddings and pass them to <code>.extract_keywords</code> to speed up the tuning the model:</p>
<div class="highlight"><pre><span></span><code><span class="n">keywords</span> <span class="o">=</span> <span class="n">kw_model</span><span class="o">.</span><span class="n">extract_keywords</span><span class="p">(</span><span class="n">docs</span><span class="p">,</span> <span class="n">doc_embeddings</span><span class="o">=</span><span class="n">doc_embeddings</span><span class="p">,</span> <span class="n">word_embeddings</span><span class="o">=</span><span class="n">word_embeddings</span><span class="p">)</span>
</code></pre></div>

            <details class="quote">
              <summary>Source code in <code>keybert\_model.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">283</span>
<span class="normal">284</span>
<span class="normal">285</span>
<span class="normal">286</span>
<span class="normal">287</span>
<span class="normal">288</span>
<span class="normal">289</span>
<span class="normal">290</span>
<span class="normal">291</span>
<span class="normal">292</span>
<span class="normal">293</span>
<span class="normal">294</span>
<span class="normal">295</span>
<span class="normal">296</span>
<span class="normal">297</span>
<span class="normal">298</span>
<span class="normal">299</span>
<span class="normal">300</span>
<span class="normal">301</span>
<span class="normal">302</span>
<span class="normal">303</span>
<span class="normal">304</span>
<span class="normal">305</span>
<span class="normal">306</span>
<span class="normal">307</span>
<span class="normal">308</span>
<span class="normal">309</span>
<span class="normal">310</span>
<span class="normal">311</span>
<span class="normal">312</span>
<span class="normal">313</span>
<span class="normal">314</span>
<span class="normal">315</span>
<span class="normal">316</span>
<span class="normal">317</span>
<span class="normal">318</span>
<span class="normal">319</span>
<span class="normal">320</span>
<span class="normal">321</span>
<span class="normal">322</span>
<span class="normal">323</span>
<span class="normal">324</span>
<span class="normal">325</span>
<span class="normal">326</span>
<span class="normal">327</span>
<span class="normal">328</span>
<span class="normal">329</span>
<span class="normal">330</span>
<span class="normal">331</span>
<span class="normal">332</span>
<span class="normal">333</span>
<span class="normal">334</span>
<span class="normal">335</span>
<span class="normal">336</span>
<span class="normal">337</span>
<span class="normal">338</span>
<span class="normal">339</span>
<span class="normal">340</span>
<span class="normal">341</span>
<span class="normal">342</span>
<span class="normal">343</span>
<span class="normal">344</span>
<span class="normal">345</span>
<span class="normal">346</span>
<span class="normal">347</span>
<span class="normal">348</span>
<span class="normal">349</span>
<span class="normal">350</span>
<span class="normal">351</span>
<span class="normal">352</span>
<span class="normal">353</span>
<span class="normal">354</span>
<span class="normal">355</span>
<span class="normal">356</span>
<span class="normal">357</span>
<span class="normal">358</span>
<span class="normal">359</span>
<span class="normal">360</span>
<span class="normal">361</span>
<span class="normal">362</span>
<span class="normal">363</span>
<span class="normal">364</span>
<span class="normal">365</span>
<span class="normal">366</span>
<span class="normal">367</span>
<span class="normal">368</span>
<span class="normal">369</span>
<span class="normal">370</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">extract_embeddings</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">docs</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]],</span>
    <span class="n">candidates</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">keyphrase_ngram_range</span><span class="p">:</span> <span class="n">Tuple</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span>
    <span class="n">stop_words</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]]</span> <span class="o">=</span> <span class="s2">&quot;english&quot;</span><span class="p">,</span>
    <span class="n">min_df</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span>
    <span class="n">vectorizer</span><span class="p">:</span> <span class="n">CountVectorizer</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Union</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="n">Tuple</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="nb">float</span><span class="p">]],</span> <span class="n">List</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="n">Tuple</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="nb">float</span><span class="p">]]]]:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Extract document and word embeddings for the input documents and the</span>
<span class="sd">    generated candidate keywords/keyphrases respectively.</span>

<span class="sd">    Note that all potential keywords/keyphrases are not returned but only their</span>
<span class="sd">    word embeddings. This means that the values of `candidates`, `keyphrase_ngram_range`,</span>
<span class="sd">    `stop_words`, and `min_df` need to be the same between using `.extract_embeddings` and</span>
<span class="sd">    `.extract_keywords`.</span>

<span class="sd">    Arguments:</span>
<span class="sd">        docs: The document(s) for which to extract keywords/keyphrases</span>
<span class="sd">        candidates: Candidate keywords/keyphrases to use instead of extracting them from the document(s)</span>
<span class="sd">                    NOTE: This is not used if you passed a `vectorizer`.</span>
<span class="sd">        keyphrase_ngram_range: Length, in words, of the extracted keywords/keyphrases.</span>
<span class="sd">                               NOTE: This is not used if you passed a `vectorizer`.</span>
<span class="sd">        stop_words: Stopwords to remove from the document.</span>
<span class="sd">                    NOTE: This is not used if you passed a `vectorizer`.</span>
<span class="sd">        min_df: Minimum document frequency of a word across all documents</span>
<span class="sd">                if keywords for multiple documents need to be extracted.</span>
<span class="sd">                NOTE: This is not used if you passed a `vectorizer`.</span>
<span class="sd">        vectorizer: Pass in your own `CountVectorizer` from</span>
<span class="sd">                    `sklearn.feature_extraction.text.CountVectorizer`</span>

<span class="sd">    Returns:</span>
<span class="sd">        doc_embeddings: The embeddings of each document.</span>
<span class="sd">        word_embeddings: The embeddings of each potential keyword/keyphrase across</span>
<span class="sd">                         across the vocabulary of the set of input documents.</span>
<span class="sd">                         NOTE: The `word_embeddings` should be generated through</span>
<span class="sd">                         `.extract_embeddings` as the order of these embeddings depend</span>
<span class="sd">                         on the vectorizer that was used to generate its vocabulary.</span>

<span class="sd">    Usage:</span>

<span class="sd">    To generate the word and document embeddings from a set of documents:</span>

<span class="sd">    ```python</span>
<span class="sd">    from keybert import KeyBERT</span>

<span class="sd">    kw_model = KeyBERT()</span>
<span class="sd">    doc_embeddings, word_embeddings = kw_model.extract_embeddings(docs)</span>
<span class="sd">    ```</span>

<span class="sd">    You can then use these embeddings and pass them to `.extract_keywords` to speed up the tuning the model:</span>

<span class="sd">    ```python</span>
<span class="sd">    keywords = kw_model.extract_keywords(docs, doc_embeddings=doc_embeddings, word_embeddings=word_embeddings)</span>
<span class="sd">    ```</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># Check for a single, empty document</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">docs</span><span class="p">,</span> <span class="nb">str</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">docs</span><span class="p">:</span>
            <span class="n">docs</span> <span class="o">=</span> <span class="p">[</span><span class="n">docs</span><span class="p">]</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">return</span> <span class="p">[]</span>

    <span class="c1"># Extract potential words using a vectorizer / tokenizer</span>
    <span class="k">if</span> <span class="n">vectorizer</span><span class="p">:</span>
        <span class="n">count</span> <span class="o">=</span> <span class="n">vectorizer</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">docs</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">try</span><span class="p">:</span>
            <span class="n">count</span> <span class="o">=</span> <span class="n">CountVectorizer</span><span class="p">(</span>
                <span class="n">ngram_range</span><span class="o">=</span><span class="n">keyphrase_ngram_range</span><span class="p">,</span>
                <span class="n">stop_words</span><span class="o">=</span><span class="n">stop_words</span><span class="p">,</span>
                <span class="n">min_df</span><span class="o">=</span><span class="n">min_df</span><span class="p">,</span>
                <span class="n">vocabulary</span><span class="o">=</span><span class="n">candidates</span><span class="p">,</span>
            <span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">docs</span><span class="p">)</span>
        <span class="k">except</span> <span class="ne">ValueError</span><span class="p">:</span>
            <span class="k">return</span> <span class="p">[]</span>

    <span class="c1"># Scikit-Learn Deprecation: get_feature_names is deprecated in 1.0</span>
    <span class="c1"># and will be removed in 1.2. Please use get_feature_names_out instead.</span>
    <span class="k">if</span> <span class="n">version</span><span class="o">.</span><span class="n">parse</span><span class="p">(</span><span class="n">sklearn_version</span><span class="p">)</span> <span class="o">&gt;=</span> <span class="n">version</span><span class="o">.</span><span class="n">parse</span><span class="p">(</span><span class="s2">&quot;1.0.0&quot;</span><span class="p">):</span>
        <span class="n">words</span> <span class="o">=</span> <span class="n">count</span><span class="o">.</span><span class="n">get_feature_names_out</span><span class="p">()</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">words</span> <span class="o">=</span> <span class="n">count</span><span class="o">.</span><span class="n">get_feature_names</span><span class="p">()</span>

    <span class="n">doc_embeddings</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">embed</span><span class="p">(</span><span class="n">docs</span><span class="p">)</span>
    <span class="n">word_embeddings</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">embed</span><span class="p">(</span><span class="n">words</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">doc_embeddings</span><span class="p">,</span> <span class="n">word_embeddings</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h2 id="keybert._model.KeyBERT.extract_keywords" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">extract_keywords</span><span class="p">(</span><span class="n">docs</span><span class="p">,</span> <span class="n">candidates</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">keyphrase_ngram_range</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">stop_words</span><span class="o">=</span><span class="s1">&#39;english&#39;</span><span class="p">,</span> <span class="n">top_n</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">min_df</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">use_maxsum</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">use_mmr</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">diversity</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">nr_candidates</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span> <span class="n">vectorizer</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">highlight</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">seed_keywords</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">doc_embeddings</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">word_embeddings</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">threshold</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span></code>

<a href="#keybert._model.KeyBERT.extract_keywords" class="headerlink" title="Permanent link">&para;</a></h2>


    <div class="doc doc-contents ">

        <p>Extract keywords and/or keyphrases.</p>
<p>To get the biggest speed-up, make sure to pass multiple documents
at once instead of iterating over a single document.</p>


<p><span class="doc-section-title">Parameters:</span></p>
    <table>
      <thead>
        <tr>
          <th>Name</th>
          <th>Type</th>
          <th>Description</th>
          <th>Default</th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>docs</code>
            </td>
            <td>
                  <code><span title="typing.Union">Union</span>[str, <span title="typing.List">List</span>[str]]</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>The document(s) for which to extract keywords/keyphrases</p>
              </div>
            </td>
            <td>
                <em>required</em>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>candidates</code>
            </td>
            <td>
                  <code><span title="typing.List">List</span>[str]</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Candidate keywords/keyphrases to use instead of extracting them from the document(s)
        NOTE: This is not used if you passed a <code>vectorizer</code>.</p>
              </div>
            </td>
            <td>
                  <code>None</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>keyphrase_ngram_range</code>
            </td>
            <td>
                  <code><span title="typing.Tuple">Tuple</span>[int, int]</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Length, in words, of the extracted keywords/keyphrases.
                   NOTE: This is not used if you passed a <code>vectorizer</code>.</p>
              </div>
            </td>
            <td>
                  <code>(1, 1)</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>stop_words</code>
            </td>
            <td>
                  <code><span title="typing.Union">Union</span>[str, <span title="typing.List">List</span>[str]]</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Stopwords to remove from the document.
        NOTE: This is not used if you passed a <code>vectorizer</code>.</p>
              </div>
            </td>
            <td>
                  <code>&#39;english&#39;</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>top_n</code>
            </td>
            <td>
                  <code>int</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Return the top n keywords/keyphrases</p>
              </div>
            </td>
            <td>
                  <code>5</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>min_df</code>
            </td>
            <td>
                  <code>int</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Minimum document frequency of a word across all documents
    if keywords for multiple documents need to be extracted.
    NOTE: This is not used if you passed a <code>vectorizer</code>.</p>
              </div>
            </td>
            <td>
                  <code>1</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>use_maxsum</code>
            </td>
            <td>
                  <code>bool</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Whether to use Max Sum Distance for the selection
        of keywords/keyphrases.</p>
              </div>
            </td>
            <td>
                  <code>False</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>use_mmr</code>
            </td>
            <td>
                  <code>bool</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Whether to use Maximal Marginal Relevance (MMR) for the
     selection of keywords/keyphrases.</p>
              </div>
            </td>
            <td>
                  <code>False</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>diversity</code>
            </td>
            <td>
                  <code>float</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>The diversity of the results between 0 and 1 if <code>use_mmr</code>
       is set to True.</p>
              </div>
            </td>
            <td>
                  <code>0.5</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>nr_candidates</code>
            </td>
            <td>
                  <code>int</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>The number of candidates to consider if <code>use_maxsum</code> is
           set to True.</p>
              </div>
            </td>
            <td>
                  <code>20</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>vectorizer</code>
            </td>
            <td>
                  <code><span title="sklearn.feature_extraction.text.CountVectorizer">CountVectorizer</span></code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Pass in your own <code>CountVectorizer</code> from
        <code>sklearn.feature_extraction.text.CountVectorizer</code></p>
              </div>
            </td>
            <td>
                  <code>None</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>highlight</code>
            </td>
            <td>
                  <code>bool</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Whether to print the document and highlight its keywords/keyphrases.
       NOTE: This does not work if multiple documents are passed.</p>
              </div>
            </td>
            <td>
                  <code>False</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>seed_keywords</code>
            </td>
            <td>
                  <code><span title="typing.Union">Union</span>[<span title="typing.List">List</span>[str], <span title="typing.List">List</span>[<span title="typing.List">List</span>[str]]]</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Seed keywords that may guide the extraction of keywords by
           steering the similarities towards the seeded keywords.
           NOTE: when multiple documents are passed,
           <code>seed_keywords</code>funtions in either of the two ways below:
           - globally: when a flat list of str is passed, keywords are shared by all documents,
           - locally: when a nested list of str is passed, keywords differs among documents.</p>
              </div>
            </td>
            <td>
                  <code>None</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>doc_embeddings</code>
            </td>
            <td>
                  <code><span title="numpy.array">array</span></code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>The embeddings of each document.</p>
              </div>
            </td>
            <td>
                  <code>None</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>word_embeddings</code>
            </td>
            <td>
                  <code><span title="numpy.array">array</span></code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>The embeddings of each potential keyword/keyphrase across
             across the vocabulary of the set of input documents.
             NOTE: The <code>word_embeddings</code> should be generated through
             <code>.extract_embeddings</code> as the order of these embeddings depend
             on the vectorizer that was used to generate its vocabulary.</p>
              </div>
            </td>
            <td>
                  <code>None</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>threshold</code>
            </td>
            <td>
                  <code>float</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Minimum similarity value between 0 and 1 used to decide how similar documents need to receive the same keywords.</p>
              </div>
            </td>
            <td>
                  <code>None</code>
            </td>
          </tr>
      </tbody>
    </table>


    <p><span class="doc-section-title">Returns:</span></p>
    <table>
      <thead>
        <tr>
<th>Name</th>          <th>Type</th>
          <th>Description</th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
<td><code>keywords</code></td>            <td>
                  <code><span title="typing.Union">Union</span>[<span title="typing.List">List</span>[<span title="typing.Tuple">Tuple</span>[str, float]], <span title="typing.List">List</span>[<span title="typing.List">List</span>[<span title="typing.Tuple">Tuple</span>[str, float]]]]</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>The top n keywords for a document with their respective distances
      to the input document.</p>
              </div>
            </td>
          </tr>
      </tbody>
    </table>
        <p>Usage:</p>
<p>To extract keywords from a single document:</p>
<div class="highlight"><pre><span></span><code><span class="kn">from</span> <span class="nn">keybert</span> <span class="kn">import</span> <span class="n">KeyBERT</span>

<span class="n">kw_model</span> <span class="o">=</span> <span class="n">KeyBERT</span><span class="p">()</span>
<span class="n">keywords</span> <span class="o">=</span> <span class="n">kw_model</span><span class="o">.</span><span class="n">extract_keywords</span><span class="p">(</span><span class="n">doc</span><span class="p">)</span>
</code></pre></div>
<p>To extract keywords from multiple documents, which is typically quite a bit faster:</p>
<div class="highlight"><pre><span></span><code><span class="kn">from</span> <span class="nn">keybert</span> <span class="kn">import</span> <span class="n">KeyBERT</span>

<span class="n">kw_model</span> <span class="o">=</span> <span class="n">KeyBERT</span><span class="p">()</span>
<span class="n">keywords</span> <span class="o">=</span> <span class="n">kw_model</span><span class="o">.</span><span class="n">extract_keywords</span><span class="p">(</span><span class="n">docs</span><span class="p">)</span>
</code></pre></div>

            <details class="quote">
              <summary>Source code in <code>keybert\_model.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"> 70</span>
<span class="normal"> 71</span>
<span class="normal"> 72</span>
<span class="normal"> 73</span>
<span class="normal"> 74</span>
<span class="normal"> 75</span>
<span class="normal"> 76</span>
<span class="normal"> 77</span>
<span class="normal"> 78</span>
<span class="normal"> 79</span>
<span class="normal"> 80</span>
<span class="normal"> 81</span>
<span class="normal"> 82</span>
<span class="normal"> 83</span>
<span class="normal"> 84</span>
<span class="normal"> 85</span>
<span class="normal"> 86</span>
<span class="normal"> 87</span>
<span class="normal"> 88</span>
<span class="normal"> 89</span>
<span class="normal"> 90</span>
<span class="normal"> 91</span>
<span class="normal"> 92</span>
<span class="normal"> 93</span>
<span class="normal"> 94</span>
<span class="normal"> 95</span>
<span class="normal"> 96</span>
<span class="normal"> 97</span>
<span class="normal"> 98</span>
<span class="normal"> 99</span>
<span class="normal">100</span>
<span class="normal">101</span>
<span class="normal">102</span>
<span class="normal">103</span>
<span class="normal">104</span>
<span class="normal">105</span>
<span class="normal">106</span>
<span class="normal">107</span>
<span class="normal">108</span>
<span class="normal">109</span>
<span class="normal">110</span>
<span class="normal">111</span>
<span class="normal">112</span>
<span class="normal">113</span>
<span class="normal">114</span>
<span class="normal">115</span>
<span class="normal">116</span>
<span class="normal">117</span>
<span class="normal">118</span>
<span class="normal">119</span>
<span class="normal">120</span>
<span class="normal">121</span>
<span class="normal">122</span>
<span class="normal">123</span>
<span class="normal">124</span>
<span class="normal">125</span>
<span class="normal">126</span>
<span class="normal">127</span>
<span class="normal">128</span>
<span class="normal">129</span>
<span class="normal">130</span>
<span class="normal">131</span>
<span class="normal">132</span>
<span class="normal">133</span>
<span class="normal">134</span>
<span class="normal">135</span>
<span class="normal">136</span>
<span class="normal">137</span>
<span class="normal">138</span>
<span class="normal">139</span>
<span class="normal">140</span>
<span class="normal">141</span>
<span class="normal">142</span>
<span class="normal">143</span>
<span class="normal">144</span>
<span class="normal">145</span>
<span class="normal">146</span>
<span class="normal">147</span>
<span class="normal">148</span>
<span class="normal">149</span>
<span class="normal">150</span>
<span class="normal">151</span>
<span class="normal">152</span>
<span class="normal">153</span>
<span class="normal">154</span>
<span class="normal">155</span>
<span class="normal">156</span>
<span class="normal">157</span>
<span class="normal">158</span>
<span class="normal">159</span>
<span class="normal">160</span>
<span class="normal">161</span>
<span class="normal">162</span>
<span class="normal">163</span>
<span class="normal">164</span>
<span class="normal">165</span>
<span class="normal">166</span>
<span class="normal">167</span>
<span class="normal">168</span>
<span class="normal">169</span>
<span class="normal">170</span>
<span class="normal">171</span>
<span class="normal">172</span>
<span class="normal">173</span>
<span class="normal">174</span>
<span class="normal">175</span>
<span class="normal">176</span>
<span class="normal">177</span>
<span class="normal">178</span>
<span class="normal">179</span>
<span class="normal">180</span>
<span class="normal">181</span>
<span class="normal">182</span>
<span class="normal">183</span>
<span class="normal">184</span>
<span class="normal">185</span>
<span class="normal">186</span>
<span class="normal">187</span>
<span class="normal">188</span>
<span class="normal">189</span>
<span class="normal">190</span>
<span class="normal">191</span>
<span class="normal">192</span>
<span class="normal">193</span>
<span class="normal">194</span>
<span class="normal">195</span>
<span class="normal">196</span>
<span class="normal">197</span>
<span class="normal">198</span>
<span class="normal">199</span>
<span class="normal">200</span>
<span class="normal">201</span>
<span class="normal">202</span>
<span class="normal">203</span>
<span class="normal">204</span>
<span class="normal">205</span>
<span class="normal">206</span>
<span class="normal">207</span>
<span class="normal">208</span>
<span class="normal">209</span>
<span class="normal">210</span>
<span class="normal">211</span>
<span class="normal">212</span>
<span class="normal">213</span>
<span class="normal">214</span>
<span class="normal">215</span>
<span class="normal">216</span>
<span class="normal">217</span>
<span class="normal">218</span>
<span class="normal">219</span>
<span class="normal">220</span>
<span class="normal">221</span>
<span class="normal">222</span>
<span class="normal">223</span>
<span class="normal">224</span>
<span class="normal">225</span>
<span class="normal">226</span>
<span class="normal">227</span>
<span class="normal">228</span>
<span class="normal">229</span>
<span class="normal">230</span>
<span class="normal">231</span>
<span class="normal">232</span>
<span class="normal">233</span>
<span class="normal">234</span>
<span class="normal">235</span>
<span class="normal">236</span>
<span class="normal">237</span>
<span class="normal">238</span>
<span class="normal">239</span>
<span class="normal">240</span>
<span class="normal">241</span>
<span class="normal">242</span>
<span class="normal">243</span>
<span class="normal">244</span>
<span class="normal">245</span>
<span class="normal">246</span>
<span class="normal">247</span>
<span class="normal">248</span>
<span class="normal">249</span>
<span class="normal">250</span>
<span class="normal">251</span>
<span class="normal">252</span>
<span class="normal">253</span>
<span class="normal">254</span>
<span class="normal">255</span>
<span class="normal">256</span>
<span class="normal">257</span>
<span class="normal">258</span>
<span class="normal">259</span>
<span class="normal">260</span>
<span class="normal">261</span>
<span class="normal">262</span>
<span class="normal">263</span>
<span class="normal">264</span>
<span class="normal">265</span>
<span class="normal">266</span>
<span class="normal">267</span>
<span class="normal">268</span>
<span class="normal">269</span>
<span class="normal">270</span>
<span class="normal">271</span>
<span class="normal">272</span>
<span class="normal">273</span>
<span class="normal">274</span>
<span class="normal">275</span>
<span class="normal">276</span>
<span class="normal">277</span>
<span class="normal">278</span>
<span class="normal">279</span>
<span class="normal">280</span>
<span class="normal">281</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">extract_keywords</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">docs</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]],</span>
    <span class="n">candidates</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">keyphrase_ngram_range</span><span class="p">:</span> <span class="n">Tuple</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span>
    <span class="n">stop_words</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]]</span> <span class="o">=</span> <span class="s2">&quot;english&quot;</span><span class="p">,</span>
    <span class="n">top_n</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">5</span><span class="p">,</span>
    <span class="n">min_df</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span>
    <span class="n">use_maxsum</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">use_mmr</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">diversity</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.5</span><span class="p">,</span>
    <span class="n">nr_candidates</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">20</span><span class="p">,</span>
    <span class="n">vectorizer</span><span class="p">:</span> <span class="n">CountVectorizer</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">highlight</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">seed_keywords</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">],</span> <span class="n">List</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">doc_embeddings</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">word_embeddings</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">threshold</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Union</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="n">Tuple</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="nb">float</span><span class="p">]],</span> <span class="n">List</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="n">Tuple</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="nb">float</span><span class="p">]]]]:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Extract keywords and/or keyphrases.</span>

<span class="sd">    To get the biggest speed-up, make sure to pass multiple documents</span>
<span class="sd">    at once instead of iterating over a single document.</span>

<span class="sd">    Arguments:</span>
<span class="sd">        docs: The document(s) for which to extract keywords/keyphrases</span>
<span class="sd">        candidates: Candidate keywords/keyphrases to use instead of extracting them from the document(s)</span>
<span class="sd">                    NOTE: This is not used if you passed a `vectorizer`.</span>
<span class="sd">        keyphrase_ngram_range: Length, in words, of the extracted keywords/keyphrases.</span>
<span class="sd">                               NOTE: This is not used if you passed a `vectorizer`.</span>
<span class="sd">        stop_words: Stopwords to remove from the document.</span>
<span class="sd">                    NOTE: This is not used if you passed a `vectorizer`.</span>
<span class="sd">        top_n: Return the top n keywords/keyphrases</span>
<span class="sd">        min_df: Minimum document frequency of a word across all documents</span>
<span class="sd">                if keywords for multiple documents need to be extracted.</span>
<span class="sd">                NOTE: This is not used if you passed a `vectorizer`.</span>
<span class="sd">        use_maxsum: Whether to use Max Sum Distance for the selection</span>
<span class="sd">                    of keywords/keyphrases.</span>
<span class="sd">        use_mmr: Whether to use Maximal Marginal Relevance (MMR) for the</span>
<span class="sd">                 selection of keywords/keyphrases.</span>
<span class="sd">        diversity: The diversity of the results between 0 and 1 if `use_mmr`</span>
<span class="sd">                   is set to True.</span>
<span class="sd">        nr_candidates: The number of candidates to consider if `use_maxsum` is</span>
<span class="sd">                       set to True.</span>
<span class="sd">        vectorizer: Pass in your own `CountVectorizer` from</span>
<span class="sd">                    `sklearn.feature_extraction.text.CountVectorizer`</span>
<span class="sd">        highlight: Whether to print the document and highlight its keywords/keyphrases.</span>
<span class="sd">                   NOTE: This does not work if multiple documents are passed.</span>
<span class="sd">        seed_keywords: Seed keywords that may guide the extraction of keywords by</span>
<span class="sd">                       steering the similarities towards the seeded keywords.</span>
<span class="sd">                       NOTE: when multiple documents are passed,</span>
<span class="sd">                       `seed_keywords`funtions in either of the two ways below:</span>
<span class="sd">                       - globally: when a flat list of str is passed, keywords are shared by all documents,</span>
<span class="sd">                       - locally: when a nested list of str is passed, keywords differs among documents.</span>
<span class="sd">        doc_embeddings: The embeddings of each document.</span>
<span class="sd">        word_embeddings: The embeddings of each potential keyword/keyphrase across</span>
<span class="sd">                         across the vocabulary of the set of input documents.</span>
<span class="sd">                         NOTE: The `word_embeddings` should be generated through</span>
<span class="sd">                         `.extract_embeddings` as the order of these embeddings depend</span>
<span class="sd">                         on the vectorizer that was used to generate its vocabulary.</span>
<span class="sd">        threshold: Minimum similarity value between 0 and 1 used to decide how similar documents need to receive the same keywords.</span>

<span class="sd">    Returns:</span>
<span class="sd">        keywords: The top n keywords for a document with their respective distances</span>
<span class="sd">                  to the input document.</span>

<span class="sd">    Usage:</span>

<span class="sd">    To extract keywords from a single document:</span>

<span class="sd">    ```python</span>
<span class="sd">    from keybert import KeyBERT</span>

<span class="sd">    kw_model = KeyBERT()</span>
<span class="sd">    keywords = kw_model.extract_keywords(doc)</span>
<span class="sd">    ```</span>

<span class="sd">    To extract keywords from multiple documents, which is typically quite a bit faster:</span>

<span class="sd">    ```python</span>
<span class="sd">    from keybert import KeyBERT</span>

<span class="sd">    kw_model = KeyBERT()</span>
<span class="sd">    keywords = kw_model.extract_keywords(docs)</span>
<span class="sd">    ```</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># Check for a single, empty document</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">docs</span><span class="p">,</span> <span class="nb">str</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">docs</span><span class="p">:</span>
            <span class="n">docs</span> <span class="o">=</span> <span class="p">[</span><span class="n">docs</span><span class="p">]</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">return</span> <span class="p">[]</span>

    <span class="c1"># Extract potential words using a vectorizer / tokenizer</span>
    <span class="k">if</span> <span class="n">vectorizer</span><span class="p">:</span>
        <span class="n">count</span> <span class="o">=</span> <span class="n">vectorizer</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">docs</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">try</span><span class="p">:</span>
            <span class="n">count</span> <span class="o">=</span> <span class="n">CountVectorizer</span><span class="p">(</span>
                <span class="n">ngram_range</span><span class="o">=</span><span class="n">keyphrase_ngram_range</span><span class="p">,</span>
                <span class="n">stop_words</span><span class="o">=</span><span class="n">stop_words</span><span class="p">,</span>
                <span class="n">min_df</span><span class="o">=</span><span class="n">min_df</span><span class="p">,</span>
                <span class="n">vocabulary</span><span class="o">=</span><span class="n">candidates</span><span class="p">,</span>
            <span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">docs</span><span class="p">)</span>
        <span class="k">except</span> <span class="ne">ValueError</span><span class="p">:</span>
            <span class="k">return</span> <span class="p">[]</span>

    <span class="c1"># Scikit-Learn Deprecation: get_feature_names is deprecated in 1.0</span>
    <span class="c1"># and will be removed in 1.2. Please use get_feature_names_out instead.</span>
    <span class="k">if</span> <span class="n">version</span><span class="o">.</span><span class="n">parse</span><span class="p">(</span><span class="n">sklearn_version</span><span class="p">)</span> <span class="o">&gt;=</span> <span class="n">version</span><span class="o">.</span><span class="n">parse</span><span class="p">(</span><span class="s2">&quot;1.0.0&quot;</span><span class="p">):</span>
        <span class="n">words</span> <span class="o">=</span> <span class="n">count</span><span class="o">.</span><span class="n">get_feature_names_out</span><span class="p">()</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">words</span> <span class="o">=</span> <span class="n">count</span><span class="o">.</span><span class="n">get_feature_names</span><span class="p">()</span>
    <span class="n">df</span> <span class="o">=</span> <span class="n">count</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">docs</span><span class="p">)</span>

    <span class="c1"># Check if the right number of word embeddings are generated compared with the vectorizer</span>
    <span class="k">if</span> <span class="n">word_embeddings</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">word_embeddings</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">!=</span> <span class="nb">len</span><span class="p">(</span><span class="n">words</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="s2">&quot;Make sure that the `word_embeddings` are generated from the function &quot;</span>
                <span class="s2">&quot;`.extract_embeddings`. </span><span class="se">\n</span><span class="s2">Moreover, the `candidates`, `keyphrase_ngram_range`,&quot;</span>
                <span class="s2">&quot;`stop_words`, and `min_df` parameters need to have the same values in both &quot;</span>
                <span class="s2">&quot;`.extract_embeddings` and `.extract_keywords`.&quot;</span>
            <span class="p">)</span>

    <span class="c1"># Extract embeddings</span>
    <span class="k">if</span> <span class="n">doc_embeddings</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">doc_embeddings</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">embed</span><span class="p">(</span><span class="n">docs</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">word_embeddings</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">word_embeddings</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">embed</span><span class="p">(</span><span class="n">words</span><span class="p">)</span>

    <span class="c1"># Guided KeyBERT either local (keywords shared among documents) or global (keywords per document)</span>
    <span class="k">if</span> <span class="n">seed_keywords</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">seed_keywords</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="nb">str</span><span class="p">):</span>
            <span class="n">seed_embeddings</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">embed</span><span class="p">(</span><span class="n">seed_keywords</span><span class="p">)</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">keepdims</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        <span class="k">elif</span> <span class="nb">len</span><span class="p">(</span><span class="n">docs</span><span class="p">)</span> <span class="o">!=</span> <span class="nb">len</span><span class="p">(</span><span class="n">seed_keywords</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;The length of docs must match the length of seed_keywords&quot;</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">seed_embeddings</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">vstack</span><span class="p">(</span>
                <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">embed</span><span class="p">(</span><span class="n">keywords</span><span class="p">)</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">keepdims</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span> <span class="k">for</span> <span class="n">keywords</span> <span class="ow">in</span> <span class="n">seed_keywords</span><span class="p">]</span>
            <span class="p">)</span>
        <span class="n">doc_embeddings</span> <span class="o">=</span> <span class="p">(</span><span class="n">doc_embeddings</span> <span class="o">*</span> <span class="mi">3</span> <span class="o">+</span> <span class="n">seed_embeddings</span><span class="p">)</span> <span class="o">/</span> <span class="mi">4</span>

    <span class="c1"># Find keywords</span>
    <span class="n">all_keywords</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">index</span><span class="p">,</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">docs</span><span class="p">):</span>
        <span class="k">try</span><span class="p">:</span>
            <span class="c1"># Select embeddings</span>
            <span class="n">candidate_indices</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="n">index</span><span class="p">]</span><span class="o">.</span><span class="n">nonzero</span><span class="p">()[</span><span class="mi">1</span><span class="p">]</span>
            <span class="n">candidates</span> <span class="o">=</span> <span class="p">[</span><span class="n">words</span><span class="p">[</span><span class="n">index</span><span class="p">]</span> <span class="k">for</span> <span class="n">index</span> <span class="ow">in</span> <span class="n">candidate_indices</span><span class="p">]</span>
            <span class="n">candidate_embeddings</span> <span class="o">=</span> <span class="n">word_embeddings</span><span class="p">[</span><span class="n">candidate_indices</span><span class="p">]</span>
            <span class="n">doc_embedding</span> <span class="o">=</span> <span class="n">doc_embeddings</span><span class="p">[</span><span class="n">index</span><span class="p">]</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>

            <span class="c1"># Maximal Marginal Relevance (MMR)</span>
            <span class="k">if</span> <span class="n">use_mmr</span><span class="p">:</span>
                <span class="n">keywords</span> <span class="o">=</span> <span class="n">mmr</span><span class="p">(</span>
                    <span class="n">doc_embedding</span><span class="p">,</span>
                    <span class="n">candidate_embeddings</span><span class="p">,</span>
                    <span class="n">candidates</span><span class="p">,</span>
                    <span class="n">top_n</span><span class="p">,</span>
                    <span class="n">diversity</span><span class="p">,</span>
                <span class="p">)</span>

            <span class="c1"># Max Sum Distance</span>
            <span class="k">elif</span> <span class="n">use_maxsum</span><span class="p">:</span>
                <span class="n">keywords</span> <span class="o">=</span> <span class="n">max_sum_distance</span><span class="p">(</span>
                    <span class="n">doc_embedding</span><span class="p">,</span>
                    <span class="n">candidate_embeddings</span><span class="p">,</span>
                    <span class="n">candidates</span><span class="p">,</span>
                    <span class="n">top_n</span><span class="p">,</span>
                    <span class="n">nr_candidates</span><span class="p">,</span>
                <span class="p">)</span>

            <span class="c1"># Cosine-based keyword extraction</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">distances</span> <span class="o">=</span> <span class="n">cosine_similarity</span><span class="p">(</span><span class="n">doc_embedding</span><span class="p">,</span> <span class="n">candidate_embeddings</span><span class="p">)</span>
                <span class="n">keywords</span> <span class="o">=</span> <span class="p">[</span>
                    <span class="p">(</span><span class="n">candidates</span><span class="p">[</span><span class="n">index</span><span class="p">],</span> <span class="nb">round</span><span class="p">(</span><span class="nb">float</span><span class="p">(</span><span class="n">distances</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="n">index</span><span class="p">]),</span> <span class="mi">4</span><span class="p">))</span>
                    <span class="k">for</span> <span class="n">index</span> <span class="ow">in</span> <span class="n">distances</span><span class="o">.</span><span class="n">argsort</span><span class="p">()[</span><span class="mi">0</span><span class="p">][</span><span class="o">-</span><span class="n">top_n</span><span class="p">:]</span>
                <span class="p">][::</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>

            <span class="n">all_keywords</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">keywords</span><span class="p">)</span>

        <span class="c1"># Capturing empty keywords</span>
        <span class="k">except</span> <span class="ne">ValueError</span><span class="p">:</span>
            <span class="n">all_keywords</span><span class="o">.</span><span class="n">append</span><span class="p">([])</span>

    <span class="c1"># Highlight keywords in the document</span>
    <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">all_keywords</span><span class="p">)</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">highlight</span><span class="p">:</span>
            <span class="n">highlight_document</span><span class="p">(</span><span class="n">docs</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">all_keywords</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">count</span><span class="p">)</span>
        <span class="n">all_keywords</span> <span class="o">=</span> <span class="n">all_keywords</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>

    <span class="c1"># Fine-tune keywords using an LLM</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">llm</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="kn">import</span> <span class="nn">torch</span>

        <span class="n">doc_embeddings</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">from_numpy</span><span class="p">(</span><span class="n">doc_embeddings</span><span class="p">)</span><span class="o">.</span><span class="n">float</span><span class="p">()</span>
        <span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">is_available</span><span class="p">():</span>
            <span class="n">doc_embeddings</span> <span class="o">=</span> <span class="n">doc_embeddings</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="s2">&quot;cuda&quot;</span><span class="p">)</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">all_keywords</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="nb">tuple</span><span class="p">):</span>
            <span class="n">candidate_keywords</span> <span class="o">=</span> <span class="p">[[</span><span class="n">keyword</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="k">for</span> <span class="n">keyword</span> <span class="ow">in</span> <span class="n">all_keywords</span><span class="p">]]</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">candidate_keywords</span> <span class="o">=</span> <span class="p">[[</span><span class="n">keyword</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="k">for</span> <span class="n">keyword</span> <span class="ow">in</span> <span class="n">keywords</span><span class="p">]</span> <span class="k">for</span> <span class="n">keywords</span> <span class="ow">in</span> <span class="n">all_keywords</span><span class="p">]</span>
        <span class="n">keywords</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">llm</span><span class="o">.</span><span class="n">extract_keywords</span><span class="p">(</span>
            <span class="n">docs</span><span class="p">,</span>
            <span class="n">embeddings</span><span class="o">=</span><span class="n">doc_embeddings</span><span class="p">,</span>
            <span class="n">candidate_keywords</span><span class="o">=</span><span class="n">candidate_keywords</span><span class="p">,</span>
            <span class="n">threshold</span><span class="o">=</span><span class="n">threshold</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="k">return</span> <span class="n">keywords</span>
    <span class="k">return</span> <span class="n">all_keywords</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>



  </div>

    </div>

</div>












                
              </article>
            </div>
          
          
<script>var target=document.getElementById(location.hash.slice(1));target&&target.name&&(target.checked=target.name.startsWith("__tabbed_"))</script>
        </div>
        
          <button type="button" class="md-top md-icon" data-md-component="top" hidden>
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M13 20h-2V8l-5.5 5.5-1.42-1.42L12 4.16l7.92 7.92-1.42 1.42L13 8z"/></svg>
  Back to top
</button>
        
      </main>
      
        <footer class="md-footer">
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-copyright">
  
    <div class="md-copyright__highlight">
      Copyright &copy; 2025 Maintained by <a href="https://github.com/MaartenGr">Maarten Grootendorst</a>.
    </div>
  
  
    Made with
    <a href="https://squidfunk.github.io/mkdocs-material/" target="_blank" rel="noopener">
      Material for MkDocs
    </a>
  
</div>
      
    </div>
  </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    
    
    <script id="__config" type="application/json">{"base": "..", "features": ["navigation.tabs", "navigation.sections", "navigation.instant", "navigation.top", "navigation.tracking", "toc.follow"], "search": "../assets/javascripts/workers/search.6ce7567c.min.js", "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version": "Select version"}}</script>
    
    
      <script src="../assets/javascripts/bundle.83f73b43.min.js"></script>
      
    
  </body>
</html>