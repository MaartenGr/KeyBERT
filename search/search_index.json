{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"index.html","title":"Home","text":""},{"location":"index.html#keybert","title":"KeyBERT","text":"<p>KeyBERT is a minimal and easy-to-use keyword extraction technique that leverages BERT embeddings to create keywords and keyphrases that are most similar to a document.</p>"},{"location":"index.html#about-the-project","title":"About the Project","text":"<p>Although there are already many methods available for keyword generation (e.g., Rake, YAKE!, TF-IDF, etc.) I wanted to create a very basic, but powerful method for extracting keywords and keyphrases. This is where KeyBERT comes in! Which uses BERT-embeddings and simple cosine similarity to find the sub-phrases in a document that are the most similar to the document itself.</p> <p>First, document embeddings are extracted with BERT to get a document-level representation. Then, word embeddings are extracted for N-gram words/phrases. Finally, we use cosine similarity to find the words/phrases that are the most similar to the document. The most similar words could then be identified as the words that best describe the entire document.</p> <p>KeyBERT is by no means unique and is created as a quick and easy method for creating keywords and keyphrases. Although there are many great papers and solutions out there that use BERT-embeddings (e.g., 1, 2, 3, ), I could not find a BERT-based solution that did not have to be trained from scratch and could be used for beginners (correct me if I'm wrong!). Thus, the goal was a <code>pip install keybert</code> and at most 3 lines of code in usage.</p>"},{"location":"index.html#installation","title":"Installation","text":"<p>Installation can be done using pypi:</p> <pre><code>pip install keybert\n</code></pre> <p>You may want to install more depending on the transformers and language backends that you will be using. The possible installations are:</p> <pre><code>pip install keybert[flair]\npip install keybert[gensim]\npip install keybert[spacy]\npip install keybert[use]\n</code></pre>"},{"location":"index.html#usage","title":"Usage","text":"<p>The most minimal example can be seen below for the extraction of keywords: <pre><code>from keybert import KeyBERT\n\ndoc = \"\"\"\n         Supervised learning is the machine learning task of learning a function that\n         maps an input to an output based on example input-output pairs. It infers a\n         function from labeled training data consisting of a set of training examples.\n         In supervised learning, each example is a pair consisting of an input object\n         (typically a vector) and a desired output value (also called the supervisory signal).\n         A supervised learning algorithm analyzes the training data and produces an inferred function,\n         which can be used for mapping new examples. An optimal scenario will allow for the\n         algorithm to correctly determine the class labels for unseen instances. This requires\n         the learning algorithm to generalize from the training data to unseen situations in a\n         'reasonable' way (see inductive bias).\n      \"\"\"\nkw_model = KeyBERT()\nkeywords = kw_model.extract_keywords(doc)\n</code></pre></p> <p>You can set <code>keyphrase_ngram_range</code> to set the length of the resulting keywords/keyphrases:</p> <pre><code>&gt;&gt;&gt; kw_model.extract_keywords(doc, keyphrase_ngram_range=(1, 1), stop_words=None)\n[('learning', 0.4604),\n ('algorithm', 0.4556),\n ('training', 0.4487),\n ('class', 0.4086),\n ('mapping', 0.3700)]\n</code></pre> <p>To extract keyphrases, simply set <code>keyphrase_ngram_range</code> to (1, 2) or higher depending on the number of words you would like in the resulting keyphrases:</p> <pre><code>&gt;&gt;&gt; kw_model.extract_keywords(doc, keyphrase_ngram_range=(1, 2), stop_words=None)\n[('learning algorithm', 0.6978),\n ('machine learning', 0.6305),\n ('supervised learning', 0.5985),\n ('algorithm analyzes', 0.5860),\n ('learning function', 0.5850)]\n</code></pre> <p>NOTE</p> <p>You can also pass multiple documents at once if you are looking for a major speed-up!</p>"},{"location":"changelog.html","title":"Changelog","text":""},{"location":"changelog.html#version-090","title":"Version 0.9.0","text":"<p>Release date: 05 Februari, 2025</p> <p>Model2Vec</p> <p>You can use Model2Vec for blazingly fast embedding as follows:</p> <pre><code>from keybert import KeyBERT\nfrom model2vec import StaticModel\n\nembedding_model = StaticModel.from_pretrained(\"minishlab/potion-base-8M\")\nkw_model = KeyBERT(embedding_model)\n</code></pre> <p>Light-weight KeyBERT</p> <p>You can now install a light-weight KeyBERT with:</p> <p><code>pip install keybert --no-deps scikit-learn model2vec</code></p> <p>Fixes</p> <ul> <li>Add Model2Vec &amp; light-weight installation in #253</li> <li>Add Text Generation Inference with JSON output by @joaomsimoes in #235</li> <li>Update pre-commit hooks @afuetterer in #237</li> <li>Set up lint job using pre-commit/action @afuetterer in #238</li> </ul>"},{"location":"changelog.html#version-085","title":"Version 0.8.5","text":"<p>Release date: 14 June, 2024</p> <ul> <li>Use <code>batch_size</code> parameter with <code>keybert.backend.SentenceTransformerBackend</code> by @adhadse in #210</li> <li>Add system_prompt param to LLMs by @lucafirefox in #214</li> <li>Update OpenAI API response by @lucafirefox in #213</li> <li>Drop support for python 3.6 and 3.7 by @afuetterer in #230</li> <li>Bump github actions versions by @afuetterer in #228</li> <li>Switch from setup.py to pyproject.toml by @afuetterer in #231</li> </ul>"},{"location":"changelog.html#version-084","title":"Version 0.8.4","text":"<p>Release date: 15 Februari, 2024</p> <ul> <li>Update default Cohere model to <code>command</code> by @sam-frampton in #194</li> <li>Fix KeyLLM fails when no GPU is available by @igor-pechersky in #201</li> <li>Fix <code>AttributeError: 'tuple' object has no attribute 'page_content'</code> in LangChain in #199</li> </ul>"},{"location":"changelog.html#version-083","title":"Version 0.8.3","text":"<p>Release date: 29 November, 2023</p> <ul> <li>Fix support for openai&gt;=1</li> </ul> <p>You can now use it as follows:</p> <pre><code>import openai\nfrom keybert.llm import OpenAI\nfrom keybert import KeyLLM\n\n# Create your LLM\nclient = openai.OpenAI(api_key=MY_API_KEY)\nllm = OpenAI(client)\n\n# Load it in KeyLLM\nkw_model = KeyLLM(llm)\n</code></pre>"},{"location":"changelog.html#version-082","title":"Version 0.8.2","text":"<p>Release date: 29 September, 2023</p> <ul> <li>Fixed cuda error when using pre-calculated embeddings with <code>KeyBERT</code> + <code>KeyLLM</code></li> </ul>"},{"location":"changelog.html#version-081","title":"Version 0.8.1","text":"<p>Release date: 29 September, 2023</p> <ul> <li>Remove unnecessary print statements</li> </ul>"},{"location":"changelog.html#version-080","title":"Version 0.8.0","text":"<p>Release date: 29 September, 2023</p> <p>Highlights:</p> <ul> <li>Use <code>KeyLLM</code> to leverage LLMs for extracting keywords</li> <li>Use it either with or without candidate keywords generated through <code>KeyBERT</code></li> <li>Multiple LLMs are integrated: OpenAI, Cohere, LangChain, HF, and LiteLLM</li> </ul> <pre><code>import openai\nfrom keybert.llm import OpenAI\nfrom keybert import KeyLLM\n\n# Create your LLM\nopenai.api_key = \"sk-...\"\nllm = OpenAI()\n\n# Load it in KeyLLM\nkw_model = KeyLLM(llm)\n</code></pre> <p>See here for full documentation on use cases of <code>KeyLLM</code> and here for the implemented Large Language Models.</p> <p>Fixes:</p> <ul> <li>Enable Guided KeyBERT for seed keywords differing among docs by @shengbo-ma in #152</li> </ul>"},{"location":"changelog.html#version-070","title":"Version 0.7.0","text":"<p>Release date: 3 November, 2022</p> <p>Highlights:</p> <ul> <li>Cleaned up documentation and added several visual representations of the algorithm (excluding MMR / MaxSum)</li> <li>Added function to extract and pass word- and document embeddings which should make fine-tuning much faster</li> </ul> <pre><code>from keybert import KeyBERT\n\nkw_model = KeyBERT()\n\n# Prepare embeddings\ndoc_embeddings, word_embeddings = kw_model.extract_embeddings(docs)\n\n# Extract keywords without needing to re-calculate embeddings\nkeywords = kw_model.extract_keywords(docs, doc_embeddings=doc_embeddings, word_embeddings=word_embeddings)\n</code></pre> <p>Do note that the parameters passed to <code>.extract_embeddings</code> for creating the vectorizer should be exactly the same as those in <code>.extract_keywords</code>.</p> <p>Fixes:</p> <ul> <li>Redundant documentation was removed by @mabhay3420 in #123</li> <li>Fixed Gensim backend not working after v4 migration (#71)</li> <li>Fixed <code>candidates</code> not working (#122)</li> </ul>"},{"location":"changelog.html#version-060","title":"Version 0.6.0","text":"<p>Release date: 25 July, 2022</p> <p>Highlights:</p> <ul> <li>Major speedup, up to 2x to 5x when passing multiple documents (for MMR and MaxSum) compared to single documents</li> <li>Same results whether passing a single document or multiple documents</li> <li>MMR and MaxSum now work when passing a single document or multiple documents</li> <li>Improved documentation</li> <li>Added \ud83e\udd17 Hugging Face Transformers</li> </ul> <pre><code>from keybert import KeyBERT\nfrom transformers.pipelines import pipeline\n\nhf_model = pipeline(\"feature-extraction\", model=\"distilbert-base-cased\")\nkw_model = KeyBERT(model=hf_model)\n</code></pre> <ul> <li>Highlighting support for Chinese texts<ul> <li>Now uses the <code>CountVectorizer</code> for creating the tokens</li> <li>This should also improve the highlighting for most applications and higher n-grams</li> </ul> </li> </ul> <p></p> <p>NOTE: Although highlighting for Chinese texts is improved, since I am not familiar with the Chinese language there is a good chance it is not yet as optimized as for other languages. Any feedback with respect to this is highly appreciated!</p> <p>Fixes:</p> <ul> <li>Fix typo in ReadMe by @priyanshul-govil in #117</li> <li>Add missing optional dependencies (gensim, use, and spacy) by @yusuke1997  in #114</li> </ul>"},{"location":"changelog.html#version-051","title":"Version 0.5.1","text":"<p>Release date:  31 March, 2022</p> <ul> <li>Added a page about leveraging <code>CountVectorizer</code> and <code>KeyphraseVectorizers</code><ul> <li>Shoutout to @TimSchopf for creating and optimizing the package!</li> <li>The <code>KeyphraseVectorizers</code> package can be found here</li> </ul> </li> <li>Fixed Max Sum Similarity returning incorrect similarities #92<ul> <li>Thanks to @kunihik0 for the PR!</li> </ul> </li> <li>Fixed out of bounds condition in MMR<ul> <li>Thanks to @artmatsak for the PR!</li> </ul> </li> <li>Started styling with Flake8 and Black (which was long overdue)<ul> <li>Added pre-commit to make following through a bit easier with styling</li> </ul> </li> </ul>"},{"location":"changelog.html#version-050","title":"Version 0.5.0","text":"<p>Release date:  28 September, 2021</p> <p>Highlights:</p> <ul> <li>Added Guided KeyBERT<ul> <li>kw_model.extract_keywords(doc, seed_keywords=seed_keywords)</li> <li>Thanks to @zolekode for the inspiration!</li> </ul> </li> <li>Use the newest all-* models from SBERT</li> </ul> <p>Miscellaneous:</p> <ul> <li>Added instructions in the FAQ to extract keywords from Chinese documents</li> <li>Fix typo in ReadMe by @koaning in #51</li> </ul>"},{"location":"changelog.html#version-040","title":"Version 0.4.0","text":"<p>Release date:  23 June, 2021</p> <p>Highlights:</p> <ul> <li>Highlight a document's keywords with:<ul> <li><code>keywords = kw_model.extract_keywords(doc, highlight=True)</code></li> </ul> </li> <li>Use <code>paraphrase-MiniLM-L6-v2</code> as the default embedder which gives great results!</li> </ul> <p>Miscellaneous:</p> <ul> <li>Update Flair dependencies</li> <li>Added FAQ</li> </ul>"},{"location":"changelog.html#version-030","title":"Version 0.3.0","text":"<p>Release date:  10 May, 2021</p> <p>The two main features are candidate keywords and several backends to use instead of Flair and SentenceTransformers!</p> <p>Highlights:</p> <ul> <li>Use candidate words instead of extracting those from the documents (#25)<ul> <li><code>KeyBERT().extract_keywords(doc, candidates)</code></li> </ul> </li> <li>Spacy, Gensim, USE, and Custom Backends were added (see documentation here)</li> </ul> <p>Fixes:</p> <ul> <li>Improved imports</li> <li>Fix encoding error when locally installing KeyBERT (#30)</li> </ul> <p>Miscellaneous:</p> <ul> <li>Improved documentation (ReadMe &amp; MKDocs)</li> <li>Add the main tutorial as a shield</li> <li>Typos (#31, #35)</li> </ul>"},{"location":"changelog.html#version-020","title":"Version 0.2.0","text":"<p>Release date:  9 Feb, 2021</p> <p>Highlights:</p> <ul> <li>Add similarity scores to the output</li> <li>Add Flair as a possible back-end</li> <li>Update documentation + improved testing</li> </ul>"},{"location":"changelog.html#version-012","title":"Version 0.1.2","text":"<p>Release date:  28 Oct, 2020</p> <p>Added Max Sum Similarity as an option to diversify your results.</p>"},{"location":"changelog.html#version-010","title":"Version 0.1.0","text":"<p>Release date:  27 Oct, 2020</p> <p>This first release includes keyword/keyphrase extraction using BERT and simple cosine similarity. There is also an option to use Maximal Marginal Relevance to select the candidate keywords/keyphrases.</p>"},{"location":"faq.html","title":"FAQ","text":""},{"location":"faq.html#which-embedding-model-works-best-for-which-language","title":"Which embedding model works best for which language?","text":"<p>Unfortunately, there is not a definitive list of the best models for each language, this highly depends on your data, the model, and your specific use-case. However, the default model in KeyBERT (<code>\"all-MiniLM-L6-v2\"</code>) works great for English documents. In contrast, for multi-lingual documents or any other language, <code>\"paraphrase-multilingual-MiniLM-L12-v2\"\"</code> has shown great performance.</p> <p>If you want to use a model that provides a higher quality, but takes more compute time, then I would advise using <code>paraphrase-mpnet-base-v2</code> and <code>paraphrase-multilingual-mpnet-base-v2</code> instead.</p>"},{"location":"faq.html#should-i-preprocess-the-data","title":"Should I preprocess the data?","text":"<p>No. By using document embeddings there is typically no need to preprocess the data as all parts of a document are important in understanding the general topic of the document. Although this holds true in 99% of cases, if you have data that contains a lot of noise, for example, HTML-tags, then it would be best to remove them. HTML-tags typically do not contribute to the meaning of a document and should therefore be removed. However, if you apply topic modeling to HTML-code to extract topics of code, then it becomes important.</p>"},{"location":"faq.html#how-can-i-speed-up-the-model","title":"How can I speed up the model?","text":"<p>Since KeyBERT uses large language models as its backend, a GPU is typically prefered when using this package. Although it is possible to use it without a dedicated GPU, the inference speed will be significantly slower.</p> <p>A second method for speeding up KeyBERT is by passing it multiple documents at once. By doing this, words need to only be embedded a single time, which can result in a major speed up.</p> <p>This is faster:</p> <pre><code>from keybert import KeyBERT\n\nkw_model = KeyBERT()\n\nkeywords = kw_model.extract_keywords(my_list_of_documents)\n</code></pre> <p>This is slower:</p> <pre><code>from keybert import KeyBERT\n\nkw_model = KeyBERT()\n\nkeywords = []\nfor document in my_list_of_documents:\n    keyword = kw_model.extract_keywords(document)\n    keywords.append(keyword)\n</code></pre>"},{"location":"faq.html#how-can-i-use-keybert-with-chinese-documents","title":"How can I use KeyBERT with Chinese documents?","text":"<p>You need to make sure you use a tokenizer in KeyBERT that supports tokenization of Chinese. I suggest installing <code>jieba</code> for this:</p> <pre><code>from sklearn.feature_extraction.text import CountVectorizer\nimport jieba\n\ndef tokenize_zh(text):\n    words = jieba.lcut(text)\n    return words\n\nvectorizer = CountVectorizer(tokenizer=tokenize_zh)\n</code></pre> <p>Then, simply pass the vectorizer to your KeyBERT instance:</p> <pre><code>from keybert import KeyBERT\n\nkw_model = KeyBERT()\nkeywords = kw_model.extract_keywords(doc, vectorizer=vectorizer)\n</code></pre> <p>It also supports highlighting:</p> <p></p>"},{"location":"api/cohere.html","title":"<code>Cohere</code>","text":"<p>               Bases: <code>BaseLLM</code></p> <p>Use the Cohere API to generate topic labels based on their generative model.</p> <p>Find more about their models here: https://docs.cohere.ai/docs</p> <p>NOTE: The resulting keywords are expected to be separated by commas so any changes to the prompt will have to make sure that the resulting keywords are comma-separated.</p> <p>Parameters:</p> Name Type Description Default <code>client</code> <p>A cohere.Client</p> required <code>model</code> <code>str</code> <p>Model to use within Cohere, defaults to <code>\"xlarge\"</code>.</p> <code>'command'</code> <code>prompt</code> <code>str</code> <p>The prompt to be used in the model. If no prompt is given,     <code>self.default_prompt_</code> is used instead.     NOTE: Use <code>\"[KEYWORDS]\"</code> and <code>\"[DOCUMENTS]\"</code> in the prompt     to decide where the keywords and documents need to be     inserted.</p> <code>None</code> <code>delay_in_seconds</code> <code>float</code> <p>The delay in seconds between consecutive prompts               in order to prevent RateLimitErrors.</p> <code>None</code> <code>verbose</code> <code>bool</code> <p>Set this to True if you want to see a progress bar for the      keyword extraction.</p> <code>False</code> <p>Usage:</p> <p>To use this, you will need to install cohere first:</p> <p><code>pip install cohere</code></p> <p>Then, get yourself an API key and use Cohere's API as follows:</p> <pre><code>import cohere\nfrom keybert.llm import Cohere\nfrom keybert import KeyLLM\n\n# Create your LLM\nco = cohere.Client(my_api_key)\nllm = Cohere(co)\n\n# Load it in KeyLLM\nkw_model = KeyLLM(llm)\n\n# Extract keywords\ndocument = \"The website mentions that it only takes a couple of days to deliver but I still have not received mine.\"\nkeywords = kw_model.extract_keywords(document)\n</code></pre> <p>You can also use a custom prompt:</p> <pre><code>prompt = \"I have the following document: [DOCUMENT]. What keywords does it contain? Make sure to separate the keywords with commas.\"\nllm = Cohere(co, prompt=prompt)\n</code></pre> Source code in <code>keybert\\llm\\_cohere.py</code> <pre><code>class Cohere(BaseLLM):\n    \"\"\"Use the Cohere API to generate topic labels based on their\n    generative model.\n\n    Find more about their models here:\n    https://docs.cohere.ai/docs\n\n    NOTE: The resulting keywords are expected to be separated by commas so\n    any changes to the prompt will have to make sure that the resulting\n    keywords are comma-separated.\n\n    Arguments:\n        client: A cohere.Client\n        model: Model to use within Cohere, defaults to `\"xlarge\"`.\n        prompt: The prompt to be used in the model. If no prompt is given,\n                `self.default_prompt_` is used instead.\n                NOTE: Use `\"[KEYWORDS]\"` and `\"[DOCUMENTS]\"` in the prompt\n                to decide where the keywords and documents need to be\n                inserted.\n        delay_in_seconds: The delay in seconds between consecutive prompts\n                          in order to prevent RateLimitErrors.\n        verbose: Set this to True if you want to see a progress bar for the\n                 keyword extraction.\n\n    Usage:\n\n    To use this, you will need to install cohere first:\n\n    `pip install cohere`\n\n    Then, get yourself an API key and use Cohere's API as follows:\n\n    ```python\n    import cohere\n    from keybert.llm import Cohere\n    from keybert import KeyLLM\n\n    # Create your LLM\n    co = cohere.Client(my_api_key)\n    llm = Cohere(co)\n\n    # Load it in KeyLLM\n    kw_model = KeyLLM(llm)\n\n    # Extract keywords\n    document = \"The website mentions that it only takes a couple of days to deliver but I still have not received mine.\"\n    keywords = kw_model.extract_keywords(document)\n    ```\n\n    You can also use a custom prompt:\n\n    ```python\n    prompt = \"I have the following document: [DOCUMENT]. What keywords does it contain? Make sure to separate the keywords with commas.\"\n    llm = Cohere(co, prompt=prompt)\n    ```\n    \"\"\"\n\n    def __init__(\n        self, client, model: str = \"command\", prompt: str = None, delay_in_seconds: float = None, verbose: bool = False\n    ):\n        self.client = client\n        self.model = model\n        self.prompt = prompt if prompt is not None else DEFAULT_PROMPT\n        self.default_prompt_ = DEFAULT_PROMPT\n        self.delay_in_seconds = delay_in_seconds\n        self.verbose = verbose\n\n    def extract_keywords(self, documents: List[str], candidate_keywords: List[List[str]] = None):\n        \"\"\"Extract topics.\n\n        Arguments:\n            documents: The documents to extract keywords from\n            candidate_keywords: A list of candidate keywords that the LLM will fine-tune\n                        For example, it will create a nicer representation of\n                        the candidate keywords, remove redundant keywords, or\n                        shorten them depending on the input prompt.\n\n        Returns:\n            all_keywords: All keywords for each document\n        \"\"\"\n        all_keywords = []\n        candidate_keywords = process_candidate_keywords(documents, candidate_keywords)\n\n        for document, candidates in tqdm(zip(documents, candidate_keywords), disable=not self.verbose):\n            prompt = self.prompt.replace(\"[DOCUMENT]\", document)\n            if candidates is not None:\n                prompt = prompt.replace(\"[CANDIDATES]\", \", \".join(candidates))\n\n            # Delay\n            if self.delay_in_seconds:\n                time.sleep(self.delay_in_seconds)\n\n            request = self.client.generate(\n                model=self.model, prompt=prompt, max_tokens=50, num_generations=1, stop_sequences=[\"\\n\"]\n            )\n            keywords = request.generations[0].text.strip()\n            keywords = [keyword.strip() for keyword in keywords.split(\",\")]\n            all_keywords.append(keywords)\n\n        return all_keywords\n</code></pre>"},{"location":"api/cohere.html#keybert.llm._cohere.Cohere.extract_keywords","title":"<code>extract_keywords(documents, candidate_keywords=None)</code>","text":"<p>Extract topics.</p> <p>Parameters:</p> Name Type Description Default <code>documents</code> <code>List[str]</code> <p>The documents to extract keywords from</p> required <code>candidate_keywords</code> <code>List[List[str]]</code> <p>A list of candidate keywords that the LLM will fine-tune         For example, it will create a nicer representation of         the candidate keywords, remove redundant keywords, or         shorten them depending on the input prompt.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>all_keywords</code> <p>All keywords for each document</p> Source code in <code>keybert\\llm\\_cohere.py</code> <pre><code>def extract_keywords(self, documents: List[str], candidate_keywords: List[List[str]] = None):\n    \"\"\"Extract topics.\n\n    Arguments:\n        documents: The documents to extract keywords from\n        candidate_keywords: A list of candidate keywords that the LLM will fine-tune\n                    For example, it will create a nicer representation of\n                    the candidate keywords, remove redundant keywords, or\n                    shorten them depending on the input prompt.\n\n    Returns:\n        all_keywords: All keywords for each document\n    \"\"\"\n    all_keywords = []\n    candidate_keywords = process_candidate_keywords(documents, candidate_keywords)\n\n    for document, candidates in tqdm(zip(documents, candidate_keywords), disable=not self.verbose):\n        prompt = self.prompt.replace(\"[DOCUMENT]\", document)\n        if candidates is not None:\n            prompt = prompt.replace(\"[CANDIDATES]\", \", \".join(candidates))\n\n        # Delay\n        if self.delay_in_seconds:\n            time.sleep(self.delay_in_seconds)\n\n        request = self.client.generate(\n            model=self.model, prompt=prompt, max_tokens=50, num_generations=1, stop_sequences=[\"\\n\"]\n        )\n        keywords = request.generations[0].text.strip()\n        keywords = [keyword.strip() for keyword in keywords.split(\",\")]\n        all_keywords.append(keywords)\n\n    return all_keywords\n</code></pre>"},{"location":"api/keybert.html","title":"<code>KeyBERT</code>","text":"<p>A minimal method for keyword extraction with BERT.</p> <p>The keyword extraction is done by finding the sub-phrases in a document that are the most similar to the document itself.</p> <p>First, document embeddings are extracted with BERT to get a document-level representation. Then, word embeddings are extracted for N-gram words/phrases. Finally, we use cosine similarity to find the words/phrases that are the most similar to the document.</p> <p>The most similar words could then be identified as the words that best describe the entire document.</p> Input DocumentTokenize WordsEmbed TokensExtract EmbeddingsEmbed DocumentCalculateCosine SimilarityMost microbats use echolocationto navigate and find food.Most microbats use echolocationto navigate and find food.mostmicrobatsuse echolocationtonavigate andfindfood0.110.550.28............0.720.960.34mostfoodMost microbats...mostfood.......08.73We use the CountVectorizer from Scikit-Learn to tokenize our document into candidate kewords/keyphrases.We can use any language model that can embed both documents and keywords, like sentence-transformers.We calculate the cosine similarity between all candidate keywords and the input document. The keywords that have the largest similarity to the document are extracted. Source code in <code>keybert\\_model.py</code> <pre><code>class KeyBERT:\n    \"\"\"A minimal method for keyword extraction with BERT.\n\n    The keyword extraction is done by finding the sub-phrases in\n    a document that are the most similar to the document itself.\n\n    First, document embeddings are extracted with BERT to get a\n    document-level representation. Then, word embeddings are extracted\n    for N-gram words/phrases. Finally, we use cosine similarity to find the\n    words/phrases that are the most similar to the document.\n\n    The most similar words could then be identified as the words that\n    best describe the entire document.\n\n    &lt;div class=\"excalidraw\"&gt;\n    --8&lt;-- \"docs/images/pipeline.svg\"\n    &lt;/div&gt;\n    \"\"\"\n\n    def __init__(\n        self,\n        model=\"all-MiniLM-L6-v2\",\n        llm: BaseLLM = None,\n    ):\n        \"\"\"KeyBERT initialization.\n\n        Arguments:\n            model: Use a custom embedding model or a specific KeyBERT Backend.\n                   The following backends are currently supported:\n                      * SentenceTransformers\n                      * \ud83e\udd17 Transformers\n                      * Flair\n                      * Spacy\n                      * Gensim\n                      * USE (TF-Hub)\n                    You can also pass in a string that points to one of the following\n                    sentence-transformers models:\n                      * https://www.sbert.net/docs/pretrained_models.html\n            llm: The Large Language Model used to extract keywords\n        \"\"\"\n        self.model = select_backend(model)\n\n        if isinstance(llm, BaseLLM):\n            self.llm = KeyLLM(llm)\n        else:\n            self.llm = llm\n\n    def extract_keywords(\n        self,\n        docs: Union[str, List[str]],\n        candidates: List[str] = None,\n        keyphrase_ngram_range: Tuple[int, int] = (1, 1),\n        stop_words: Union[str, List[str]] = \"english\",\n        top_n: int = 5,\n        min_df: int = 1,\n        use_maxsum: bool = False,\n        use_mmr: bool = False,\n        diversity: float = 0.5,\n        nr_candidates: int = 20,\n        vectorizer: CountVectorizer = None,\n        highlight: bool = False,\n        seed_keywords: Union[List[str], List[List[str]]] = None,\n        doc_embeddings: np.array = None,\n        word_embeddings: np.array = None,\n        threshold: float = None,\n    ) -&gt; Union[List[Tuple[str, float]], List[List[Tuple[str, float]]]]:\n        \"\"\"Extract keywords and/or keyphrases.\n\n        To get the biggest speed-up, make sure to pass multiple documents\n        at once instead of iterating over a single document.\n\n        Arguments:\n            docs: The document(s) for which to extract keywords/keyphrases\n            candidates: Candidate keywords/keyphrases to use instead of extracting them from the document(s)\n                        NOTE: This is not used if you passed a `vectorizer`.\n            keyphrase_ngram_range: Length, in words, of the extracted keywords/keyphrases.\n                                   NOTE: This is not used if you passed a `vectorizer`.\n            stop_words: Stopwords to remove from the document.\n                        NOTE: This is not used if you passed a `vectorizer`.\n            top_n: Return the top n keywords/keyphrases\n            min_df: Minimum document frequency of a word across all documents\n                    if keywords for multiple documents need to be extracted.\n                    NOTE: This is not used if you passed a `vectorizer`.\n            use_maxsum: Whether to use Max Sum Distance for the selection\n                        of keywords/keyphrases.\n            use_mmr: Whether to use Maximal Marginal Relevance (MMR) for the\n                     selection of keywords/keyphrases.\n            diversity: The diversity of the results between 0 and 1 if `use_mmr`\n                       is set to True.\n            nr_candidates: The number of candidates to consider if `use_maxsum` is\n                           set to True.\n            vectorizer: Pass in your own `CountVectorizer` from\n                        `sklearn.feature_extraction.text.CountVectorizer`\n            highlight: Whether to print the document and highlight its keywords/keyphrases.\n                       NOTE: This does not work if multiple documents are passed.\n            seed_keywords: Seed keywords that may guide the extraction of keywords by\n                           steering the similarities towards the seeded keywords.\n                           NOTE: when multiple documents are passed,\n                           `seed_keywords`funtions in either of the two ways below:\n                           - globally: when a flat list of str is passed, keywords are shared by all documents,\n                           - locally: when a nested list of str is passed, keywords differs among documents.\n            doc_embeddings: The embeddings of each document.\n            word_embeddings: The embeddings of each potential keyword/keyphrase across\n                             across the vocabulary of the set of input documents.\n                             NOTE: The `word_embeddings` should be generated through\n                             `.extract_embeddings` as the order of these embeddings depend\n                             on the vectorizer that was used to generate its vocabulary.\n            threshold: Minimum similarity value between 0 and 1 used to decide how similar documents need to receive the same keywords.\n\n        Returns:\n            keywords: The top n keywords for a document with their respective distances\n                      to the input document.\n\n        Usage:\n\n        To extract keywords from a single document:\n\n        ```python\n        from keybert import KeyBERT\n\n        kw_model = KeyBERT()\n        keywords = kw_model.extract_keywords(doc)\n        ```\n\n        To extract keywords from multiple documents, which is typically quite a bit faster:\n\n        ```python\n        from keybert import KeyBERT\n\n        kw_model = KeyBERT()\n        keywords = kw_model.extract_keywords(docs)\n        ```\n        \"\"\"\n        # Check for a single, empty document\n        if isinstance(docs, str):\n            if docs:\n                docs = [docs]\n            else:\n                return []\n\n        # Extract potential words using a vectorizer / tokenizer\n        if vectorizer:\n            count = vectorizer.fit(docs)\n        else:\n            try:\n                count = CountVectorizer(\n                    ngram_range=keyphrase_ngram_range,\n                    stop_words=stop_words,\n                    min_df=min_df,\n                    vocabulary=candidates,\n                ).fit(docs)\n            except ValueError:\n                return []\n\n        # Scikit-Learn Deprecation: get_feature_names is deprecated in 1.0\n        # and will be removed in 1.2. Please use get_feature_names_out instead.\n        if version.parse(sklearn_version) &gt;= version.parse(\"1.0.0\"):\n            words = count.get_feature_names_out()\n        else:\n            words = count.get_feature_names()\n        df = count.transform(docs)\n\n        # Check if the right number of word embeddings are generated compared with the vectorizer\n        if word_embeddings is not None:\n            if word_embeddings.shape[0] != len(words):\n                raise ValueError(\n                    \"Make sure that the `word_embeddings` are generated from the function \"\n                    \"`.extract_embeddings`. \\nMoreover, the `candidates`, `keyphrase_ngram_range`,\"\n                    \"`stop_words`, and `min_df` parameters need to have the same values in both \"\n                    \"`.extract_embeddings` and `.extract_keywords`.\"\n                )\n\n        # Extract embeddings\n        if doc_embeddings is None:\n            doc_embeddings = self.model.embed(docs)\n        if word_embeddings is None:\n            word_embeddings = self.model.embed(words)\n\n        # Guided KeyBERT either local (keywords shared among documents) or global (keywords per document)\n        if seed_keywords is not None:\n            if isinstance(seed_keywords[0], str):\n                seed_embeddings = self.model.embed(seed_keywords).mean(axis=0, keepdims=True)\n            elif len(docs) != len(seed_keywords):\n                raise ValueError(\"The length of docs must match the length of seed_keywords\")\n            else:\n                seed_embeddings = np.vstack(\n                    [self.model.embed(keywords).mean(axis=0, keepdims=True) for keywords in seed_keywords]\n                )\n            doc_embeddings = (doc_embeddings * 3 + seed_embeddings) / 4\n\n        # Find keywords\n        all_keywords = []\n        for index, _ in enumerate(docs):\n            try:\n                # Select embeddings\n                candidate_indices = df[index].nonzero()[1]\n                candidates = [words[index] for index in candidate_indices]\n                candidate_embeddings = word_embeddings[candidate_indices]\n                doc_embedding = doc_embeddings[index].reshape(1, -1)\n\n                # Maximal Marginal Relevance (MMR)\n                if use_mmr:\n                    keywords = mmr(\n                        doc_embedding,\n                        candidate_embeddings,\n                        candidates,\n                        top_n,\n                        diversity,\n                    )\n\n                # Max Sum Distance\n                elif use_maxsum:\n                    keywords = max_sum_distance(\n                        doc_embedding,\n                        candidate_embeddings,\n                        candidates,\n                        top_n,\n                        nr_candidates,\n                    )\n\n                # Cosine-based keyword extraction\n                else:\n                    distances = cosine_similarity(doc_embedding, candidate_embeddings)\n                    keywords = [\n                        (candidates[index], round(float(distances[0][index]), 4))\n                        for index in distances.argsort()[0][-top_n:]\n                    ][::-1]\n\n                all_keywords.append(keywords)\n\n            # Capturing empty keywords\n            except ValueError:\n                all_keywords.append([])\n\n        # Highlight keywords in the document\n        if len(all_keywords) == 1:\n            if highlight:\n                highlight_document(docs[0], all_keywords[0], count)\n            all_keywords = all_keywords[0]\n\n        # Fine-tune keywords using an LLM\n        if self.llm is not None:\n            import torch\n\n            doc_embeddings = torch.from_numpy(doc_embeddings).float()\n            if torch.cuda.is_available():\n                doc_embeddings = doc_embeddings.to(\"cuda\")\n            if isinstance(all_keywords[0], tuple):\n                candidate_keywords = [[keyword[0] for keyword in all_keywords]]\n            else:\n                candidate_keywords = [[keyword[0] for keyword in keywords] for keywords in all_keywords]\n            keywords = self.llm.extract_keywords(\n                docs,\n                embeddings=doc_embeddings,\n                candidate_keywords=candidate_keywords,\n                threshold=threshold,\n            )\n            return keywords\n        return all_keywords\n\n    def extract_embeddings(\n        self,\n        docs: Union[str, List[str]],\n        candidates: List[str] = None,\n        keyphrase_ngram_range: Tuple[int, int] = (1, 1),\n        stop_words: Union[str, List[str]] = \"english\",\n        min_df: int = 1,\n        vectorizer: CountVectorizer = None,\n    ) -&gt; Union[List[Tuple[str, float]], List[List[Tuple[str, float]]]]:\n        \"\"\"Extract document and word embeddings for the input documents and the\n        generated candidate keywords/keyphrases respectively.\n\n        Note that all potential keywords/keyphrases are not returned but only their\n        word embeddings. This means that the values of `candidates`, `keyphrase_ngram_range`,\n        `stop_words`, and `min_df` need to be the same between using `.extract_embeddings` and\n        `.extract_keywords`.\n\n        Arguments:\n            docs: The document(s) for which to extract keywords/keyphrases\n            candidates: Candidate keywords/keyphrases to use instead of extracting them from the document(s)\n                        NOTE: This is not used if you passed a `vectorizer`.\n            keyphrase_ngram_range: Length, in words, of the extracted keywords/keyphrases.\n                                   NOTE: This is not used if you passed a `vectorizer`.\n            stop_words: Stopwords to remove from the document.\n                        NOTE: This is not used if you passed a `vectorizer`.\n            min_df: Minimum document frequency of a word across all documents\n                    if keywords for multiple documents need to be extracted.\n                    NOTE: This is not used if you passed a `vectorizer`.\n            vectorizer: Pass in your own `CountVectorizer` from\n                        `sklearn.feature_extraction.text.CountVectorizer`\n\n        Returns:\n            doc_embeddings: The embeddings of each document.\n            word_embeddings: The embeddings of each potential keyword/keyphrase across\n                             across the vocabulary of the set of input documents.\n                             NOTE: The `word_embeddings` should be generated through\n                             `.extract_embeddings` as the order of these embeddings depend\n                             on the vectorizer that was used to generate its vocabulary.\n\n        Usage:\n\n        To generate the word and document embeddings from a set of documents:\n\n        ```python\n        from keybert import KeyBERT\n\n        kw_model = KeyBERT()\n        doc_embeddings, word_embeddings = kw_model.extract_embeddings(docs)\n        ```\n\n        You can then use these embeddings and pass them to `.extract_keywords` to speed up the tuning the model:\n\n        ```python\n        keywords = kw_model.extract_keywords(docs, doc_embeddings=doc_embeddings, word_embeddings=word_embeddings)\n        ```\n        \"\"\"\n        # Check for a single, empty document\n        if isinstance(docs, str):\n            if docs:\n                docs = [docs]\n            else:\n                return []\n\n        # Extract potential words using a vectorizer / tokenizer\n        if vectorizer:\n            count = vectorizer.fit(docs)\n        else:\n            try:\n                count = CountVectorizer(\n                    ngram_range=keyphrase_ngram_range,\n                    stop_words=stop_words,\n                    min_df=min_df,\n                    vocabulary=candidates,\n                ).fit(docs)\n            except ValueError:\n                return []\n\n        # Scikit-Learn Deprecation: get_feature_names is deprecated in 1.0\n        # and will be removed in 1.2. Please use get_feature_names_out instead.\n        if version.parse(sklearn_version) &gt;= version.parse(\"1.0.0\"):\n            words = count.get_feature_names_out()\n        else:\n            words = count.get_feature_names()\n\n        doc_embeddings = self.model.embed(docs)\n        word_embeddings = self.model.embed(words)\n\n        return doc_embeddings, word_embeddings\n</code></pre>"},{"location":"api/keybert.html#keybert._model.KeyBERT.__init__","title":"<code>__init__(model='all-MiniLM-L6-v2', llm=None)</code>","text":"<p>KeyBERT initialization.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <p>Use a custom embedding model or a specific KeyBERT Backend.    The following backends are currently supported:       * SentenceTransformers       * \ud83e\udd17 Transformers       * Flair       * Spacy       * Gensim       * USE (TF-Hub)     You can also pass in a string that points to one of the following     sentence-transformers models:       * https://www.sbert.net/docs/pretrained_models.html</p> <code>'all-MiniLM-L6-v2'</code> <code>llm</code> <code>BaseLLM</code> <p>The Large Language Model used to extract keywords</p> <code>None</code> Source code in <code>keybert\\_model.py</code> <pre><code>def __init__(\n    self,\n    model=\"all-MiniLM-L6-v2\",\n    llm: BaseLLM = None,\n):\n    \"\"\"KeyBERT initialization.\n\n    Arguments:\n        model: Use a custom embedding model or a specific KeyBERT Backend.\n               The following backends are currently supported:\n                  * SentenceTransformers\n                  * \ud83e\udd17 Transformers\n                  * Flair\n                  * Spacy\n                  * Gensim\n                  * USE (TF-Hub)\n                You can also pass in a string that points to one of the following\n                sentence-transformers models:\n                  * https://www.sbert.net/docs/pretrained_models.html\n        llm: The Large Language Model used to extract keywords\n    \"\"\"\n    self.model = select_backend(model)\n\n    if isinstance(llm, BaseLLM):\n        self.llm = KeyLLM(llm)\n    else:\n        self.llm = llm\n</code></pre>"},{"location":"api/keybert.html#keybert._model.KeyBERT.extract_embeddings","title":"<code>extract_embeddings(docs, candidates=None, keyphrase_ngram_range=(1, 1), stop_words='english', min_df=1, vectorizer=None)</code>","text":"<p>Extract document and word embeddings for the input documents and the generated candidate keywords/keyphrases respectively.</p> <p>Note that all potential keywords/keyphrases are not returned but only their word embeddings. This means that the values of <code>candidates</code>, <code>keyphrase_ngram_range</code>, <code>stop_words</code>, and <code>min_df</code> need to be the same between using <code>.extract_embeddings</code> and <code>.extract_keywords</code>.</p> <p>Parameters:</p> Name Type Description Default <code>docs</code> <code>Union[str, List[str]]</code> <p>The document(s) for which to extract keywords/keyphrases</p> required <code>candidates</code> <code>List[str]</code> <p>Candidate keywords/keyphrases to use instead of extracting them from the document(s)         NOTE: This is not used if you passed a <code>vectorizer</code>.</p> <code>None</code> <code>keyphrase_ngram_range</code> <code>Tuple[int, int]</code> <p>Length, in words, of the extracted keywords/keyphrases.                    NOTE: This is not used if you passed a <code>vectorizer</code>.</p> <code>(1, 1)</code> <code>stop_words</code> <code>Union[str, List[str]]</code> <p>Stopwords to remove from the document.         NOTE: This is not used if you passed a <code>vectorizer</code>.</p> <code>'english'</code> <code>min_df</code> <code>int</code> <p>Minimum document frequency of a word across all documents     if keywords for multiple documents need to be extracted.     NOTE: This is not used if you passed a <code>vectorizer</code>.</p> <code>1</code> <code>vectorizer</code> <code>CountVectorizer</code> <p>Pass in your own <code>CountVectorizer</code> from         <code>sklearn.feature_extraction.text.CountVectorizer</code></p> <code>None</code> <p>Returns:</p> Name Type Description <code>doc_embeddings</code> <code>Union[List[Tuple[str, float]], List[List[Tuple[str, float]]]]</code> <p>The embeddings of each document.</p> <code>word_embeddings</code> <code>Union[List[Tuple[str, float]], List[List[Tuple[str, float]]]]</code> <p>The embeddings of each potential keyword/keyphrase across              across the vocabulary of the set of input documents.              NOTE: The <code>word_embeddings</code> should be generated through              <code>.extract_embeddings</code> as the order of these embeddings depend              on the vectorizer that was used to generate its vocabulary.</p> <p>Usage:</p> <p>To generate the word and document embeddings from a set of documents:</p> <pre><code>from keybert import KeyBERT\n\nkw_model = KeyBERT()\ndoc_embeddings, word_embeddings = kw_model.extract_embeddings(docs)\n</code></pre> <p>You can then use these embeddings and pass them to <code>.extract_keywords</code> to speed up the tuning the model:</p> <pre><code>keywords = kw_model.extract_keywords(docs, doc_embeddings=doc_embeddings, word_embeddings=word_embeddings)\n</code></pre> Source code in <code>keybert\\_model.py</code> <pre><code>def extract_embeddings(\n    self,\n    docs: Union[str, List[str]],\n    candidates: List[str] = None,\n    keyphrase_ngram_range: Tuple[int, int] = (1, 1),\n    stop_words: Union[str, List[str]] = \"english\",\n    min_df: int = 1,\n    vectorizer: CountVectorizer = None,\n) -&gt; Union[List[Tuple[str, float]], List[List[Tuple[str, float]]]]:\n    \"\"\"Extract document and word embeddings for the input documents and the\n    generated candidate keywords/keyphrases respectively.\n\n    Note that all potential keywords/keyphrases are not returned but only their\n    word embeddings. This means that the values of `candidates`, `keyphrase_ngram_range`,\n    `stop_words`, and `min_df` need to be the same between using `.extract_embeddings` and\n    `.extract_keywords`.\n\n    Arguments:\n        docs: The document(s) for which to extract keywords/keyphrases\n        candidates: Candidate keywords/keyphrases to use instead of extracting them from the document(s)\n                    NOTE: This is not used if you passed a `vectorizer`.\n        keyphrase_ngram_range: Length, in words, of the extracted keywords/keyphrases.\n                               NOTE: This is not used if you passed a `vectorizer`.\n        stop_words: Stopwords to remove from the document.\n                    NOTE: This is not used if you passed a `vectorizer`.\n        min_df: Minimum document frequency of a word across all documents\n                if keywords for multiple documents need to be extracted.\n                NOTE: This is not used if you passed a `vectorizer`.\n        vectorizer: Pass in your own `CountVectorizer` from\n                    `sklearn.feature_extraction.text.CountVectorizer`\n\n    Returns:\n        doc_embeddings: The embeddings of each document.\n        word_embeddings: The embeddings of each potential keyword/keyphrase across\n                         across the vocabulary of the set of input documents.\n                         NOTE: The `word_embeddings` should be generated through\n                         `.extract_embeddings` as the order of these embeddings depend\n                         on the vectorizer that was used to generate its vocabulary.\n\n    Usage:\n\n    To generate the word and document embeddings from a set of documents:\n\n    ```python\n    from keybert import KeyBERT\n\n    kw_model = KeyBERT()\n    doc_embeddings, word_embeddings = kw_model.extract_embeddings(docs)\n    ```\n\n    You can then use these embeddings and pass them to `.extract_keywords` to speed up the tuning the model:\n\n    ```python\n    keywords = kw_model.extract_keywords(docs, doc_embeddings=doc_embeddings, word_embeddings=word_embeddings)\n    ```\n    \"\"\"\n    # Check for a single, empty document\n    if isinstance(docs, str):\n        if docs:\n            docs = [docs]\n        else:\n            return []\n\n    # Extract potential words using a vectorizer / tokenizer\n    if vectorizer:\n        count = vectorizer.fit(docs)\n    else:\n        try:\n            count = CountVectorizer(\n                ngram_range=keyphrase_ngram_range,\n                stop_words=stop_words,\n                min_df=min_df,\n                vocabulary=candidates,\n            ).fit(docs)\n        except ValueError:\n            return []\n\n    # Scikit-Learn Deprecation: get_feature_names is deprecated in 1.0\n    # and will be removed in 1.2. Please use get_feature_names_out instead.\n    if version.parse(sklearn_version) &gt;= version.parse(\"1.0.0\"):\n        words = count.get_feature_names_out()\n    else:\n        words = count.get_feature_names()\n\n    doc_embeddings = self.model.embed(docs)\n    word_embeddings = self.model.embed(words)\n\n    return doc_embeddings, word_embeddings\n</code></pre>"},{"location":"api/keybert.html#keybert._model.KeyBERT.extract_keywords","title":"<code>extract_keywords(docs, candidates=None, keyphrase_ngram_range=(1, 1), stop_words='english', top_n=5, min_df=1, use_maxsum=False, use_mmr=False, diversity=0.5, nr_candidates=20, vectorizer=None, highlight=False, seed_keywords=None, doc_embeddings=None, word_embeddings=None, threshold=None)</code>","text":"<p>Extract keywords and/or keyphrases.</p> <p>To get the biggest speed-up, make sure to pass multiple documents at once instead of iterating over a single document.</p> <p>Parameters:</p> Name Type Description Default <code>docs</code> <code>Union[str, List[str]]</code> <p>The document(s) for which to extract keywords/keyphrases</p> required <code>candidates</code> <code>List[str]</code> <p>Candidate keywords/keyphrases to use instead of extracting them from the document(s)         NOTE: This is not used if you passed a <code>vectorizer</code>.</p> <code>None</code> <code>keyphrase_ngram_range</code> <code>Tuple[int, int]</code> <p>Length, in words, of the extracted keywords/keyphrases.                    NOTE: This is not used if you passed a <code>vectorizer</code>.</p> <code>(1, 1)</code> <code>stop_words</code> <code>Union[str, List[str]]</code> <p>Stopwords to remove from the document.         NOTE: This is not used if you passed a <code>vectorizer</code>.</p> <code>'english'</code> <code>top_n</code> <code>int</code> <p>Return the top n keywords/keyphrases</p> <code>5</code> <code>min_df</code> <code>int</code> <p>Minimum document frequency of a word across all documents     if keywords for multiple documents need to be extracted.     NOTE: This is not used if you passed a <code>vectorizer</code>.</p> <code>1</code> <code>use_maxsum</code> <code>bool</code> <p>Whether to use Max Sum Distance for the selection         of keywords/keyphrases.</p> <code>False</code> <code>use_mmr</code> <code>bool</code> <p>Whether to use Maximal Marginal Relevance (MMR) for the      selection of keywords/keyphrases.</p> <code>False</code> <code>diversity</code> <code>float</code> <p>The diversity of the results between 0 and 1 if <code>use_mmr</code>        is set to True.</p> <code>0.5</code> <code>nr_candidates</code> <code>int</code> <p>The number of candidates to consider if <code>use_maxsum</code> is            set to True.</p> <code>20</code> <code>vectorizer</code> <code>CountVectorizer</code> <p>Pass in your own <code>CountVectorizer</code> from         <code>sklearn.feature_extraction.text.CountVectorizer</code></p> <code>None</code> <code>highlight</code> <code>bool</code> <p>Whether to print the document and highlight its keywords/keyphrases.        NOTE: This does not work if multiple documents are passed.</p> <code>False</code> <code>seed_keywords</code> <code>Union[List[str], List[List[str]]]</code> <p>Seed keywords that may guide the extraction of keywords by            steering the similarities towards the seeded keywords.            NOTE: when multiple documents are passed,            <code>seed_keywords</code>funtions in either of the two ways below:            - globally: when a flat list of str is passed, keywords are shared by all documents,            - locally: when a nested list of str is passed, keywords differs among documents.</p> <code>None</code> <code>doc_embeddings</code> <code>array</code> <p>The embeddings of each document.</p> <code>None</code> <code>word_embeddings</code> <code>array</code> <p>The embeddings of each potential keyword/keyphrase across              across the vocabulary of the set of input documents.              NOTE: The <code>word_embeddings</code> should be generated through              <code>.extract_embeddings</code> as the order of these embeddings depend              on the vectorizer that was used to generate its vocabulary.</p> <code>None</code> <code>threshold</code> <code>float</code> <p>Minimum similarity value between 0 and 1 used to decide how similar documents need to receive the same keywords.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>keywords</code> <code>Union[List[Tuple[str, float]], List[List[Tuple[str, float]]]]</code> <p>The top n keywords for a document with their respective distances       to the input document.</p> <p>Usage:</p> <p>To extract keywords from a single document:</p> <pre><code>from keybert import KeyBERT\n\nkw_model = KeyBERT()\nkeywords = kw_model.extract_keywords(doc)\n</code></pre> <p>To extract keywords from multiple documents, which is typically quite a bit faster:</p> <pre><code>from keybert import KeyBERT\n\nkw_model = KeyBERT()\nkeywords = kw_model.extract_keywords(docs)\n</code></pre> Source code in <code>keybert\\_model.py</code> <pre><code>def extract_keywords(\n    self,\n    docs: Union[str, List[str]],\n    candidates: List[str] = None,\n    keyphrase_ngram_range: Tuple[int, int] = (1, 1),\n    stop_words: Union[str, List[str]] = \"english\",\n    top_n: int = 5,\n    min_df: int = 1,\n    use_maxsum: bool = False,\n    use_mmr: bool = False,\n    diversity: float = 0.5,\n    nr_candidates: int = 20,\n    vectorizer: CountVectorizer = None,\n    highlight: bool = False,\n    seed_keywords: Union[List[str], List[List[str]]] = None,\n    doc_embeddings: np.array = None,\n    word_embeddings: np.array = None,\n    threshold: float = None,\n) -&gt; Union[List[Tuple[str, float]], List[List[Tuple[str, float]]]]:\n    \"\"\"Extract keywords and/or keyphrases.\n\n    To get the biggest speed-up, make sure to pass multiple documents\n    at once instead of iterating over a single document.\n\n    Arguments:\n        docs: The document(s) for which to extract keywords/keyphrases\n        candidates: Candidate keywords/keyphrases to use instead of extracting them from the document(s)\n                    NOTE: This is not used if you passed a `vectorizer`.\n        keyphrase_ngram_range: Length, in words, of the extracted keywords/keyphrases.\n                               NOTE: This is not used if you passed a `vectorizer`.\n        stop_words: Stopwords to remove from the document.\n                    NOTE: This is not used if you passed a `vectorizer`.\n        top_n: Return the top n keywords/keyphrases\n        min_df: Minimum document frequency of a word across all documents\n                if keywords for multiple documents need to be extracted.\n                NOTE: This is not used if you passed a `vectorizer`.\n        use_maxsum: Whether to use Max Sum Distance for the selection\n                    of keywords/keyphrases.\n        use_mmr: Whether to use Maximal Marginal Relevance (MMR) for the\n                 selection of keywords/keyphrases.\n        diversity: The diversity of the results between 0 and 1 if `use_mmr`\n                   is set to True.\n        nr_candidates: The number of candidates to consider if `use_maxsum` is\n                       set to True.\n        vectorizer: Pass in your own `CountVectorizer` from\n                    `sklearn.feature_extraction.text.CountVectorizer`\n        highlight: Whether to print the document and highlight its keywords/keyphrases.\n                   NOTE: This does not work if multiple documents are passed.\n        seed_keywords: Seed keywords that may guide the extraction of keywords by\n                       steering the similarities towards the seeded keywords.\n                       NOTE: when multiple documents are passed,\n                       `seed_keywords`funtions in either of the two ways below:\n                       - globally: when a flat list of str is passed, keywords are shared by all documents,\n                       - locally: when a nested list of str is passed, keywords differs among documents.\n        doc_embeddings: The embeddings of each document.\n        word_embeddings: The embeddings of each potential keyword/keyphrase across\n                         across the vocabulary of the set of input documents.\n                         NOTE: The `word_embeddings` should be generated through\n                         `.extract_embeddings` as the order of these embeddings depend\n                         on the vectorizer that was used to generate its vocabulary.\n        threshold: Minimum similarity value between 0 and 1 used to decide how similar documents need to receive the same keywords.\n\n    Returns:\n        keywords: The top n keywords for a document with their respective distances\n                  to the input document.\n\n    Usage:\n\n    To extract keywords from a single document:\n\n    ```python\n    from keybert import KeyBERT\n\n    kw_model = KeyBERT()\n    keywords = kw_model.extract_keywords(doc)\n    ```\n\n    To extract keywords from multiple documents, which is typically quite a bit faster:\n\n    ```python\n    from keybert import KeyBERT\n\n    kw_model = KeyBERT()\n    keywords = kw_model.extract_keywords(docs)\n    ```\n    \"\"\"\n    # Check for a single, empty document\n    if isinstance(docs, str):\n        if docs:\n            docs = [docs]\n        else:\n            return []\n\n    # Extract potential words using a vectorizer / tokenizer\n    if vectorizer:\n        count = vectorizer.fit(docs)\n    else:\n        try:\n            count = CountVectorizer(\n                ngram_range=keyphrase_ngram_range,\n                stop_words=stop_words,\n                min_df=min_df,\n                vocabulary=candidates,\n            ).fit(docs)\n        except ValueError:\n            return []\n\n    # Scikit-Learn Deprecation: get_feature_names is deprecated in 1.0\n    # and will be removed in 1.2. Please use get_feature_names_out instead.\n    if version.parse(sklearn_version) &gt;= version.parse(\"1.0.0\"):\n        words = count.get_feature_names_out()\n    else:\n        words = count.get_feature_names()\n    df = count.transform(docs)\n\n    # Check if the right number of word embeddings are generated compared with the vectorizer\n    if word_embeddings is not None:\n        if word_embeddings.shape[0] != len(words):\n            raise ValueError(\n                \"Make sure that the `word_embeddings` are generated from the function \"\n                \"`.extract_embeddings`. \\nMoreover, the `candidates`, `keyphrase_ngram_range`,\"\n                \"`stop_words`, and `min_df` parameters need to have the same values in both \"\n                \"`.extract_embeddings` and `.extract_keywords`.\"\n            )\n\n    # Extract embeddings\n    if doc_embeddings is None:\n        doc_embeddings = self.model.embed(docs)\n    if word_embeddings is None:\n        word_embeddings = self.model.embed(words)\n\n    # Guided KeyBERT either local (keywords shared among documents) or global (keywords per document)\n    if seed_keywords is not None:\n        if isinstance(seed_keywords[0], str):\n            seed_embeddings = self.model.embed(seed_keywords).mean(axis=0, keepdims=True)\n        elif len(docs) != len(seed_keywords):\n            raise ValueError(\"The length of docs must match the length of seed_keywords\")\n        else:\n            seed_embeddings = np.vstack(\n                [self.model.embed(keywords).mean(axis=0, keepdims=True) for keywords in seed_keywords]\n            )\n        doc_embeddings = (doc_embeddings * 3 + seed_embeddings) / 4\n\n    # Find keywords\n    all_keywords = []\n    for index, _ in enumerate(docs):\n        try:\n            # Select embeddings\n            candidate_indices = df[index].nonzero()[1]\n            candidates = [words[index] for index in candidate_indices]\n            candidate_embeddings = word_embeddings[candidate_indices]\n            doc_embedding = doc_embeddings[index].reshape(1, -1)\n\n            # Maximal Marginal Relevance (MMR)\n            if use_mmr:\n                keywords = mmr(\n                    doc_embedding,\n                    candidate_embeddings,\n                    candidates,\n                    top_n,\n                    diversity,\n                )\n\n            # Max Sum Distance\n            elif use_maxsum:\n                keywords = max_sum_distance(\n                    doc_embedding,\n                    candidate_embeddings,\n                    candidates,\n                    top_n,\n                    nr_candidates,\n                )\n\n            # Cosine-based keyword extraction\n            else:\n                distances = cosine_similarity(doc_embedding, candidate_embeddings)\n                keywords = [\n                    (candidates[index], round(float(distances[0][index]), 4))\n                    for index in distances.argsort()[0][-top_n:]\n                ][::-1]\n\n            all_keywords.append(keywords)\n\n        # Capturing empty keywords\n        except ValueError:\n            all_keywords.append([])\n\n    # Highlight keywords in the document\n    if len(all_keywords) == 1:\n        if highlight:\n            highlight_document(docs[0], all_keywords[0], count)\n        all_keywords = all_keywords[0]\n\n    # Fine-tune keywords using an LLM\n    if self.llm is not None:\n        import torch\n\n        doc_embeddings = torch.from_numpy(doc_embeddings).float()\n        if torch.cuda.is_available():\n            doc_embeddings = doc_embeddings.to(\"cuda\")\n        if isinstance(all_keywords[0], tuple):\n            candidate_keywords = [[keyword[0] for keyword in all_keywords]]\n        else:\n            candidate_keywords = [[keyword[0] for keyword in keywords] for keywords in all_keywords]\n        keywords = self.llm.extract_keywords(\n            docs,\n            embeddings=doc_embeddings,\n            candidate_keywords=candidate_keywords,\n            threshold=threshold,\n        )\n        return keywords\n    return all_keywords\n</code></pre>"},{"location":"api/keyllm.html","title":"<code>KeyLLM</code>","text":"<p>A minimal method for keyword extraction with Large Language Models (LLM).</p> <p>The keyword extraction is done by simply asking the LLM to extract a number of keywords from a single piece of text.</p> Source code in <code>keybert\\_llm.py</code> <pre><code>class KeyLLM:\n    \"\"\"A minimal method for keyword extraction with Large Language Models (LLM).\n\n    The keyword extraction is done by simply asking the LLM to extract a\n    number of keywords from a single piece of text.\n    \"\"\"\n\n    def __init__(self, llm):\n        \"\"\"KeyBERT initialization.\n\n        Arguments:\n            llm: The Large Language Model to use\n        \"\"\"\n        self.llm = llm\n\n    def extract_keywords(\n        self,\n        docs: Union[str, List[str]],\n        check_vocab: bool = False,\n        candidate_keywords: List[List[str]] = None,\n        threshold: float = None,\n        embeddings=None,\n    ) -&gt; Union[List[str], List[List[str]]]:\n        \"\"\"Extract keywords and/or keyphrases.\n\n        To get the biggest speed-up, make sure to pass multiple documents\n        at once instead of iterating over a single document.\n\n        NOTE: The resulting keywords are expected to be separated by commas so\n        any changes to the prompt will have to make sure that the resulting\n        keywords are comma-separated.\n\n        Arguments:\n            docs: The document(s) for which to extract keywords/keyphrases\n            check_vocab: Only return keywords that appear exactly in the documents\n            candidate_keywords: Candidate keywords for each document\n            threshold: Minimum similarity value between 0 and 1 used to decide how similar documents need to receive the same keywords.\n            embeddings: The embeddings of each document.\n\n        Returns:\n            keywords: The top n keywords for a document with their respective distances\n                      to the input document.\n\n        Usage:\n\n        To extract keywords from a single document:\n\n        ```python\n        import openai\n        from keybert.llm import OpenAI\n        from keybert import KeyLLM\n\n        # Create your LLM\n        client = openai.OpenAI(api_key=MY_API_KEY)\n        llm = OpenAI(client)\n\n        # Load it in KeyLLM\n        kw_model = KeyLLM(llm)\n\n        # Extract keywords\n        document = \"The website mentions that it only takes a couple of days to deliver but I still have not received mine.\"\n        keywords = kw_model.extract_keywords(document)\n        ```\n        \"\"\"\n        # Check for a single, empty document\n        if isinstance(docs, str):\n            if docs:\n                docs = [docs]\n            else:\n                return []\n\n        if HAS_SBERT and threshold is not None and embeddings is not None:\n            # Find similar documents\n            clusters = util.community_detection(embeddings, min_community_size=2, threshold=threshold)\n            in_cluster = set([cluster for cluster_set in clusters for cluster in cluster_set])\n            out_cluster = set(list(range(len(docs)))).difference(in_cluster)\n\n            # Extract keywords for all documents not in a cluster\n            if out_cluster:\n                selected_docs = [docs[index] for index in out_cluster]\n                if candidate_keywords is not None:\n                    selected_keywords = [candidate_keywords[index] for index in out_cluster]\n                else:\n                    selected_keywords = None\n                out_cluster_keywords = self.llm.extract_keywords(\n                    selected_docs,\n                    selected_keywords,\n                )\n                out_cluster_keywords = {index: words for words, index in zip(out_cluster_keywords, out_cluster)}\n\n            # Extract keywords for only the first document in a cluster\n            if in_cluster:\n                selected_docs = [docs[cluster[0]] for cluster in clusters]\n                if candidate_keywords is not None:\n                    selected_keywords = [candidate_keywords[cluster[0]] for cluster in clusters]\n                else:\n                    selected_keywords = None\n                in_cluster_keywords = self.llm.extract_keywords(selected_docs, selected_keywords)\n                in_cluster_keywords = {\n                    doc_id: in_cluster_keywords[index] for index, cluster in enumerate(clusters) for doc_id in cluster\n                }\n\n            # Update out cluster keywords with in cluster keywords\n            if out_cluster:\n                if in_cluster:\n                    out_cluster_keywords.update(in_cluster_keywords)\n                keywords = [out_cluster_keywords[index] for index in range(len(docs))]\n            else:\n                keywords = [in_cluster_keywords[index] for index in range(len(docs))]\n        else:\n            # Extract keywords using a Large Language Model (LLM)\n            keywords = self.llm.extract_keywords(docs, candidate_keywords)\n\n        # Only extract keywords that appear in the input document\n        if check_vocab:\n            updated_keywords = []\n            for keyword_set, document in zip(keywords, docs):\n                updated_keyword_set = []\n                for keyword in keyword_set:\n                    if keyword in document:\n                        updated_keyword_set.append(keyword)\n                updated_keywords.append(updated_keyword_set)\n            return updated_keywords\n\n        return keywords\n</code></pre>"},{"location":"api/keyllm.html#keybert._llm.KeyLLM.__init__","title":"<code>__init__(llm)</code>","text":"<p>KeyBERT initialization.</p> <p>Parameters:</p> Name Type Description Default <code>llm</code> <p>The Large Language Model to use</p> required Source code in <code>keybert\\_llm.py</code> <pre><code>def __init__(self, llm):\n    \"\"\"KeyBERT initialization.\n\n    Arguments:\n        llm: The Large Language Model to use\n    \"\"\"\n    self.llm = llm\n</code></pre>"},{"location":"api/keyllm.html#keybert._llm.KeyLLM.extract_keywords","title":"<code>extract_keywords(docs, check_vocab=False, candidate_keywords=None, threshold=None, embeddings=None)</code>","text":"<p>Extract keywords and/or keyphrases.</p> <p>To get the biggest speed-up, make sure to pass multiple documents at once instead of iterating over a single document.</p> <p>NOTE: The resulting keywords are expected to be separated by commas so any changes to the prompt will have to make sure that the resulting keywords are comma-separated.</p> <p>Parameters:</p> Name Type Description Default <code>docs</code> <code>Union[str, List[str]]</code> <p>The document(s) for which to extract keywords/keyphrases</p> required <code>check_vocab</code> <code>bool</code> <p>Only return keywords that appear exactly in the documents</p> <code>False</code> <code>candidate_keywords</code> <code>List[List[str]]</code> <p>Candidate keywords for each document</p> <code>None</code> <code>threshold</code> <code>float</code> <p>Minimum similarity value between 0 and 1 used to decide how similar documents need to receive the same keywords.</p> <code>None</code> <code>embeddings</code> <p>The embeddings of each document.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>keywords</code> <code>Union[List[str], List[List[str]]]</code> <p>The top n keywords for a document with their respective distances       to the input document.</p> <p>Usage:</p> <p>To extract keywords from a single document:</p> <pre><code>import openai\nfrom keybert.llm import OpenAI\nfrom keybert import KeyLLM\n\n# Create your LLM\nclient = openai.OpenAI(api_key=MY_API_KEY)\nllm = OpenAI(client)\n\n# Load it in KeyLLM\nkw_model = KeyLLM(llm)\n\n# Extract keywords\ndocument = \"The website mentions that it only takes a couple of days to deliver but I still have not received mine.\"\nkeywords = kw_model.extract_keywords(document)\n</code></pre> Source code in <code>keybert\\_llm.py</code> <pre><code>def extract_keywords(\n    self,\n    docs: Union[str, List[str]],\n    check_vocab: bool = False,\n    candidate_keywords: List[List[str]] = None,\n    threshold: float = None,\n    embeddings=None,\n) -&gt; Union[List[str], List[List[str]]]:\n    \"\"\"Extract keywords and/or keyphrases.\n\n    To get the biggest speed-up, make sure to pass multiple documents\n    at once instead of iterating over a single document.\n\n    NOTE: The resulting keywords are expected to be separated by commas so\n    any changes to the prompt will have to make sure that the resulting\n    keywords are comma-separated.\n\n    Arguments:\n        docs: The document(s) for which to extract keywords/keyphrases\n        check_vocab: Only return keywords that appear exactly in the documents\n        candidate_keywords: Candidate keywords for each document\n        threshold: Minimum similarity value between 0 and 1 used to decide how similar documents need to receive the same keywords.\n        embeddings: The embeddings of each document.\n\n    Returns:\n        keywords: The top n keywords for a document with their respective distances\n                  to the input document.\n\n    Usage:\n\n    To extract keywords from a single document:\n\n    ```python\n    import openai\n    from keybert.llm import OpenAI\n    from keybert import KeyLLM\n\n    # Create your LLM\n    client = openai.OpenAI(api_key=MY_API_KEY)\n    llm = OpenAI(client)\n\n    # Load it in KeyLLM\n    kw_model = KeyLLM(llm)\n\n    # Extract keywords\n    document = \"The website mentions that it only takes a couple of days to deliver but I still have not received mine.\"\n    keywords = kw_model.extract_keywords(document)\n    ```\n    \"\"\"\n    # Check for a single, empty document\n    if isinstance(docs, str):\n        if docs:\n            docs = [docs]\n        else:\n            return []\n\n    if HAS_SBERT and threshold is not None and embeddings is not None:\n        # Find similar documents\n        clusters = util.community_detection(embeddings, min_community_size=2, threshold=threshold)\n        in_cluster = set([cluster for cluster_set in clusters for cluster in cluster_set])\n        out_cluster = set(list(range(len(docs)))).difference(in_cluster)\n\n        # Extract keywords for all documents not in a cluster\n        if out_cluster:\n            selected_docs = [docs[index] for index in out_cluster]\n            if candidate_keywords is not None:\n                selected_keywords = [candidate_keywords[index] for index in out_cluster]\n            else:\n                selected_keywords = None\n            out_cluster_keywords = self.llm.extract_keywords(\n                selected_docs,\n                selected_keywords,\n            )\n            out_cluster_keywords = {index: words for words, index in zip(out_cluster_keywords, out_cluster)}\n\n        # Extract keywords for only the first document in a cluster\n        if in_cluster:\n            selected_docs = [docs[cluster[0]] for cluster in clusters]\n            if candidate_keywords is not None:\n                selected_keywords = [candidate_keywords[cluster[0]] for cluster in clusters]\n            else:\n                selected_keywords = None\n            in_cluster_keywords = self.llm.extract_keywords(selected_docs, selected_keywords)\n            in_cluster_keywords = {\n                doc_id: in_cluster_keywords[index] for index, cluster in enumerate(clusters) for doc_id in cluster\n            }\n\n        # Update out cluster keywords with in cluster keywords\n        if out_cluster:\n            if in_cluster:\n                out_cluster_keywords.update(in_cluster_keywords)\n            keywords = [out_cluster_keywords[index] for index in range(len(docs))]\n        else:\n            keywords = [in_cluster_keywords[index] for index in range(len(docs))]\n    else:\n        # Extract keywords using a Large Language Model (LLM)\n        keywords = self.llm.extract_keywords(docs, candidate_keywords)\n\n    # Only extract keywords that appear in the input document\n    if check_vocab:\n        updated_keywords = []\n        for keyword_set, document in zip(keywords, docs):\n            updated_keyword_set = []\n            for keyword in keyword_set:\n                if keyword in document:\n                    updated_keyword_set.append(keyword)\n            updated_keywords.append(updated_keyword_set)\n        return updated_keywords\n\n    return keywords\n</code></pre>"},{"location":"api/langchain.html","title":"<code>LangChain</code>","text":"<p>               Bases: <code>BaseLLM</code></p> <p>Using chains in langchain to generate keywords.</p> <p>Currently, only chains from question answering is implemented. See: https://langchain.readthedocs.io/en/latest/modules/chains/combine_docs_examples/question_answering.html</p> <p>NOTE: The resulting keywords are expected to be separated by commas so any changes to the prompt will have to make sure that the resulting keywords are comma-separated.</p> <p>Parameters:</p> Name Type Description Default <code>chain</code> <p>A langchain chain that has two input parameters, <code>input_documents</code> and <code>query</code>.</p> required <code>prompt</code> <code>str</code> <p>The prompt to be used in the model. If no prompt is given,     <code>self.default_prompt_</code> is used instead.</p> <code>None</code> <code>verbose</code> <code>bool</code> <p>Set this to True if you want to see a progress bar for the      keyword extraction.</p> <code>False</code> <p>Usage:</p> <p>To use this, you will need to install the langchain package first. Additionally, you will need an underlying LLM to support langchain, like openai:</p> <p><code>pip install langchain</code> <code>pip install openai</code></p> <p>Then, you can create your chain as follows:</p> <pre><code>from langchain.chains.question_answering import load_qa_chain\nfrom langchain.llms import OpenAI\nchain = load_qa_chain(OpenAI(temperature=0, openai_api_key=my_openai_api_key), chain_type=\"stuff\")\n</code></pre> <p>Finally, you can pass the chain to KeyBERT as follows:</p> <pre><code>from keybert.llm import LangChain\nfrom keybert import KeyLLM\n\n# Create your LLM\nllm = LangChain(chain)\n\n# Load it in KeyLLM\nkw_model = KeyLLM(llm)\n\n# Extract keywords\ndocument = \"The website mentions that it only takes a couple of days to deliver but I still have not received mine.\"\nkeywords = kw_model.extract_keywords(document)\n</code></pre> <p>You can also use a custom prompt:</p> <pre><code>prompt = \"What are these documents about? Please give a single label.\"\nllm = LangChain(chain, prompt=prompt)\n</code></pre> Source code in <code>keybert\\llm\\_langchain.py</code> <pre><code>class LangChain(BaseLLM):\n    \"\"\"Using chains in langchain to generate keywords.\n\n    Currently, only chains from question answering is implemented. See:\n    https://langchain.readthedocs.io/en/latest/modules/chains/combine_docs_examples/question_answering.html\n\n    NOTE: The resulting keywords are expected to be separated by commas so\n    any changes to the prompt will have to make sure that the resulting\n    keywords are comma-separated.\n\n    Arguments:\n        chain: A langchain chain that has two input parameters, `input_documents` and `query`.\n        prompt: The prompt to be used in the model. If no prompt is given,\n                `self.default_prompt_` is used instead.\n        verbose: Set this to True if you want to see a progress bar for the\n                 keyword extraction.\n\n    Usage:\n\n    To use this, you will need to install the langchain package first.\n    Additionally, you will need an underlying LLM to support langchain,\n    like openai:\n\n    `pip install langchain`\n    `pip install openai`\n\n    Then, you can create your chain as follows:\n\n    ```python\n    from langchain.chains.question_answering import load_qa_chain\n    from langchain.llms import OpenAI\n    chain = load_qa_chain(OpenAI(temperature=0, openai_api_key=my_openai_api_key), chain_type=\"stuff\")\n    ```\n\n    Finally, you can pass the chain to KeyBERT as follows:\n\n    ```python\n    from keybert.llm import LangChain\n    from keybert import KeyLLM\n\n    # Create your LLM\n    llm = LangChain(chain)\n\n    # Load it in KeyLLM\n    kw_model = KeyLLM(llm)\n\n    # Extract keywords\n    document = \"The website mentions that it only takes a couple of days to deliver but I still have not received mine.\"\n    keywords = kw_model.extract_keywords(document)\n    ```\n\n    You can also use a custom prompt:\n\n    ```python\n    prompt = \"What are these documents about? Please give a single label.\"\n    llm = LangChain(chain, prompt=prompt)\n    ```\n    \"\"\"\n\n    def __init__(\n        self,\n        chain,\n        prompt: str = None,\n        verbose: bool = False,\n    ):\n        self.chain = chain\n        self.prompt = prompt if prompt is not None else DEFAULT_PROMPT\n        self.default_prompt_ = DEFAULT_PROMPT\n        self.verbose = verbose\n\n    def extract_keywords(self, documents: List[str], candidate_keywords: List[List[str]] = None):\n        \"\"\"Extract topics.\n\n        Arguments:\n            documents: The documents to extract keywords from\n            candidate_keywords: A list of candidate keywords that the LLM will fine-tune\n                        For example, it will create a nicer representation of\n                        the candidate keywords, remove redundant keywords, or\n                        shorten them depending on the input prompt.\n\n        Returns:\n            all_keywords: All keywords for each document\n        \"\"\"\n        all_keywords = []\n        candidate_keywords = process_candidate_keywords(documents, candidate_keywords)\n\n        for document, candidates in tqdm(zip(documents, candidate_keywords), disable=not self.verbose):\n            prompt = self.prompt.replace(\"[DOCUMENT]\", document)\n            if candidates is not None:\n                prompt = prompt.replace(\"[CANDIDATES]\", \", \".join(candidates))\n            input_document = Document(page_content=document)\n            keywords = self.chain.run(input_documents=[input_document], question=self.prompt).strip()\n            keywords = [keyword.strip() for keyword in keywords.split(\",\")]\n            all_keywords.append(keywords)\n\n        return all_keywords\n</code></pre>"},{"location":"api/langchain.html#keybert.llm._langchain.LangChain.extract_keywords","title":"<code>extract_keywords(documents, candidate_keywords=None)</code>","text":"<p>Extract topics.</p> <p>Parameters:</p> Name Type Description Default <code>documents</code> <code>List[str]</code> <p>The documents to extract keywords from</p> required <code>candidate_keywords</code> <code>List[List[str]]</code> <p>A list of candidate keywords that the LLM will fine-tune         For example, it will create a nicer representation of         the candidate keywords, remove redundant keywords, or         shorten them depending on the input prompt.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>all_keywords</code> <p>All keywords for each document</p> Source code in <code>keybert\\llm\\_langchain.py</code> <pre><code>def extract_keywords(self, documents: List[str], candidate_keywords: List[List[str]] = None):\n    \"\"\"Extract topics.\n\n    Arguments:\n        documents: The documents to extract keywords from\n        candidate_keywords: A list of candidate keywords that the LLM will fine-tune\n                    For example, it will create a nicer representation of\n                    the candidate keywords, remove redundant keywords, or\n                    shorten them depending on the input prompt.\n\n    Returns:\n        all_keywords: All keywords for each document\n    \"\"\"\n    all_keywords = []\n    candidate_keywords = process_candidate_keywords(documents, candidate_keywords)\n\n    for document, candidates in tqdm(zip(documents, candidate_keywords), disable=not self.verbose):\n        prompt = self.prompt.replace(\"[DOCUMENT]\", document)\n        if candidates is not None:\n            prompt = prompt.replace(\"[CANDIDATES]\", \", \".join(candidates))\n        input_document = Document(page_content=document)\n        keywords = self.chain.run(input_documents=[input_document], question=self.prompt).strip()\n        keywords = [keyword.strip() for keyword in keywords.split(\",\")]\n        all_keywords.append(keywords)\n\n    return all_keywords\n</code></pre>"},{"location":"api/litellm.html","title":"<code>LiteLLM</code>","text":"<p>               Bases: <code>BaseLLM</code></p> <p>Extract keywords using LiteLLM to call any LLM API using OpenAI format such as Anthropic, Huggingface, Cohere, TogetherAI, Azure, OpenAI, etc.</p> <p>NOTE: The resulting keywords are expected to be separated by commas so any changes to the prompt will have to make sure that the resulting keywords are comma-separated.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>str</code> <p>Model to use within LiteLLM, defaults to OpenAI's <code>\"gpt-3.5-turbo\"</code>.</p> <code>'gpt-3.5-turbo'</code> <code>generator_kwargs</code> <code>Mapping[str, Any]</code> <p>Kwargs passed to <code>litellm.completion</code>               for fine-tuning the output.</p> <code>{}</code> <code>prompt</code> <code>str</code> <p>The prompt to be used in the model. If no prompt is given,     <code>self.default_prompt_</code> is used instead.     NOTE: Use <code>\"[DOCUMENT]\"</code> in the prompt     to decide where the document needs to be inserted</p> <code>None</code> <code>system_prompt</code> <code>str</code> <p>The message that sets the behavior of the assistant.            It's typically used to provide high-level instructions            for the conversation.</p> <code>'You are a helpful assistant.'</code> <code>delay_in_seconds</code> <code>float</code> <p>The delay in seconds between consecutive prompts               in order to prevent RateLimitErrors.</p> <code>None</code> <code>verbose</code> <code>bool</code> <p>Set this to True if you want to see a progress bar for the      keyword extraction.</p> <code>False</code> <p>Usage:</p> <p>Let's use OpenAI as an example:</p> <pre><code>import os\nfrom keybert.llm import LiteLLM\nfrom keybert import KeyLLM\n\n# Select LLM\nos.environ[\"OPENAI_API_KEY\"] = \"sk-...\"\nllm = LiteLLM(\"gpt-3.5-turbo\")\n\n# Load it in KeyLLM\nkw_model = KeyLLM(llm)\n\n# Extract keywords\ndocument = \"The website mentions that it only takes a couple of days to deliver but I still have not received mine.\"\nkeywords = kw_model.extract_keywords(document)\n</code></pre> <p>You can also use a custom prompt:</p> <pre><code>prompt = \"I have the following document: [DOCUMENT] \\nThis document contains the following keywords separated by commas: '\"\nllm = LiteLLM(\"gpt-3.5-turbo\", prompt=prompt)\n</code></pre> Source code in <code>keybert\\llm\\_litellm.py</code> <pre><code>class LiteLLM(BaseLLM):\n    r\"\"\"Extract keywords using LiteLLM to call any LLM API using OpenAI format\n    such as Anthropic, Huggingface, Cohere, TogetherAI, Azure, OpenAI, etc.\n\n    NOTE: The resulting keywords are expected to be separated by commas so\n    any changes to the prompt will have to make sure that the resulting\n    keywords are comma-separated.\n\n    Arguments:\n        model: Model to use within LiteLLM, defaults to OpenAI's `\"gpt-3.5-turbo\"`.\n        generator_kwargs: Kwargs passed to `litellm.completion`\n                          for fine-tuning the output.\n        prompt: The prompt to be used in the model. If no prompt is given,\n                `self.default_prompt_` is used instead.\n                NOTE: Use `\"[DOCUMENT]\"` in the prompt\n                to decide where the document needs to be inserted\n        system_prompt: The message that sets the behavior of the assistant.\n                       It's typically used to provide high-level instructions\n                       for the conversation.\n        delay_in_seconds: The delay in seconds between consecutive prompts\n                          in order to prevent RateLimitErrors.\n        verbose: Set this to True if you want to see a progress bar for the\n                 keyword extraction.\n\n    Usage:\n\n    Let's use OpenAI as an example:\n\n    ```python\n    import os\n    from keybert.llm import LiteLLM\n    from keybert import KeyLLM\n\n    # Select LLM\n    os.environ[\"OPENAI_API_KEY\"] = \"sk-...\"\n    llm = LiteLLM(\"gpt-3.5-turbo\")\n\n    # Load it in KeyLLM\n    kw_model = KeyLLM(llm)\n\n    # Extract keywords\n    document = \"The website mentions that it only takes a couple of days to deliver but I still have not received mine.\"\n    keywords = kw_model.extract_keywords(document)\n    ```\n\n    You can also use a custom prompt:\n\n    ```python\n    prompt = \"I have the following document: [DOCUMENT] \\nThis document contains the following keywords separated by commas: '\"\n    llm = LiteLLM(\"gpt-3.5-turbo\", prompt=prompt)\n    ```\n    \"\"\"\n\n    def __init__(\n        self,\n        model: str = \"gpt-3.5-turbo\",\n        prompt: str = None,\n        system_prompt: str = \"You are a helpful assistant.\",\n        generator_kwargs: Mapping[str, Any] = {},\n        delay_in_seconds: float = None,\n        verbose: bool = False,\n    ):\n        self.model = model\n\n        if prompt is None:\n            self.prompt = DEFAULT_PROMPT\n        else:\n            self.prompt = prompt\n\n        self.system_prompt = system_prompt\n        self.default_prompt_ = DEFAULT_PROMPT\n        self.delay_in_seconds = delay_in_seconds\n        self.verbose = verbose\n\n        self.generator_kwargs = generator_kwargs\n        if self.generator_kwargs.get(\"model\"):\n            self.model = generator_kwargs.get(\"model\")\n        if self.generator_kwargs.get(\"prompt\"):\n            del self.generator_kwargs[\"prompt\"]\n\n    def extract_keywords(self, documents: List[str], candidate_keywords: List[List[str]] = None):\n        \"\"\"Extract topics.\n\n        Arguments:\n            documents: The documents to extract keywords from\n            candidate_keywords: A list of candidate keywords that the LLM will fine-tune\n                        For example, it will create a nicer representation of\n                        the candidate keywords, remove redundant keywords, or\n                        shorten them depending on the input prompt.\n\n        Returns:\n            all_keywords: All keywords for each document\n        \"\"\"\n        all_keywords = []\n        candidate_keywords = process_candidate_keywords(documents, candidate_keywords)\n\n        for document, candidates in tqdm(zip(documents, candidate_keywords), disable=not self.verbose):\n            prompt = self.prompt.replace(\"[DOCUMENT]\", document)\n            if candidates is not None:\n                prompt = prompt.replace(\"[CANDIDATES]\", \", \".join(candidates))\n\n            # Delay\n            if self.delay_in_seconds:\n                time.sleep(self.delay_in_seconds)\n\n            # Use a chat model\n            messages = [{\"role\": \"system\", \"content\": self.system_prompt}, {\"role\": \"user\", \"content\": prompt}]\n            kwargs = {\"model\": self.model, \"messages\": messages, **self.generator_kwargs}\n\n            response = completion(**kwargs)\n            keywords = response[\"choices\"][0][\"message\"][\"content\"].strip()\n            keywords = [keyword.strip() for keyword in keywords.split(\",\")]\n            all_keywords.append(keywords)\n\n        return all_keywords\n</code></pre>"},{"location":"api/litellm.html#keybert.llm._litellm.LiteLLM.extract_keywords","title":"<code>extract_keywords(documents, candidate_keywords=None)</code>","text":"<p>Extract topics.</p> <p>Parameters:</p> Name Type Description Default <code>documents</code> <code>List[str]</code> <p>The documents to extract keywords from</p> required <code>candidate_keywords</code> <code>List[List[str]]</code> <p>A list of candidate keywords that the LLM will fine-tune         For example, it will create a nicer representation of         the candidate keywords, remove redundant keywords, or         shorten them depending on the input prompt.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>all_keywords</code> <p>All keywords for each document</p> Source code in <code>keybert\\llm\\_litellm.py</code> <pre><code>def extract_keywords(self, documents: List[str], candidate_keywords: List[List[str]] = None):\n    \"\"\"Extract topics.\n\n    Arguments:\n        documents: The documents to extract keywords from\n        candidate_keywords: A list of candidate keywords that the LLM will fine-tune\n                    For example, it will create a nicer representation of\n                    the candidate keywords, remove redundant keywords, or\n                    shorten them depending on the input prompt.\n\n    Returns:\n        all_keywords: All keywords for each document\n    \"\"\"\n    all_keywords = []\n    candidate_keywords = process_candidate_keywords(documents, candidate_keywords)\n\n    for document, candidates in tqdm(zip(documents, candidate_keywords), disable=not self.verbose):\n        prompt = self.prompt.replace(\"[DOCUMENT]\", document)\n        if candidates is not None:\n            prompt = prompt.replace(\"[CANDIDATES]\", \", \".join(candidates))\n\n        # Delay\n        if self.delay_in_seconds:\n            time.sleep(self.delay_in_seconds)\n\n        # Use a chat model\n        messages = [{\"role\": \"system\", \"content\": self.system_prompt}, {\"role\": \"user\", \"content\": prompt}]\n        kwargs = {\"model\": self.model, \"messages\": messages, **self.generator_kwargs}\n\n        response = completion(**kwargs)\n        keywords = response[\"choices\"][0][\"message\"][\"content\"].strip()\n        keywords = [keyword.strip() for keyword in keywords.split(\",\")]\n        all_keywords.append(keywords)\n\n    return all_keywords\n</code></pre>"},{"location":"api/maxsum.html","title":"<code>Max Sum Distance</code>","text":"<p>Calculate Max Sum Distance for extraction of keywords.</p> <p>We take the 2 x top_n most similar words/phrases to the document. Then, we take all top_n combinations from the 2 x top_n words and extract the combination that are the least similar to each other by cosine similarity.</p> <p>This is O(n^2) and therefore not advised if you use a large <code>top_n</code></p> <p>Parameters:</p> Name Type Description Default <code>doc_embedding</code> <code>ndarray</code> <p>The document embeddings</p> required <code>word_embeddings</code> <code>ndarray</code> <p>The embeddings of the selected candidate keywords/phrases</p> required <code>words</code> <code>List[str]</code> <p>The selected candidate keywords/keyphrases</p> required <code>top_n</code> <code>int</code> <p>The number of keywords/keyhprases to return</p> required <code>nr_candidates</code> <code>int</code> <p>The number of candidates to consider</p> required <p>Returns:</p> Type Description <code>List[Tuple[str, float]]</code> <p>List[Tuple[str, float]]: The selected keywords/keyphrases with their distances</p> Source code in <code>keybert\\_maxsum.py</code> <pre><code>def max_sum_distance(\n    doc_embedding: np.ndarray,\n    word_embeddings: np.ndarray,\n    words: List[str],\n    top_n: int,\n    nr_candidates: int,\n) -&gt; List[Tuple[str, float]]:\n    \"\"\"Calculate Max Sum Distance for extraction of keywords.\n\n    We take the 2 x top_n most similar words/phrases to the document.\n    Then, we take all top_n combinations from the 2 x top_n words and\n    extract the combination that are the least similar to each other\n    by cosine similarity.\n\n    This is O(n^2) and therefore not advised if you use a large `top_n`\n\n    Arguments:\n        doc_embedding: The document embeddings\n        word_embeddings: The embeddings of the selected candidate keywords/phrases\n        words: The selected candidate keywords/keyphrases\n        top_n: The number of keywords/keyhprases to return\n        nr_candidates: The number of candidates to consider\n\n    Returns:\n         List[Tuple[str, float]]: The selected keywords/keyphrases with their distances\n    \"\"\"\n    if nr_candidates &lt; top_n:\n        raise Exception(\"Make sure that the number of candidates exceeds the number \" \"of keywords to return.\")\n    elif top_n &gt; len(words):\n        return []\n\n    # Calculate distances and extract keywords\n    distances = cosine_similarity(doc_embedding, word_embeddings)\n    distances_words = cosine_similarity(word_embeddings, word_embeddings)\n\n    # Get 2*top_n words as candidates based on cosine similarity\n    words_idx = list(distances.argsort()[0][-nr_candidates:])\n    words_vals = [words[index] for index in words_idx]\n    candidates = distances_words[np.ix_(words_idx, words_idx)]\n\n    # Calculate the combination of words that are the least similar to each other\n    min_sim = 100_000\n    candidate = None\n    for combination in itertools.combinations(range(len(words_idx)), top_n):\n        sim = sum([candidates[i][j] for i in combination for j in combination if i != j])\n        if sim &lt; min_sim:\n            candidate = combination\n            min_sim = sim\n\n    return [(words_vals[idx], round(float(distances[0][words_idx[idx]]), 4)) for idx in candidate]\n</code></pre>"},{"location":"api/mmr.html","title":"<code>Maximal Marginal Relevance</code>","text":"<p>Calculate Maximal Marginal Relevance (MMR) between candidate keywords and the document.</p> <p>MMR considers the similarity of keywords/keyphrases with the document, along with the similarity of already selected keywords and keyphrases. This results in a selection of keywords that maximize their within diversity with respect to the document.</p> <p>Parameters:</p> Name Type Description Default <code>doc_embedding</code> <code>ndarray</code> <p>The document embeddings</p> required <code>word_embeddings</code> <code>ndarray</code> <p>The embeddings of the selected candidate keywords/phrases</p> required <code>words</code> <code>List[str]</code> <p>The selected candidate keywords/keyphrases</p> required <code>top_n</code> <code>int</code> <p>The number of keywords/keyhprases to return</p> <code>5</code> <code>diversity</code> <code>float</code> <p>How diverse the select keywords/keyphrases are.        Values between 0 and 1 with 0 being not diverse at all        and 1 being most diverse.</p> <code>0.8</code> <p>Returns:</p> Type Description <code>List[Tuple[str, float]]</code> <p>List[Tuple[str, float]]: The selected keywords/keyphrases with their distances</p> Source code in <code>keybert\\_mmr.py</code> <pre><code>def mmr(\n    doc_embedding: np.ndarray,\n    word_embeddings: np.ndarray,\n    words: List[str],\n    top_n: int = 5,\n    diversity: float = 0.8,\n) -&gt; List[Tuple[str, float]]:\n    \"\"\"Calculate Maximal Marginal Relevance (MMR)\n    between candidate keywords and the document.\n\n\n    MMR considers the similarity of keywords/keyphrases with the\n    document, along with the similarity of already selected\n    keywords and keyphrases. This results in a selection of keywords\n    that maximize their within diversity with respect to the document.\n\n    Arguments:\n        doc_embedding: The document embeddings\n        word_embeddings: The embeddings of the selected candidate keywords/phrases\n        words: The selected candidate keywords/keyphrases\n        top_n: The number of keywords/keyhprases to return\n        diversity: How diverse the select keywords/keyphrases are.\n                   Values between 0 and 1 with 0 being not diverse at all\n                   and 1 being most diverse.\n\n    Returns:\n         List[Tuple[str, float]]: The selected keywords/keyphrases with their distances\n\n    \"\"\"\n    # Extract similarity within words, and between words and the document\n    word_doc_similarity = cosine_similarity(word_embeddings, doc_embedding)\n    word_similarity = cosine_similarity(word_embeddings)\n\n    # Initialize candidates and already choose best keyword/keyphras\n    keywords_idx = [np.argmax(word_doc_similarity)]\n    candidates_idx = [i for i in range(len(words)) if i != keywords_idx[0]]\n\n    for _ in range(min(top_n - 1, len(words) - 1)):\n        # Extract similarities within candidates and\n        # between candidates and selected keywords/phrases\n        candidate_similarities = word_doc_similarity[candidates_idx, :]\n        target_similarities = np.max(word_similarity[candidates_idx][:, keywords_idx], axis=1)\n\n        # Calculate MMR\n        mmr = (1 - diversity) * candidate_similarities - diversity * target_similarities.reshape(-1, 1)\n        mmr_idx = candidates_idx[np.argmax(mmr)]\n\n        # Update keywords &amp; candidates\n        keywords_idx.append(mmr_idx)\n        candidates_idx.remove(mmr_idx)\n\n    # Extract and sort keywords in descending similarity\n    keywords = [(words[idx], round(float(word_doc_similarity.reshape(1, -1)[0][idx]), 4)) for idx in keywords_idx]\n    keywords = sorted(keywords, key=itemgetter(1), reverse=True)\n    return keywords\n</code></pre>"},{"location":"api/openai.html","title":"<code>OpenAI</code>","text":"<p>               Bases: <code>BaseLLM</code></p> <p>Using the OpenAI API to extract keywords.</p> <p>The default method is <code>openai.Completion</code> if <code>chat=False</code>. The prompts will also need to follow a completion task. If you are looking for a more interactive chats, use <code>chat=True</code> with <code>model=gpt-3.5-turbo</code>.</p> <p>For an overview see: https://platform.openai.com/docs/models</p> <p>NOTE: The resulting keywords are expected to be separated by commas so any changes to the prompt will have to make sure that the resulting keywords are comma-separated.</p> <p>Parameters:</p> Name Type Description Default <code>client</code> <p>A <code>openai.OpenAI</code> client</p> required <code>model</code> <code>str</code> <p>Model to use within OpenAI, defaults to <code>\"text-ada-001\"</code>.    NOTE: If a <code>gpt-3.5-turbo</code> model is used, make sure to set    <code>chat</code> to True.</p> <code>'gpt-3.5-turbo-instruct'</code> <code>generator_kwargs</code> <code>Mapping[str, Any]</code> <p>Kwargs passed to <code>openai.Completion.create</code>               for fine-tuning the output.</p> <code>{}</code> <code>prompt</code> <code>str</code> <p>The prompt to be used in the model. If no prompt is given,     <code>self.default_prompt_</code> is used instead.     NOTE: Use <code>\"[DOCUMENT]\"</code> in the prompt     to decide where the document needs to be inserted</p> <code>None</code> <code>system_prompt</code> <code>str</code> <p>The message that sets the behavior of the assistant.            It's typically used to provide high-level instructions            for the conversation.</p> <code>'You are a helpful assistant.'</code> <code>delay_in_seconds</code> <code>float</code> <p>The delay in seconds between consecutive prompts               in order to prevent RateLimitErrors.</p> <code>None</code> <code>exponential_backoff</code> <code>bool</code> <p>Retry requests with a random exponential backoff.                  A short sleep is used when a rate limit error is hit,                  then the requests is retried. Increase the sleep length                  if errors are hit until 10 unsuccesfull requests.                  If True, overrides <code>delay_in_seconds</code>.</p> <code>False</code> <code>chat</code> <code>bool</code> <p>Set this to True if a chat model is used. Generally, this GPT 3.5 or higher   See: https://platform.openai.com/docs/models/gpt-3-5</p> <code>False</code> <code>verbose</code> <code>bool</code> <p>Set this to True if you want to see a progress bar for the      keyword extraction.</p> <code>False</code> <p>Usage:</p> <p>To use this, you will need to install the openai package first:</p> <p><code>pip install openai</code></p> <p>Then, get yourself an API key and use OpenAI's API as follows:</p> <pre><code>import openai\nfrom keybert.llm import OpenAI\nfrom keybert import KeyLLM\n\n# Create your LLM\nclient = openai.OpenAI(api_key=MY_API_KEY)\nllm = OpenAI(client)\n\n# Load it in KeyLLM\nkw_model = KeyLLM(llm)\n\n# Extract keywords\ndocument = \"The website mentions that it only takes a couple of days to deliver but I still have not received mine.\"\nkeywords = kw_model.extract_keywords(document)\n</code></pre> <p>You can also use a custom prompt:</p> <pre><code>prompt = \"I have the following document: [DOCUMENT] \\nThis document contains the following keywords separated by commas: '\"\nllm = OpenAI(client, prompt=prompt, delay_in_seconds=5)\n</code></pre> <p>If you want to use OpenAI's ChatGPT model:</p> <pre><code>llm = OpenAI(client, model=\"gpt-3.5-turbo\", delay_in_seconds=10, chat=True)\n</code></pre> Source code in <code>keybert\\llm\\_openai.py</code> <pre><code>class OpenAI(BaseLLM):\n    r\"\"\"Using the OpenAI API to extract keywords.\n\n    The default method is `openai.Completion` if `chat=False`.\n    The prompts will also need to follow a completion task. If you\n    are looking for a more interactive chats, use `chat=True`\n    with `model=gpt-3.5-turbo`.\n\n    For an overview see:\n    https://platform.openai.com/docs/models\n\n    NOTE: The resulting keywords are expected to be separated by commas so\n    any changes to the prompt will have to make sure that the resulting\n    keywords are comma-separated.\n\n    Arguments:\n        client: A `openai.OpenAI` client\n        model: Model to use within OpenAI, defaults to `\"text-ada-001\"`.\n               NOTE: If a `gpt-3.5-turbo` model is used, make sure to set\n               `chat` to True.\n        generator_kwargs: Kwargs passed to `openai.Completion.create`\n                          for fine-tuning the output.\n        prompt: The prompt to be used in the model. If no prompt is given,\n                `self.default_prompt_` is used instead.\n                NOTE: Use `\"[DOCUMENT]\"` in the prompt\n                to decide where the document needs to be inserted\n        system_prompt: The message that sets the behavior of the assistant.\n                       It's typically used to provide high-level instructions\n                       for the conversation.\n        delay_in_seconds: The delay in seconds between consecutive prompts\n                          in order to prevent RateLimitErrors.\n        exponential_backoff: Retry requests with a random exponential backoff.\n                             A short sleep is used when a rate limit error is hit,\n                             then the requests is retried. Increase the sleep length\n                             if errors are hit until 10 unsuccesfull requests.\n                             If True, overrides `delay_in_seconds`.\n        chat: Set this to True if a chat model is used. Generally, this GPT 3.5 or higher\n              See: https://platform.openai.com/docs/models/gpt-3-5\n        verbose: Set this to True if you want to see a progress bar for the\n                 keyword extraction.\n\n    Usage:\n\n    To use this, you will need to install the openai package first:\n\n    `pip install openai`\n\n    Then, get yourself an API key and use OpenAI's API as follows:\n\n    ```python\n    import openai\n    from keybert.llm import OpenAI\n    from keybert import KeyLLM\n\n    # Create your LLM\n    client = openai.OpenAI(api_key=MY_API_KEY)\n    llm = OpenAI(client)\n\n    # Load it in KeyLLM\n    kw_model = KeyLLM(llm)\n\n    # Extract keywords\n    document = \"The website mentions that it only takes a couple of days to deliver but I still have not received mine.\"\n    keywords = kw_model.extract_keywords(document)\n    ```\n\n    You can also use a custom prompt:\n\n    ```python\n    prompt = \"I have the following document: [DOCUMENT] \\nThis document contains the following keywords separated by commas: '\"\n    llm = OpenAI(client, prompt=prompt, delay_in_seconds=5)\n    ```\n\n    If you want to use OpenAI's ChatGPT model:\n\n    ```python\n    llm = OpenAI(client, model=\"gpt-3.5-turbo\", delay_in_seconds=10, chat=True)\n    ```\n    \"\"\"\n\n    def __init__(\n        self,\n        client,\n        model: str = \"gpt-3.5-turbo-instruct\",\n        prompt: str = None,\n        system_prompt: str = \"You are a helpful assistant.\",\n        generator_kwargs: Mapping[str, Any] = {},\n        delay_in_seconds: float = None,\n        exponential_backoff: bool = False,\n        chat: bool = False,\n        verbose: bool = False,\n    ):\n        self.client = client\n        self.model = model\n\n        if prompt is None:\n            self.prompt = DEFAULT_CHAT_PROMPT if chat else DEFAULT_PROMPT\n        else:\n            self.prompt = prompt\n\n        self.system_prompt = system_prompt\n        self.default_prompt_ = DEFAULT_CHAT_PROMPT if chat else DEFAULT_PROMPT\n        self.delay_in_seconds = delay_in_seconds\n        self.exponential_backoff = exponential_backoff\n        self.chat = chat\n        self.verbose = verbose\n\n        self.generator_kwargs = generator_kwargs\n        if self.generator_kwargs.get(\"model\"):\n            self.model = generator_kwargs.get(\"model\")\n        if self.generator_kwargs.get(\"prompt\"):\n            del self.generator_kwargs[\"prompt\"]\n        if not self.generator_kwargs.get(\"stop\") and not chat:\n            self.generator_kwargs[\"stop\"] = \"\\n\"\n\n    def extract_keywords(self, documents: List[str], candidate_keywords: List[List[str]] = None):\n        \"\"\"Extract topics.\n\n        Arguments:\n            documents: The documents to extract keywords from\n            candidate_keywords: A list of candidate keywords that the LLM will fine-tune\n                        For example, it will create a nicer representation of\n                        the candidate keywords, remove redundant keywords, or\n                        shorten them depending on the input prompt.\n\n        Returns:\n            all_keywords: All keywords for each document\n        \"\"\"\n        all_keywords = []\n        candidate_keywords = process_candidate_keywords(documents, candidate_keywords)\n\n        for document, candidates in tqdm(zip(documents, candidate_keywords), disable=not self.verbose):\n            prompt = self.prompt.replace(\"[DOCUMENT]\", document)\n            if candidates is not None:\n                prompt = prompt.replace(\"[CANDIDATES]\", \", \".join(candidates))\n\n            # Delay\n            if self.delay_in_seconds:\n                time.sleep(self.delay_in_seconds)\n\n            # Use a chat model\n            if self.chat:\n                messages = [{\"role\": \"system\", \"content\": self.system_prompt}, {\"role\": \"user\", \"content\": prompt}]\n                kwargs = {\"model\": self.model, \"messages\": messages, **self.generator_kwargs}\n                if self.exponential_backoff:\n                    response = chat_completions_with_backoff(self.client, **kwargs)\n                else:\n                    response = self.client.chat.completions.create(**kwargs)\n                keywords = response.choices[0].message.content.strip()\n\n            # Use a non-chat model\n            else:\n                if self.exponential_backoff:\n                    response = completions_with_backoff(\n                        self.client, model=self.model, prompt=prompt, **self.generator_kwargs\n                    )\n                else:\n                    response = self.client.completions.create(model=self.model, prompt=prompt, **self.generator_kwargs)\n                keywords = response.choices[0].text.strip()\n            keywords = [keyword.strip() for keyword in keywords.split(\",\")]\n            all_keywords.append(keywords)\n\n        return all_keywords\n</code></pre>"},{"location":"api/openai.html#keybert.llm._openai.OpenAI.extract_keywords","title":"<code>extract_keywords(documents, candidate_keywords=None)</code>","text":"<p>Extract topics.</p> <p>Parameters:</p> Name Type Description Default <code>documents</code> <code>List[str]</code> <p>The documents to extract keywords from</p> required <code>candidate_keywords</code> <code>List[List[str]]</code> <p>A list of candidate keywords that the LLM will fine-tune         For example, it will create a nicer representation of         the candidate keywords, remove redundant keywords, or         shorten them depending on the input prompt.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>all_keywords</code> <p>All keywords for each document</p> Source code in <code>keybert\\llm\\_openai.py</code> <pre><code>def extract_keywords(self, documents: List[str], candidate_keywords: List[List[str]] = None):\n    \"\"\"Extract topics.\n\n    Arguments:\n        documents: The documents to extract keywords from\n        candidate_keywords: A list of candidate keywords that the LLM will fine-tune\n                    For example, it will create a nicer representation of\n                    the candidate keywords, remove redundant keywords, or\n                    shorten them depending on the input prompt.\n\n    Returns:\n        all_keywords: All keywords for each document\n    \"\"\"\n    all_keywords = []\n    candidate_keywords = process_candidate_keywords(documents, candidate_keywords)\n\n    for document, candidates in tqdm(zip(documents, candidate_keywords), disable=not self.verbose):\n        prompt = self.prompt.replace(\"[DOCUMENT]\", document)\n        if candidates is not None:\n            prompt = prompt.replace(\"[CANDIDATES]\", \", \".join(candidates))\n\n        # Delay\n        if self.delay_in_seconds:\n            time.sleep(self.delay_in_seconds)\n\n        # Use a chat model\n        if self.chat:\n            messages = [{\"role\": \"system\", \"content\": self.system_prompt}, {\"role\": \"user\", \"content\": prompt}]\n            kwargs = {\"model\": self.model, \"messages\": messages, **self.generator_kwargs}\n            if self.exponential_backoff:\n                response = chat_completions_with_backoff(self.client, **kwargs)\n            else:\n                response = self.client.chat.completions.create(**kwargs)\n            keywords = response.choices[0].message.content.strip()\n\n        # Use a non-chat model\n        else:\n            if self.exponential_backoff:\n                response = completions_with_backoff(\n                    self.client, model=self.model, prompt=prompt, **self.generator_kwargs\n                )\n            else:\n                response = self.client.completions.create(model=self.model, prompt=prompt, **self.generator_kwargs)\n            keywords = response.choices[0].text.strip()\n        keywords = [keyword.strip() for keyword in keywords.split(\",\")]\n        all_keywords.append(keywords)\n\n    return all_keywords\n</code></pre>"},{"location":"api/textgeneration.html","title":"<code>TextGeneration</code>","text":"<p>               Bases: <code>BaseLLM</code></p> <p>Text2Text or text generation with transformers.</p> <p>NOTE: The resulting keywords are expected to be separated by commas so any changes to the prompt will have to make sure that the resulting keywords are comma-separated.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>Union[str, pipeline]</code> <p>A transformers pipeline that should be initialized as \"text-generation\"    for gpt-like models or \"text2text-generation\" for T5-like models.    For example, <code>pipeline('text-generation', model='gpt2')</code>. If a string    is passed, \"text-generation\" will be selected by default.</p> required <code>prompt</code> <code>str</code> <p>The prompt to be used in the model. If no prompt is given,     <code>self.default_prompt_</code> is used instead.     NOTE: Use <code>\"[KEYWORDS]\"</code> and <code>\"[DOCUMENTS]\"</code> in the prompt     to decide where the keywords and documents need to be     inserted.</p> <code>None</code> <code>pipeline_kwargs</code> <code>Mapping[str, Any]</code> <p>Kwargs that you can pass to the transformers.pipeline              when it is called.</p> <code>{}</code> <code>random_state</code> <code>int</code> <p>A random state to be passed to <code>transformers.set_seed</code></p> <code>42</code> <code>verbose</code> <code>bool</code> <p>Set this to True if you want to see a progress bar for the      keyword extraction.</p> <code>False</code> <p>Usage:</p> <p>To use a gpt-like model:</p> <pre><code>from keybert.llm import TextGeneration\nfrom keybert import KeyLLM\n\n# Create your LLM\ngenerator = pipeline('text-generation', model='gpt2')\nllm = TextGeneration(generator)\n\n# Load it in KeyLLM\nkw_model = KeyLLM(llm)\n\n# Extract keywords\ndocument = \"The website mentions that it only takes a couple of days to deliver but I still have not received mine.\"\nkeywords = kw_model.extract_keywords(document)\n</code></pre> <p>You can use a custom prompt and decide where the document should be inserted with the <code>[DOCUMENT]</code> tag:</p> <pre><code>from keybert.llm import TextGeneration\n\nprompt = \"I have the following documents '[DOCUMENT]'. Please give me the keywords that are present in this document and separate them with commas:\"\n\n# Create your representation model\ngenerator = pipeline('text2text-generation', model='google/flan-t5-base')\nllm = TextGeneration(generator)\n</code></pre> Source code in <code>keybert\\llm\\_textgeneration.py</code> <pre><code>class TextGeneration(BaseLLM):\n    \"\"\"Text2Text or text generation with transformers.\n\n    NOTE: The resulting keywords are expected to be separated by commas so\n    any changes to the prompt will have to make sure that the resulting\n    keywords are comma-separated.\n\n    Arguments:\n        model: A transformers pipeline that should be initialized as \"text-generation\"\n               for gpt-like models or \"text2text-generation\" for T5-like models.\n               For example, `pipeline('text-generation', model='gpt2')`. If a string\n               is passed, \"text-generation\" will be selected by default.\n        prompt: The prompt to be used in the model. If no prompt is given,\n                `self.default_prompt_` is used instead.\n                NOTE: Use `\"[KEYWORDS]\"` and `\"[DOCUMENTS]\"` in the prompt\n                to decide where the keywords and documents need to be\n                inserted.\n        pipeline_kwargs: Kwargs that you can pass to the transformers.pipeline\n                         when it is called.\n        random_state: A random state to be passed to `transformers.set_seed`\n        verbose: Set this to True if you want to see a progress bar for the\n                 keyword extraction.\n\n    Usage:\n\n    To use a gpt-like model:\n\n    ```python\n    from keybert.llm import TextGeneration\n    from keybert import KeyLLM\n\n    # Create your LLM\n    generator = pipeline('text-generation', model='gpt2')\n    llm = TextGeneration(generator)\n\n    # Load it in KeyLLM\n    kw_model = KeyLLM(llm)\n\n    # Extract keywords\n    document = \"The website mentions that it only takes a couple of days to deliver but I still have not received mine.\"\n    keywords = kw_model.extract_keywords(document)\n    ```\n\n    You can use a custom prompt and decide where the document should\n    be inserted with the `[DOCUMENT]` tag:\n\n    ```python\n    from keybert.llm import TextGeneration\n\n    prompt = \"I have the following documents '[DOCUMENT]'. Please give me the keywords that are present in this document and separate them with commas:\"\n\n    # Create your representation model\n    generator = pipeline('text2text-generation', model='google/flan-t5-base')\n    llm = TextGeneration(generator)\n    ```\n    \"\"\"\n\n    def __init__(\n        self,\n        model: Union[str, pipeline],\n        prompt: str = None,\n        pipeline_kwargs: Mapping[str, Any] = {},\n        random_state: int = 42,\n        verbose: bool = False,\n    ):\n        set_seed(random_state)\n        if isinstance(model, str):\n            self.model = pipeline(\"text-generation\", model=model)\n        elif isinstance(model, Pipeline):\n            self.model = model\n        else:\n            raise ValueError(\n                \"Make sure that the HF model that you\"\n                \"pass is either a string referring to a\"\n                \"HF model or a `transformers.pipeline` object.\"\n            )\n        self.prompt = prompt if prompt is not None else DEFAULT_PROMPT\n        self.default_prompt_ = DEFAULT_PROMPT\n        self.pipeline_kwargs = pipeline_kwargs\n        self.verbose = verbose\n\n    def extract_keywords(self, documents: List[str], candidate_keywords: List[List[str]] = None):\n        \"\"\"Extract topics.\n\n        Arguments:\n            documents: The documents to extract keywords from\n            candidate_keywords: A list of candidate keywords that the LLM will fine-tune\n                        For example, it will create a nicer representation of\n                        the candidate keywords, remove redundant keywords, or\n                        shorten them depending on the input prompt.\n\n        Returns:\n            all_keywords: All keywords for each document\n        \"\"\"\n        all_keywords = []\n        candidate_keywords = process_candidate_keywords(documents, candidate_keywords)\n\n        for document, candidates in tqdm(zip(documents, candidate_keywords), disable=not self.verbose):\n            prompt = self.prompt.replace(\"[DOCUMENT]\", document)\n            if candidates is not None:\n                prompt = prompt.replace(\"[CANDIDATES]\", \", \".join(candidates))\n\n            # Extract result from generator and use that as label\n            keywords = self.model(prompt, **self.pipeline_kwargs)[0][\"generated_text\"].replace(prompt, \"\")\n            keywords = [keyword.strip() for keyword in keywords.split(\",\")]\n            all_keywords.append(keywords)\n\n        return all_keywords\n</code></pre>"},{"location":"api/textgeneration.html#keybert.llm._textgeneration.TextGeneration.extract_keywords","title":"<code>extract_keywords(documents, candidate_keywords=None)</code>","text":"<p>Extract topics.</p> <p>Parameters:</p> Name Type Description Default <code>documents</code> <code>List[str]</code> <p>The documents to extract keywords from</p> required <code>candidate_keywords</code> <code>List[List[str]]</code> <p>A list of candidate keywords that the LLM will fine-tune         For example, it will create a nicer representation of         the candidate keywords, remove redundant keywords, or         shorten them depending on the input prompt.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>all_keywords</code> <p>All keywords for each document</p> Source code in <code>keybert\\llm\\_textgeneration.py</code> <pre><code>def extract_keywords(self, documents: List[str], candidate_keywords: List[List[str]] = None):\n    \"\"\"Extract topics.\n\n    Arguments:\n        documents: The documents to extract keywords from\n        candidate_keywords: A list of candidate keywords that the LLM will fine-tune\n                    For example, it will create a nicer representation of\n                    the candidate keywords, remove redundant keywords, or\n                    shorten them depending on the input prompt.\n\n    Returns:\n        all_keywords: All keywords for each document\n    \"\"\"\n    all_keywords = []\n    candidate_keywords = process_candidate_keywords(documents, candidate_keywords)\n\n    for document, candidates in tqdm(zip(documents, candidate_keywords), disable=not self.verbose):\n        prompt = self.prompt.replace(\"[DOCUMENT]\", document)\n        if candidates is not None:\n            prompt = prompt.replace(\"[CANDIDATES]\", \", \".join(candidates))\n\n        # Extract result from generator and use that as label\n        keywords = self.model(prompt, **self.pipeline_kwargs)[0][\"generated_text\"].replace(prompt, \"\")\n        keywords = [keyword.strip() for keyword in keywords.split(\",\")]\n        all_keywords.append(keywords)\n\n    return all_keywords\n</code></pre>"},{"location":"guides/countvectorizer.html","title":"CountVectorizer Tips &amp; Tricks","text":"<p>An unexpectly important component of KeyBERT is the CountVectorizer. In KeyBERT, it is used to split up your documents into candidate keywords and keyphrases. However, there is much more flexibility with the CountVectorizer than you might have initially thought. Since we use the vectorizer to split up the documents after embedding them, we can parse the document however we want as it does not affect the quality of the document embeddings. In this page, we will go through several examples of how you can take the CountVectorizer to the next level and improve upon the generated keywords.</p>"},{"location":"guides/countvectorizer.html#basic-usage","title":"Basic Usage","text":"<p>First, let's start with defining our text and the keyword model:</p> <pre><code>from keybert import KeyBERT\n\ndoc = \"\"\"\n         Supervised learning is the machine learning task of learning a function that\n         maps an input to an output based on example input-output pairs.[1] It infers a\n         function from labeled training data consisting of a set of training examples.[2]\n         In supervised learning, each example is a pair consisting of an input object\n         (typically a vector) and a desired output value (also called the supervisory signal).\n         A supervised learning algorithm analyzes the training data and produces an inferred function,\n         which can be used for mapping new examples. An optimal scenario will allow for the\n         algorithm to correctly determine the class labels for unseen instances. This requires\n         the learning algorithm to generalize from the training data to unseen situations in a\n         'reasonable' way (see inductive bias).\n      \"\"\"\nkw_model = KeyBERT()\n</code></pre> <p>We will use the above code throughout this tutorial as the base and built upon it with the CountVectorizer. Next, we can use a basic vectorizer when extracting keywords as follows:</p> <pre><code>&gt;&gt;&gt; vectorizer = CountVectorizer()\n&gt;&gt;&gt; keywords = kw_model.extract_keywords(doc, vectorizer=vectorizer)\n[('learning', 0.4604),\n ('algorithm', 0.4556),\n ('training', 0.4487),\n ('class', 0.4086),\n ('mapping', 0.3700)]\n</code></pre> <p>NOTE</p> <p>Although I typically like to use <code>use_mmr=True</code> as it often improves upon the generated keywords, this tutorial will do without in order give you a clear view of the effects of the CountVectorizer.</p>"},{"location":"guides/countvectorizer.html#parameters","title":"Parameters","text":"<p>There are a number of basic parameters in the CountVectorizer that we can use to improve upon the quality of the resulting keywords.</p>"},{"location":"guides/countvectorizer.html#ngram_range","title":"ngram_range","text":"<p>By setting the <code>ngram_range</code> we can decide how many tokens the keyphrases needs to be as a minimum and how long it can be as a maximum:</p> <pre><code>&gt;&gt;&gt; vectorizer = CountVectorizer(ngram_range=(1, 3))\n&gt;&gt;&gt; kw_model.extract_keywords(doc, vectorizer=vectorizer)\n[('supervised learning is', 0.7048),\n ('supervised learning algorithm', 0.6834),\n ('supervised learning', 0.6658),\n ('supervised', 0.6523),\n ('in supervised learning', 0.6474)]\n</code></pre> <p>As we can see, the length of the resulting keyphrases are higher than what we have seen before. This may happen as embeddings in vector space are often closer in distance if their document counterparts in similar in size.</p> <p>There are two interesting things happening here. First, there are many similar keyphrases that we want to diversify, which we can achieve by setting <code>use_mmr=True</code>. Second, you may have noticed stopwords appearing in the keyphrases. That we can solve by following the section below!</p>"},{"location":"guides/countvectorizer.html#stop_words","title":"stop_words","text":"<p>As we have seen in the results above, stopwords might appear in your keyphrases. To remove them, we can tell the CountVectorizer to either remove a list of keywords that we supplied ourselves or simply state for which language stopwords need to be removed:</p> <pre><code>&gt;&gt;&gt; vectorizer = CountVectorizer(ngram_range=(1, 3), stop_words=\"english\")\n&gt;&gt;&gt; kw_model.extract_keywords(doc, vectorizer=vectorizer)\n[('supervised learning algorithm', 0.6834),\n ('supervised learning', 0.6658),\n ('supervised learning example', 0.6641),\n ('supervised learning machine', 0.6528),\n ('function labeled training', 0.6526)]\n</code></pre> <p>This already looks much better! The stopwords are removed and the resulting keyphrases already look a bit more interesting and useful.</p>"},{"location":"guides/countvectorizer.html#vocabulary","title":"vocabulary","text":"<p>For some use cases, keywords can only be generated from predefined vocabularies. For example, when you already have a list of possible keywords you can use those as a vocabulary and ask the CountVectorizer to only select keywords from that list.</p> <p>First, let's define our vocabulary:</p> <pre><code>vocab = [\n 'produces inferred function',\n 'supervised',\n 'inductive',\n 'function',\n 'bias',\n 'supervisory',\n 'supervised learning',\n 'infers function',\n 'supervisory signal',\n 'inductive bias',\n 'unseen instances']\n</code></pre> <p>Then, we pass that vocabulary to our CountVectorizer and extract our keywords:</p> <pre><code>&gt;&gt;&gt; vectorizer = CountVectorizer(ngram_range=(1, 3), vocabulary=vocab)\n&gt;&gt;&gt; kw_model.extract_keywords(doc, vectorizer=vectorizer)\n[('supervised learning', 0.6658),\n ('supervised', 0.6523),\n ('supervisory signal', 0.357),\n ('inductive bias', 0.3377),\n ('produces inferred function', 0.3365)]\n</code></pre>"},{"location":"guides/countvectorizer.html#tokenizer","title":"tokenizer","text":"<p>The default tokenizer in the CountVectorizer works well for western languages but fails to tokenize some non-western languages, like Chinese. Fortunately, we can use the <code>tokenizer</code> variable in the CountVectorizer to use <code>jieba</code>, which is a package for Chinese text segmentation. Using it is straightforward:</p> <pre><code>from sklearn.feature_extraction.text import CountVectorizer\nimport jieba\n\ndef tokenize_zh(text):\n    words = jieba.lcut(text)\n    return words\n\nvectorizer = CountVectorizer(tokenizer=tokenize_zh)\n</code></pre> <p>Then, simply pass the vectorizer to your KeyBERT instance:</p> <pre><code>from keybert import KeyBERT\n\nkw_model = KeyBERT()\nkeywords = kw_model.extract_keywords(doc, vectorizer=vectorizer)\n</code></pre>"},{"location":"guides/countvectorizer.html#keyphrasevectorizers","title":"KeyphraseVectorizers","text":"<p>To even further enhance the possibilities of the CountVectorizer, Tim Schopf created an excellent package, KeyphraseVectorizers, that enriches the CountVectorizer with the possibilities to extract keyphrases with part-of-speech patterns using the Spacy library.</p> <p>The great thing about the <code>KeyphraseVectorizers</code> is that aside from leveraging part-of-speech patterns, it automatically extract keyphrases without the need to specify an n-gram range. That by itself is an amazing feature to have! Other advantages of this package:</p> <ul> <li>Extract grammatically accurate keyphases based on their part-of-speech tags.</li> <li>No need to specify n-gram ranges.</li> <li>Get document-keyphrase matrices.</li> <li>Multiple language support.</li> <li>User-defined part-of-speech patterns for keyphrase extraction possible.</li> </ul>"},{"location":"guides/countvectorizer.html#usage","title":"Usage","text":"<p>First, we need to install the package:</p> <pre><code>pip install keyphrase-vectorizers\n</code></pre> <p>Then, let's see what the output looks like with the basic <code>CountVectorizer</code> using a larger n-gram value:</p> <pre><code>&gt;&gt;&gt; vectorizer = CountVectorizer(ngram_range=(1, 3))\n&gt;&gt;&gt; kw_model.extract_keywords(doc, vectorizer=vectorizer)\n[('supervised learning is', 0.7048),\n ('supervised learning algorithm', 0.6834),\n ('supervised learning', 0.6658),\n ('supervised', 0.6523),\n ('in supervised learning', 0.6474)]\n</code></pre> <p>Not bad but as we have seen before, this can definitely be improved. Now, let's use the <code>KeyphraseCountVectorizer</code> instead:</p> <pre><code>&gt;&gt;&gt; from keyphrase_vectorizers import KeyphraseCountVectorizer\n&gt;&gt;&gt; vectorizer = KeyphraseCountVectorizer()\n&gt;&gt;&gt; kw_model.extract_keywords(doc, vectorizer=vectorizer)\n[('supervised learning algorithm', 0.6834),\n ('supervised learning', 0.6658),\n ('learning algorithm', 0.5549),\n ('training data', 0.511),\n ('training', 0.3858)]\n</code></pre> <p>A large improvement compared to the basic CountVectorizer! Now, as seen before it seems that there are still some repeated keyphrases that we want to remove. To do this, we again leverage the <code>MMR</code> function on top of KeyBERT to diversify the output:</p> <pre><code>&gt;&gt;&gt; from keyphrase_vectorizers import KeyphraseCountVectorizer\n&gt;&gt;&gt; vectorizer = KeyphraseCountVectorizer()\n&gt;&gt;&gt; kw_model.extract_keywords(doc, vectorizer=vectorizer, use_mmr=True)\n[('supervised learning algorithm', 0.6834),\n ('unseen instances', 0.3246),\n ('supervisory signal', 0.357),\n ('inductive bias', 0.3377),\n ('class labels', 0.3715)]\n</code></pre> <p>We can see much more diverse keyphrases and based on the input document the keyphrases also make sense.</p>"},{"location":"guides/countvectorizer.html#languages","title":"Languages","text":"<p>Those familiar with Spacy might know that in order to use part-of-speech, we need a language-specific model. You can find an overview of these models here. To change the language model, we only need to change one parameter in order to select a different language:</p> <pre><code>vectorizer = KeyphraseCountVectorizer(spacy_pipeline='de_core_news_sm')\n</code></pre>"},{"location":"guides/countvectorizer.html#part-of-speech","title":"Part-of-speech","text":"<p>KeyphraseVectorizers extracts the part-of-speech tags from the documents and then applies a regex pattern to extract keyphrases that fit within that pattern. The default pattern is <code>&lt;J.*&gt;*&lt;N.*&gt;+</code> which means that it extract keyphrases that have 0 or more adjectives followed by 1 or more nouns.</p> <p>However, we might not agree with that for our specific use case! Fortunately, the package allows you to use a different pattern. To visualize the effect, let's first perform it with the default settings:</p> <pre><code>&gt;&gt;&gt; vectorizer = KeyphraseCountVectorizer()\n&gt;&gt;&gt; kw_model.extract_keywords(doc, vectorizer=vectorizer)\n[('supervised learning algorithm', 0.6834),\n ('supervised learning', 0.6658),\n ('learning algorithm', 0.5549),\n ('training data', 0.511),\n ('training', 0.3858)]\n</code></pre> <p>Although the above keyphrases seem accurate, we might want to only extract a noun from the documents in order to only extract keywords and not keyphrases:</p> <pre><code>&gt;&gt;&gt; vectorizer = KeyphraseCountVectorizer(pos_pattern='&lt;N.*&gt;')\n&gt;&gt;&gt; kw_model.extract_keywords(doc, vectorizer=vectorizer)\n[('learning', 0.467),\n ('training', 0.3858),\n ('labels', 0.3728),\n ('data', 0.2993),\n ('algorithm', 0.2827)]\n</code></pre> <p>These seem much better as keywords now that we focus only on nouns in the document.</p>"},{"location":"guides/embeddings.html","title":"Embedding Models","text":"<p>In this tutorial we will be going through the embedding models that can be used in KeyBERT. Having the option to choose embedding models allow you to leverage pre-trained embeddings that suit your use-case.</p>"},{"location":"guides/embeddings.html#sentence-transformers","title":"Sentence Transformers","text":"<p>You can select any model from sentence-transformers here and pass it through KeyBERT with <code>model</code>:</p> <pre><code>from keybert import KeyBERT\nkw_model = KeyBERT(model=\"all-MiniLM-L6-v2\")\n</code></pre> <p>Or select a SentenceTransformer model with your own parameters:</p> <pre><code>from sentence_transformers import SentenceTransformer\n\nsentence_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\nkw_model = KeyBERT(model=sentence_model)\n</code></pre>"},{"location":"guides/embeddings.html#model2vec","title":"Model2Vec","text":"<p>For blazingly fast embedding models, Model2Vec is an incredible framework. To use it KeyBERT, you only need to pass their <code>StaticModel</code>:</p> <pre><code>from keybert import KeyBERT\nfrom model2vec import StaticModel\n\nembedding_model = StaticModel.from_pretrained(\"minishlab/potion-base-8M\")\nkw_model = KeyBERT(embedding_model)\n</code></pre> <p>If you want to distill a sentence-transformers model with the vocabulary of the documents, run the following:</p> <pre><code>from keybert.backend import Model2VecBackend\n\nembedding_model = Model2VecBackend(\"sentence-transformers/all-MiniLM-L6-v2\", distill=True)\n</code></pre> <p>Note that this is especially helpful if you have a very large dataset (I wouldn't recommend it with small datasets).</p> <p>Tip</p> <p>If you also want to have a light-weight installation without (sentence-)transformers, you can install KeyBERT as follows: <code>pip install keybert --no-deps scikit-learn model2vec</code> This will make the installation much smaller and the import much quicker.</p>"},{"location":"guides/embeddings.html#hugging-face-transformers","title":"\ud83e\udd17 Hugging Face Transformers","text":"<p>To use a Hugging Face transformers model, load in a pipeline and point to any model found on their model hub (https://huggingface.co/models):</p> <pre><code>from transformers.pipelines import pipeline\n\nhf_model = pipeline(\"feature-extraction\", model=\"distilbert-base-cased\")\nkw_model = KeyBERT(model=hf_model)\n</code></pre> <p>Tip!</p> <p>These transformers also work quite well using <code>sentence-transformers</code> which has a number of optimizations tricks that make using it a bit faster.</p>"},{"location":"guides/embeddings.html#flair","title":"Flair","text":"<p>Flair allows you to choose almost any embedding model that is publicly available. Flair can be used as follows:</p> <pre><code>from flair.embeddings import TransformerDocumentEmbeddings\n\nroberta = TransformerDocumentEmbeddings('roberta-base')\nkw_model = KeyBERT(model=roberta)\n</code></pre> <p>You can select any \ud83e\udd17 transformers model here.</p> <p>Moreover, you can also use Flair to use word embeddings and pool them to create document embeddings. Under the hood, Flair simply averages all word embeddings in a document. Then, we can easily pass it to KeyBERT in order to use those word embeddings as document embeddings:</p> <pre><code>from flair.embeddings import WordEmbeddings, DocumentPoolEmbeddings\n\nglove_embedding = WordEmbeddings('crawl')\ndocument_glove_embeddings = DocumentPoolEmbeddings([glove_embedding])\n\nkw_model = KeyBERT(model=document_glove_embeddings)\n</code></pre>"},{"location":"guides/embeddings.html#spacy","title":"Spacy","text":"<p>Spacy is an amazing framework for processing text. There are many models available across many languages for modeling text.</p> <p>To use Spacy's non-transformer models in KeyBERT:</p> <pre><code>import spacy\n\nnlp = spacy.load(\"en_core_web_md\", exclude=['tagger', 'parser', 'ner', 'attribute_ruler', 'lemmatizer'])\n\nkw_model = KeyBERT(model=nlp)\n</code></pre> <p>Using spacy-transformer models:</p> <pre><code>import spacy\n\nspacy.prefer_gpu()\nnlp = spacy.load(\"en_core_web_trf\", exclude=['tagger', 'parser', 'ner', 'attribute_ruler', 'lemmatizer'])\n\nkw_model = KeyBERT(model=nlp)\n</code></pre> <p>If you run into memory issues with spacy-transformer models, try:</p> <pre><code>import spacy\nfrom thinc.api import set_gpu_allocator, require_gpu\n\nnlp = spacy.load(\"en_core_web_trf\", exclude=['tagger', 'parser', 'ner', 'attribute_ruler', 'lemmatizer'])\nset_gpu_allocator(\"pytorch\")\nrequire_gpu(0)\n\nkw_model = KeyBERT(model=nlp)\n</code></pre>"},{"location":"guides/embeddings.html#universal-sentence-encoder-use","title":"Universal Sentence Encoder (USE)","text":"<p>The Universal Sentence Encoder encodes text into high dimensional vectors that are used here for embedding the documents. The model is trained and optimized for greater-than-word length text, such as sentences, phrases or short paragraphs.</p> <p>Using USE in KeyBERT is rather straightforward:</p> <pre><code>import tensorflow_hub\nembedding_model = tensorflow_hub.load(\"https://tfhub.dev/google/universal-sentence-encoder/4\")\nkw_model = KeyBERT(model=embedding_model)\n</code></pre>"},{"location":"guides/embeddings.html#gensim","title":"Gensim","text":"<p>For Gensim, KeyBERT supports its <code>gensim.downloader</code> module. Here, we can download any model word embedding model to be used in KeyBERT. Note that Gensim is primarily used for Word Embedding models. This works typically best for short documents since the word embeddings are pooled.</p> <pre><code>import gensim.downloader as api\nft = api.load('fasttext-wiki-news-subwords-300')\nkw_model = KeyBERT(model=ft)\n</code></pre>"},{"location":"guides/embeddings.html#custom-backend","title":"Custom Backend","text":"<p>If your backend or model cannot be found in the ones currently available, you can use the <code>keybert.backend.BaseEmbedder</code> class to create your own backend. Below, you will find an example of creating a SentenceTransformer backend for KeyBERT:</p> <pre><code>from keybert.backend import BaseEmbedder\nfrom sentence_transformers import SentenceTransformer\n\nclass CustomEmbedder(BaseEmbedder):\n    def __init__(self, embedding_model):\n        super().__init__()\n        self.embedding_model = embedding_model\n\n    def embed(self, documents, verbose=False):\n        embeddings = self.embedding_model.encode(documents, show_progress_bar=verbose)\n        return embeddings\n\n# Create custom backend\ndistilbert = SentenceTransformer(\"paraphrase-MiniLM-L6-v2\")\ncustom_embedder = CustomEmbedder(embedding_model=distilbert)\n\n# Pass custom backend to keybert\nkw_model = KeyBERT(model=custom_embedder)\n</code></pre>"},{"location":"guides/keyllm.html","title":"KeyLLM","text":"<p>A minimal method for keyword extraction with Large Language Models (LLM). There are a number of implementations that allow you to mix and match <code>KeyBERT</code> with <code>KeyLLM</code>. You could also choose to use <code>KeyLLM</code> without <code>KeyBERT</code>.</p> <p>We start with an example of some data:</p> <pre><code>documents = [\n\"The website mentions that it only takes a couple of days to deliver but I still have not received mine.\",\n\"I received my package!\",\n\"Whereas the most powerful LLMs have generally been accessible only through limited APIs (if at all), Meta released LLaMA's model weights to the research community under a noncommercial license.\"\n]\n</code></pre> <p>This data was chosen to show the different use cases and techniques. As you might have noticed documents 1 and 2 are quite similar whereas document 3 is about an entirely different subject. This similarity will be taken into account when using <code>KeyBERT</code> together with <code>KeyLLM</code></p> <p>Let's start with <code>KeyLLM</code> only.</p>"},{"location":"guides/keyllm.html#use-cases","title":"Use Cases","text":"<p>If you want the full performance and easiest method, you can skip the use cases below and go straight to number 5 where you will combine <code>KeyBERT</code> with <code>KeyLLM</code>.</p> <p>Tip</p> <p>If you want to use KeyLLM without any of the HuggingFace packages, you can install it as follows: <code>pip install keybert --no-deps</code> <code>pip install scikit-learn rich tqdm</code> This will make the installation much smaller and the import much quicker.</p>"},{"location":"guides/keyllm.html#1-create-keywords-with-keyllm","title":"1. Create Keywords with <code>KeyLLM</code>","text":"<p>We start by creating keywords for each document. This creation process is simply asking the LLM to come up with a bunch of keywords for each document. The focus here is on creating keywords which refers to the idea that the keywords do not necessarily need to appear in the input documents.</p> <p>Install the relevant LLM first:</p> <pre><code>pip install openai\n</code></pre> <p>Then we can use any OpenAI model, such as ChatGPT, as follows:</p> <pre><code>import openai\nfrom keybert.llm import OpenAI\nfrom keybert import KeyLLM\n\n# Create your LLM\nclient = openai.OpenAI(api_key=MY_API_KEY)\nllm = OpenAI(client)\n\n# Load it in KeyLLM\nkw_model = KeyLLM(llm)\n\n# Extract keywords\nkeywords = kw_model.extract_keywords(documents)\n</code></pre> <p>This creates the following keywords:</p> <pre><code>[['Website',\n  'Delivery',\n  'Mention',\n  'Timeframe',\n  'Not received',\n  'Order fulfillment'],\n ['Package', 'Received', 'Delivery', 'Order fulfillment'],\n ['Powerful LLMs',\n  'Limited APIs',\n  'Meta',\n  'Model weights',\n  'Research community',\n  '']]\n</code></pre>"},{"location":"guides/keyllm.html#2-extract-keywords-with-keyllm","title":"2. Extract Keywords with <code>KeyLLM</code>","text":"<p>Instead of creating keywords out of thin air, we ask the LLM to check whether they actually appear in the text and limit the keywords to those that are found in the documents. We do this by using a custom prompt together with <code>check_vocab=True</code>:</p> <pre><code>import openai\nfrom keybert.llm import OpenAI\nfrom keybert import KeyLLM\n\n# Create your LLM\nprompt = \"\"\"\nI have the following document:\n[DOCUMENT]\n\nBased on the information above, extract the keywords that best describe the topic of the text.\nMake sure to only extract keywords that appear in the text.\nUse the following format separated by commas:\n&lt;keywords&gt;\n\"\"\"\nclient = openai.OpenAI(api_key=MY_API_KEY)\nllm = OpenAI(client)\n\n# Load it in KeyLLM\nkw_model = KeyLLM(llm)\n\n# Extract keywords\nkeywords = kw_model.extract_keywords(documents, check_vocab=True); keywords\n</code></pre> <p>This creates the following keywords:</p> <pre><code>[['website', 'couple of days', 'deliver', 'received'],\n ['package', 'received'],\n ['LLMs',\n  'APIs',\n  'Meta',\n  'LLaMA',\n  'model weights',\n  'research community',\n  'noncommercial license']]\n</code></pre>"},{"location":"guides/keyllm.html#3-fine-tune-candidate-keywords","title":"3. Fine-tune Candidate Keywords","text":"<p>If you already have a list of keywords, you could fine-tune them by asking the LLM to come up with nicer tags or names that we could use.  We can use the <code>[CANDIDATES]</code> tag in the prompt to assign where they should go.</p> <pre><code>import openai\nfrom keybert.llm import OpenAI\nfrom keybert import KeyLLM\n\n# Create your LLM\nprompt = \"\"\"\nI have the following document:\n[DOCUMENT]\n\nWith the following candidate keywords:\n[CANDIDATES]\n\nBased on the information above, improve the candidate keywords to best describe the topic of the document.\n\nUse the following format separated by commas:\n&lt;keywords&gt;\n\"\"\"\nclient = openai.OpenAI(api_key=MY_API_KEY)\nllm = OpenAI(client,model=\"gpt-3.5-turbo\", prompt=prompt, chat=True)\n\n# Load it in KeyLLM\nkw_model = KeyLLM(llm)\n\n# Extract keywords\ncandidate_keywords = [['website', 'couple of days', 'deliver', 'received'],\n ['received', 'package'],\n ['most powerful LLMs',\n  'limited APIs',\n  'Meta',\n  \"LLaMA's model weights\",\n  'research community',\n  'noncommercial license']]\nkeywords = kw_model.extract_keywords(documents, candidate_keywords=candidate_keywords); keywords\n</code></pre> <p>This creates the following keywords:</p> <pre><code>[['delivery timeframe', 'discrepancy', 'website', 'order status'],\n ['received package'],\n ['most powerful language models',\n  'API limitations',\n  \"Meta's release\",\n  \"LLaMA's model weights\",\n  'research community access',\n  'noncommercial licensing']]\n</code></pre>"},{"location":"guides/keyllm.html#4-efficient-keyllm","title":"4. Efficient <code>KeyLLM</code>","text":"<p>If you have embeddings of your documents, you could use those to find documents that are most similar to one another. Those documents could then all receive the same keywords and only one of these documents will need to be passed to the LLM. This can make computation much faster as only a subset of documents will need to receive keywords.</p> <p>Tip</p> <p>Before you get started, it might be worthwhile to uninstall sentence-transformers and re-install it from the main branch. There is an issue with community detection (cluster) that might make the model run without finishing. It is as straightforward as: <code>pip uninstall sentence-transformers</code> <code>pip install --upgrade git+https://github.com/UKPLab/sentence-transformers</code></p> <pre><code>import openai\nfrom keybert.llm import OpenAI\nfrom keybert import KeyLLM\nfrom sentence_transformers import SentenceTransformer\n\n# Extract embeddings\nmodel = SentenceTransformer('all-MiniLM-L6-v2')\nembeddings = model.encode(documents, convert_to_tensor=True)\n\n# Create your LLM\nclient = openai.OpenAI(api_key=MY_API_KEY)\nllm = OpenAI(client)\n\n# Load it in KeyLLM\nkw_model = KeyLLM(llm)\n\n# Extract keywords\nkeywords = kw_model.extract_keywords(documents, embeddings=embeddings, threshold=.75)\n</code></pre> <p>This creates the following keywords:</p> <pre><code>[['Website',\n  'Delivery',\n  'Mention',\n  'Timeframe',\n  'Not received',\n  'Waiting',\n  'Order fulfillment'],\n ['Received', 'Package', 'Delivery', 'Order fulfillment'],\n ['Powerful LLMs', 'Limited APIs', 'Meta', 'LLaMA', 'Model weights']]\n</code></pre>"},{"location":"guides/keyllm.html#5-efficient-keyllm-keybert","title":"5. Efficient <code>KeyLLM</code> + <code>KeyBERT</code>","text":"<p>This is the best of both worlds. We use <code>KeyBERT</code> to generate a first pass of keywords and embeddings and give those to <code>KeyLLM</code> for a final pass. Again, the most similar documents will be clustered and they will all receive the same keywords. You can change this behavior with <code>threshold</code>. A higher value will reduce the number of documents that are clustered and a lower value will increase the number of documents that are clustered.</p> <p>Tip</p> <p>Before you get started, it might be worthwhile to uninstall sentence-transformers and re-install it from the main branch. There is an issue with community detection (cluster) that might make the model run without finishing. It is as straightforward as: <code>pip uninstall sentence-transformers</code> <code>pip install --upgrade git+https://github.com/UKPLab/sentence-transformers</code></p> <pre><code>import openai\nfrom keybert.llm import OpenAI\nfrom keybert import KeyLLM, KeyBERT\n\n# Create your LLM\nclient = openai.OpenAI(api_key=MY_API_KEY)\nllm = OpenAI(client)\n\n# Load it in KeyLLM\nkw_model = KeyBERT(llm=llm)\n\n# Extract keywords\nkeywords = kw_model.extract_keywords(documents); keywords\n</code></pre> <p>This creates the following keywords:</p> <pre><code>[['Website',\n  'Delivery',\n  'Timeframe',\n  'Mention',\n  'Order fulfillment',\n  'Not received',\n  'Waiting'],\n ['Package', 'Received', 'Confirmation', 'Delivery', 'Order fulfillment'],\n ['LLMs', 'Limited APIs', 'Meta', 'LLaMA', 'Model weights', '']]\n</code></pre>"},{"location":"guides/llms.html","title":"Large Language Models (LLM)","text":"<p>In this tutorial we will be going through the Large Language Models (LLM) that can be used in KeyLLM. Having the option to choose the LLM allow you to leverage the model that suit your use-case.</p>"},{"location":"guides/llms.html#openai","title":"OpenAI","text":"<p>To use OpenAI's external API, we need to define our key and use the <code>keybert.llm.OpenAI</code> model.</p> <p>We install the package first:</p> <pre><code>pip install openai\n</code></pre> <p>Then we run OpenAI as follows:</p> <pre><code>import openai\nfrom keybert.llm import OpenAI\nfrom keybert import KeyLLM\n\n# Create your OpenAI LLM\nclient = openai.OpenAI(api_key=MY_API_KEY)\nllm = OpenAI(client)\n\n# Load it in KeyLLM\nkw_model = KeyLLM(llm)\n\n# Extract keywords\nkeywords = kw_model.extract_keywords(MY_DOCUMENTS)\n</code></pre> <p>If you want to use a chat-based model, please run the following instead:</p> <pre><code>import openai\nfrom keybert.llm import OpenAI\nfrom keybert import KeyLLM\n\n# Create your LLM\nclient = openai.OpenAI(api_key=MY_API_KEY)\nllm = OpenAI(client, model=\"gpt-3.5-turbo\", chat=True)\n\n# Load it in KeyLLM\nkw_model = KeyLLM(llm)\n</code></pre>"},{"location":"guides/llms.html#cohere","title":"Cohere","text":"<p>To use Cohere's external API, we need to define our key and use the <code>keybert.llm.Cohere</code> model.</p> <p>We install the package first:</p> <pre><code>pip install cohere\n</code></pre> <p>Then we run Cohere as follows:</p> <pre><code>import cohere\nfrom keybert.llm import Cohere\nfrom keybert import KeyLLM\n\n# Create your OpenAI LLM\nco = cohere.Client(my_api_key)\nllm = Cohere(co)\n\n# Load it in KeyLLM\nkw_model = KeyLLM(llm)\n\n# Extract keywords\nkeywords = kw_model.extract_keywords(MY_DOCUMENTS)\n</code></pre>"},{"location":"guides/llms.html#litellm","title":"LiteLLM","text":"<p>LiteLLM allows you to use any closed-source LLM with KeyLLM</p> <p>We install the package first:</p> <pre><code>pip install litellm\n</code></pre> <p>Let's use OpenAI as an example:</p> <pre><code>import os\nfrom keybert.llm import LiteLLM\nfrom keybert import KeyLLM\n\n# Select LLM\nos.environ[\"OPENAI_API_KEY\"] = \"sk-...\"\nllm = LiteLLM(\"gpt-3.5-turbo\")\n\n# Load it in KeyLLM\nkw_model = KeyLLM(llm)\n</code></pre>"},{"location":"guides/llms.html#hugging-face-transformers","title":"\ud83e\udd17 Hugging Face Transformers","text":"<p>To use a Hugging Face transformers model, load in a pipeline and point to any model found on their model hub (https://huggingface.co/models). Let's use Llama 2 as an example:</p> <pre><code>from torch import cuda, bfloat16\nimport transformers\n\nmodel_id = 'meta-llama/Llama-2-7b-chat-hf'\n\n# 4-bit Quantization to load Llama 2 with less GPU memory\nbnb_config = transformers.BitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_quant_type='nf4',\n    bnb_4bit_use_double_quant=True,\n    bnb_4bit_compute_dtype=bfloat16\n)\n\n# Llama 2 Model &amp; Tokenizer\ntokenizer = transformers.AutoTokenizer.from_pretrained(model_id)\nmodel = transformers.AutoModelForCausalLM.from_pretrained(\n    model_id,\n    trust_remote_code=True,\n    quantization_config=bnb_config,\n    device_map='auto',\n)\nmodel.eval()\n\n# Our text generator\ngenerator = transformers.pipeline(\n    model=model, tokenizer=tokenizer,\n    task='text-generation',\n    temperature=0.1,\n    max_new_tokens=500,\n    repetition_penalty=1.1\n)\n</code></pre> <p>Then, we load the <code>generator</code> in <code>KeyLLM</code> with a custom prompt:</p> <pre><code>from keybert.llm import TextGeneration\nfrom keybert import KeyLLM\n\nprompt = \"\"\"\n&lt;s&gt;[INST] &lt;&lt;SYS&gt;&gt;\n\nYou are a helpful assistant specialized in extracting comma-separated keywords.\nYou are to the point and only give the answer in isolation without any chat-based fluff.\n\n&lt;&lt;/SYS&gt;&gt;\nI have the following document:\n- The website mentions that it only takes a couple of days to deliver but I still have not received mine.\n\nPlease give me the keywords that are present in this document and separate them with commas.\nMake sure you to only return the keywords and say nothing else. For example, don't say:\n\"Here are the keywords present in the document\"\n[/INST] meat, beef, eat, eating, emissions, steak, food, health, processed, chicken [INST]\n\nI have the following document:\n- [DOCUMENT]\n\nPlease give me the keywords that are present in this document and separate them with commas.\nMake sure you to only return the keywords and say nothing else. For example, don't say:\n\"Here are the keywords present in the document\"\n[/INST]\n\"\"\"\n\n# Load it in KeyLLM\nllm = TextGeneration(generator, prompt=prompt)\nkw_model = KeyLLM(llm)\n</code></pre>"},{"location":"guides/llms.html#langchain","title":"LangChain","text":"<p>To use LangChain, we can simply load in any LLM and pass that as a QA-chain to KeyLLM.</p> <p>We install the package first:</p> <pre><code>pip install langchain\n</code></pre> <p>Then we run LangChain as follows:</p> <pre><code>from langchain.chains.question_answering import load_qa_chain\nfrom langchain.llms import OpenAI\nchain = load_qa_chain(OpenAI(temperature=0, openai_api_key=my_openai_api_key), chain_type=\"stuff\")\n</code></pre> <p>Finally, you can pass the chain to KeyBERT as follows:</p> <pre><code>from keybert.llm import LangChain\nfrom keybert import KeyLLM\n\n# Create your LLM\nllm = LangChain(chain)\n\n# Load it in KeyLLM\nkw_model = KeyLLM(llm)\n</code></pre>"},{"location":"guides/quickstart.html","title":"Quickstart","text":""},{"location":"guides/quickstart.html#installation","title":"Installation","text":"<p>Installation can be done using pypi:</p> <pre><code>pip install keybert\n</code></pre> <p>You may want to install more depending on the transformers and language backends that you will be using. The possible installations are:</p> <pre><code>pip install keybert[flair]\npip install keybert[gensim]\npip install keybert[spacy]\npip install keybert[use]\n</code></pre> Input DocumentTokenize WordsEmbed TokensExtract EmbeddingsEmbed DocumentCalculateCosine SimilarityMost microbats use echolocationto navigate and find food.Most microbats use echolocationto navigate and find food.mostmicrobatsuse echolocationtonavigate andfindfood0.110.550.28............0.720.960.34mostfoodMost microbats...mostfood.......08.73We use the CountVectorizer from Scikit-Learn to tokenize our document into candidate kewords/keyphrases.We can use any language model that can embed both documents and keywords, like sentence-transformers.We calculate the cosine similarity between all candidate keywords and the input document. The keywords that have the largest similarity to the document are extracted."},{"location":"guides/quickstart.html#basic-usage","title":"Basic usage","text":"<p>The most minimal example can be seen below for the extraction of keywords: <pre><code>from keybert import KeyBERT\n\ndoc = \"\"\"\n         Supervised learning is the machine learning task of learning a function that\n         maps an input to an output based on example input-output pairs.[1] It infers a\n         function from labeled training data consisting of a set of training examples.[2]\n         In supervised learning, each example is a pair consisting of an input object\n         (typically a vector) and a desired output value (also called the supervisory signal).\n         A supervised learning algorithm analyzes the training data and produces an inferred function,\n         which can be used for mapping new examples. An optimal scenario will allow for the\n         algorithm to correctly determine the class labels for unseen instances. This requires\n         the learning algorithm to generalize from the training data to unseen situations in a\n         'reasonable' way (see inductive bias).\n      \"\"\"\nkw_model = KeyBERT()\nkeywords = kw_model.extract_keywords(doc)\n</code></pre></p> <p>You can set <code>keyphrase_ngram_range</code> to set the length of the resulting keywords/keyphrases:</p> <pre><code>&gt;&gt;&gt; kw_model.extract_keywords(doc, keyphrase_ngram_range=(1, 1), stop_words=None)\n[('learning', 0.4604),\n ('algorithm', 0.4556),\n ('training', 0.4487),\n ('class', 0.4086),\n ('mapping', 0.3700)]\n</code></pre> <p>To extract keyphrases, simply set <code>keyphrase_ngram_range</code> to (1, 2) or higher depending on the number of words you would like in the resulting keyphrases:</p> <pre><code>&gt;&gt;&gt; kw_model.extract_keywords(doc, keyphrase_ngram_range=(1, 2), stop_words=None)\n[('learning algorithm', 0.6978),\n ('machine learning', 0.6305),\n ('supervised learning', 0.5985),\n ('algorithm analyzes', 0.5860),\n ('learning function', 0.5850)]\n</code></pre> <p>We can highlight the keywords in the document by simply setting <code>highlight</code>:</p> <pre><code>keywords = kw_model.extract_keywords(doc, highlight=True)\n</code></pre> <p>NOTE</p> <p>For a full overview of all possible transformer models see sentence-transformer. I would advise either <code>\"all-MiniLM-L6-v2\"</code> for English documents or <code>\"paraphrase-multilingual-MiniLM-L12-v2\"</code> for multi-lingual documents or any other language.</p>"},{"location":"guides/quickstart.html#fine-tuning","title":"Fine-tuning","text":"<p>As a default, KeyBERT simply compares the documents and candidate keywords/keyphrases based on their cosine similarity. However, this might lead to very similar words ending up in the list of most accurate keywords/keyphrases. To make sure they are a bit more diversified, there are two approaches that we can take in order to fine-tune our output, Max Sum Distance and Maximal Marginal Relevance.</p>"},{"location":"guides/quickstart.html#max-sum-distance","title":"Max Sum Distance","text":"<p>To diversify the results, we take the 2 x top_n most similar words/phrases to the document. Then, we take all top_n combinations from the 2 x top_n words and extract the combination that are the least similar to each other by cosine similarity.</p> <pre><code>&gt;&gt;&gt; kw_model.extract_keywords(doc, keyphrase_ngram_range=(3, 3), stop_words='english',\n                              use_maxsum=True, nr_candidates=20, top_n=5)\n[('set training examples', 0.7504),\n ('generalize training data', 0.7727),\n ('requires learning algorithm', 0.5050),\n ('supervised learning algorithm', 0.3779),\n ('learning machine learning', 0.2891)]\n</code></pre>"},{"location":"guides/quickstart.html#maximal-marginal-relevance","title":"Maximal Marginal Relevance","text":"<p>To diversify the results, we can use Maximal Margin Relevance (MMR) to create keywords / keyphrases which is also based on cosine similarity. The results with high diversity:</p> <pre><code>kw_model.extract_keywords(doc, keyphrase_ngram_range=(3, 3), stop_words='english',\n                          use_mmr=True, diversity=0.7)\n[('algorithm generalize training', 0.7727),\n ('labels unseen instances', 0.1649),\n ('new examples optimal', 0.4185),\n ('determine class labels', 0.4774),\n ('supervised learning algorithm', 0.7502)]\n</code></pre> <p>The results with low diversity:</p> <pre><code>&gt;&gt;&gt; kw_model.extract_keywords(doc, keyphrase_ngram_range=(3, 3), stop_words='english',\n                              use_mmr=True, diversity=0.2)\n[('algorithm generalize training', 0.7727),\n ('supervised learning algorithm', 0.7502),\n ('learning machine learning', 0.7577),\n ('learning algorithm analyzes', 0.7587),\n ('learning algorithm generalize', 0.7514)]\n</code></pre>"},{"location":"guides/quickstart.html#candidate-keywordskeyphrases","title":"Candidate Keywords/Keyphrases","text":"<p>In some cases, one might want to be using candidate keywords generated by other keyword algorithms or retrieved from a select list of possible keywords/keyphrases. In KeyBERT, you can easily use those candidate keywords to perform keyword extraction:</p> <pre><code>import yake\nfrom keybert import KeyBERT\n\n# Create candidates\nkw_extractor = yake.KeywordExtractor(top=50)\ncandidates = kw_extractor.extract_keywords(doc)\ncandidates = [candidate[0] for candidate in candidates]\n\n# Pass candidates to KeyBERT\nkw_model = KeyBERT()\nkeywords = kw_model.extract_keywords(doc, candidates=candidates)\n</code></pre>"},{"location":"guides/quickstart.html#guided-keybert","title":"Guided KeyBERT","text":"<p>Guided KeyBERT is similar to Guided Topic Modeling in that it tries to steer the training towards a set of seeded terms. When applying KeyBERT it automatically extracts the most related keywords to a specific document. However, there are times when stakeholders and users are looking for specific types of keywords. For example, when publishing an article on your website through contentful, you typically already know the global keywords related to the article. However, there might be a specific topic in the article that you would like to be extracted through the keywords. To achieve this, we simply give KeyBERT a set of related seeded keywords (it can also be a single one!) and search for keywords that are similar to both the document and the seeded keywords.</p> Input DocumentTokenize WordsEmbed TokensExtract EmbeddingsAverage seed keyword and document embeddingsCalculateCosine SimilarityMost microbats use echolocationto navigate and find food.Most microbats...sonarmostmicrobatsuse echolocationtonavigate andfindfood0.110.550.320.28................0.720.960.490.34mostfoodMost microbats...mostfood.......08.73We use the CountVectorizer from Scikit-Learn to tokenize our document into candidate kewords/keyphrases.We embed the seeded keywords (e.g., the word \u201csonar\u201d) and calculate a weighted average with the document embedding (1:3). We calculate the cosine similarity between all candidate keywords and the input document. The keywords that have the largest similarity to the document are extracted. <p>Using this feature is as simple as defining a list of seeded keywords and passing them to KeyBERT:</p> <pre><code>from keybert import KeyBERT\nkw_model = KeyBERT()\n\n# Define our seeded term\nseed_keywords = [\"information\"]\nkeywords = kw_model.extract_keywords(doc, seed_keywords=seed_keywords)\n</code></pre>"},{"location":"guides/quickstart.html#prepare-embeddings","title":"Prepare embeddings","text":"<p>When you have a large dataset and you want to fine-tune parameters such as <code>diversity</code> it can take quite a while to re-calculate the document and word embeddings each time you change a parameter. Instead, we can pre-calculate these embeddings and pass them to <code>.extract_keywords</code> such that we only have to calculate it once:</p> <pre><code>from keybert import KeyBERT\n\nkw_model = KeyBERT()\ndoc_embeddings, word_embeddings = kw_model.extract_embeddings(docs)\n</code></pre> <p>You can then use these embeddings and pass them to <code>.extract_keywords</code> to speed up the tuning the model:</p> <pre><code>keywords = kw_model.extract_keywords(docs, doc_embeddings=doc_embeddings, word_embeddings=word_embeddings)\n</code></pre> <p>There are several parameters in <code>.extract_embeddings</code> that define how the list of candidate keywords/keyphrases is generated:</p> <ul> <li><code>candidates</code></li> <li><code>keyphrase_ngram_range</code></li> <li><code>stop_words</code></li> <li><code>min_df</code></li> <li><code>vectorizer</code></li> </ul> <p>The values of these parameters need to be exactly the same in <code>.extract_embeddings</code> as they are in <code>. extract_keywords</code>.</p> <p>In other words, the following will work as they use the same parameter subset:</p> <pre><code>from keybert import KeyBERT\n\nkw_model = KeyBERT()\ndoc_embeddings, word_embeddings = kw_model.extract_embeddings(docs, min_df=1, stop_words=\"english\")\nkeywords = kw_model.extract_keywords(docs, min_df=1, stop_words=\"english\",\n                                     doc_embeddings=doc_embeddings,\n                                     word_embeddings=word_embeddings)\n</code></pre> <p>The following, however, will throw an error since we did not use the same values for <code>min_df</code> and <code>stop_words</code>:</p> <pre><code>from keybert import KeyBERT\n\nkw_model = KeyBERT()\ndoc_embeddings, word_embeddings = kw_model.extract_embeddings(docs, min_df=3, stop_words=\"dutch\")\nkeywords = kw_model.extract_keywords(docs, min_df=1, stop_words=\"english\",\n                                     doc_embeddings=doc_embeddings,\n                                     word_embeddings=word_embeddings)\n</code></pre>"}]}